1 Introduction
The rapid advancement and deployment of large language models (LLMs) have revolutionized numerous fields from natural language understanding to code generation and multi-modal reasoning~\cite{bib.bib21 bib.bib5 bib.bib27 bib.bib18}. However alongside their unprecedented capabilities LLMs exhibit vulnerabilities that pose significant risks in real-world applications. These vulnerabilities include generating harmful biased or misleading content which can lead to severe consequences in safety-critical domains such as healthcare finance and autonomous systems~\cite{bib.bib28 bib.bib19 bib.bib32}.
 Existing training-time safety alignment methods involve fine-tuning the LLM to align with human values. RLHF (Reinforcement Learning from Human Feedback) ~\cite{bib.bib2 bib.bib3 bib.bib13 bib.bib22} uses human preferences (e.g. safety) as a reward signal to further tune the LLMs via reinforcement learning. Despite its remarkable performance the training process is costly inefficient and not robust enough to withstand downstream adversarial attacks such as jailbreaks ~\cite{bib.bib35 bib.bib17 bib.bib4 bib.bib9}. In contrast decoding-time interventions such as contrastive decoding ~\cite{bib.bib31 bib.bib33 bib.bib16} offer a more adaptive approach. However this indiscriminate intervention on all queries significantly degrades the quality of benign responses as we studied in Section 2[ref_id]S2.
 To address these limitations we propose CARE a novel framework for decoding-time safety alignment that integrates three key components: (a) a guard model for real-time safety monitoring; (b) a rollback mechanism with a buffer that allows the LLM to recover from unsafe trajectories to an earlier stage without discarding prior progress; and (c) an introspection-based text intervention strategy where the model generates introspective statements about its own outputs and leverages them to guide subsequent decoding steps.
 We evaluate the proposed framework on the Beavertails dataset~\cite{bib.bib12} a benchmark specifically designed to assess safety and response quality across diverse scenarios. Experimental results demonstrate two key findings. First the detect-rollback-intervene mechanism in our CARE framework significantly enhances existing state-of-the-art intervention methods such as Contrastive and Args Decoding by allowing them to improve safety without the catastrophic degradation in response quality seen in their vanilla implementations. Second our novel Introspection method consistently outperforms these already-improved baselines achieving a superior balance of safety quality and latency.
 Our contributions can be summarized as follows:
 (1)
 We introduce a detect-rollback-intervene mechanism that enables targeted and efficient safety interventions. In contrast to existing implementations that apply interventions uniformly across all queries our framework utilizes a guard model and a token-buffered rollback process to selectively apply interventions to potentially harmful content thereby minimizing disruptions to benign responses while preserving output quality.
 (2)
 We propose a novel intervention method based on the self-critique ability of LLMs enabling adaptive and interpretable control during the generation process.
 (3)
 Experimental results show that our framework achieves superior performance: it attains a low harmful response rate as measured by an open-source guard model1https://huggingface.co/cais/HarmBench-Llama-2-13b-cls and minimal degradation in response quality as evaluated by GPT-4o-11-20.


## Section References
[bib.bib21] [21] OpenAI. GPT-4 technical report. CoRR abs/2303.08774 2023.
[bib.bib5] [5] DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. CoRR abs/2501.12948 2025.
[bib.bib27] [27] Qwen Team. Qwen2.5 technical report. CoRR abs/2412.15115 2024.
[bib.bib18] [18] AI @ Meta Llama Team. The llama 3 herd of models 2024.
[bib.bib28] [28] Kun Wang Guibin Zhang Zhenhong Zhou Jiahao Wu Miao Yu Shiqian Zhao Chenlong Yin Jinhu Fu Yibo Yan Hanjun Luo Liang Lin Zhihao Xu Haolang Lu Xinye Cao Xinyun Zhou Weifei Jin Fanci Meng Junyuan Mao Hao Wu Minghe Wang Fan Zhang Junfeng Fang Chengwei Liu Yifan Zhang Qiankun Li Chongye Guo Yalan Qin Yi Ding Donghai Hong Jiaming Ji Xinfeng Li Yifan Jiang Dongxia Wang Yihao Huang Yufei Guo Jen tse Huang Yanwei Yue Wenke Huang Guancheng Wan Tianlin Li Lei Bai Jie Zhang Qing Guo Jingyi Wang Tianlong Chen Joey Tianyi Zhou Xiaojun Jia Weisong Sun Cong Wu Jing Chen Xuming Hu Yiming Li Xiao Wang Ningyu Zhang Luu Anh Tuan Guowen Xu Tianwei Zhang Xingjun Ma Xiang Wang Bo An Jun Sun Mohit Bansal Shirui Pan Yuval Elovici Bhavya Kailkhura Bo Li Yaodong Yang Hongwei Li Wenyuan Xu Yizhou Sun Wei Wang Qing Li Ke Tang Yu-Gang Jiang Felix Juefei-Xu Hui Xiong Xiaofeng Wang Shuicheng Yan Dacheng Tao Philip S. Yu Qingsong Wen and Yang Liu. A comprehensive survey in llm(-agent) full stack safety: Data training and deployment 2025.
[bib.bib19] [19] Xingjun Ma Yifeng Gao Yixu Wang Ruofan Wang Xin Wang Ye Sun Yifan Ding Hengyuan Xu Yunhao Chen Yunhan Zhao Hanxun Huang Yige Li Jiaming Zhang Xiang Zheng Yang Bai Zuxuan Wu Xipeng Qiu Jingfeng Zhang Yiming Li Jun Sun Cong Wang Jindong Gu Baoyuan Wu Siheng Chen Tianwei Zhang Yang Liu Mingming Gong Tongliang Liu Shirui Pan Cihang Xie Tianyu Pang Yinpeng Dong Ruoxi Jia Yang Zhang Shiqing Ma Xiangyu Zhang Neil Gong Chaowei Xiao Sarah M. Erfani Bo Li Masashi Sugiyama Dacheng Tao James Bailey and Yu-Gang Jiang. Safety at scale: A comprehensive survey of large model safety. CoRR abs/2502.05206 2025.
[bib.bib32] [32] Sibo Yi Yule Liu Zhen Sun Tianshuo Cong Xinlei He Jiaxing Song Ke Xu and Qi Li. Jailbreak attacks and defenses against large language models: A survey. CoRR abs/2407.04295 2024.
[bib.bib2] [2] Amanda Askell Yuntao Bai Anna Chen Dawn Drain Deep Ganguli Tom Henighan Andy Jones Nicholas Joseph Benjamin Mann Nova DasSarma Nelson Elhage Zac Hatfield-Dodds Danny Hernandez Jackson Kernion Kamal Ndousse Catherine Olsson Dario Amodei Tom B. Brown Jack Clark Sam McCandlish Chris Olah and Jared Kaplan. A general language assistant as a laboratory for alignment. CoRR abs/2112.00861 2021.
[bib.bib3] [3] Yuntao Bai Andy Jones Kamal Ndousse Amanda Askell Anna Chen Nova DasSarma Dawn Drain Stanislav Fort Deep Ganguli Tom Henighan Nicholas Joseph Saurav Kadavath Jackson Kernion Tom Conerly Sheer El Showk Nelson Elhage Zac Hatfield-Dodds Danny Hernandez Tristan Hume Scott Johnston Shauna Kravec Liane Lovitt Neel Nanda Catherine Olsson Dario Amodei Tom B. Brown Jack Clark Sam McCandlish Chris Olah Benjamin Mann and Jared Kaplan. Training a helpful and harmless assistant with reinforcement learning from human feedback. CoRR abs/2204.05862 2022.
[bib.bib13] [13] Atoosa Kasirzadeh and Iason Gabriel. In conversation with artificial intelligence: aligning language models with human values. CoRR abs/2209.00731 2022.
[bib.bib22] [22] Long Ouyang Jeffrey Wu Xu Jiang Diogo Almeida Carroll L. Wainwright Pamela Mishkin Chong Zhang Sandhini Agarwal Katarina Slama Alex Ray John Schulman Jacob Hilton Fraser Kelton Luke Miller Maddie Simens Amanda Askell Peter Welinder Paul F. Christiano Jan Leike and Ryan Lowe. Training language models to follow instructions with human feedback. In Sanmi Koyejo S. Mohamed A. Agarwal Danielle Belgrave K. Cho and A. Oh editors Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022 NeurIPS 2022 New Orleans LA USA November 28 - December 9 2022 2022.
[bib.bib35] [35] Andy Zou Zifan Wang J. Zico Kolter and Matt Fredrikson. Universal and transferable adversarial attacks on aligned language models. CoRR abs/2307.15043 2023.
[bib.bib17] [17] Xiaogeng Liu Nan Xu Muhao Chen and Chaowei Xiao. Autodan: Generating stealthy jailbreak prompts on aligned large language models. CoRR abs/2310.04451 2023.
[bib.bib4] [4] Patrick Chao Alexander Robey Edgar Dobriban Hamed Hassani George J. Pappas and Eric Wong. Jailbreaking black box large language models in twenty queries. CoRR abs/2310.08419 2023.
[bib.bib9] [9] Xiaomeng Hu Pin-Yu Chen and Tsung-Yi Ho. Attention slipping: A mechanistic understanding of jailbreak attacks and defenses in llms. CoRR abs/2507.04365 2025.
[bib.bib31] [31] Zhangchen Xu Fengqing Jiang Luyao Niu Jinyuan Jia Bill Yuchen Lin and Radha Poovendran. Safedecoding: Defending against jailbreak attacks via safety-aware decoding. 2024.
[bib.bib33] [33] Qihuang Zhong Liang Ding Juhua Liu Bo Du and Dacheng Tao. ROSE doesn’t do that: Boosting the safety of instruction-tuned large language models with reverse prompt contrastive decoding. In Findings of the Association for Computational Linguistics ACL 2024 Bangkok Thailand and virtual meeting August 11-16 2024 pages 13721–13736 2024.
[bib.bib16] [16] Xiang Lisa Li Ari Holtzman Daniel Fried Percy Liang Jason Eisner Tatsunori Hashimoto Luke Zettlemoyer and Mike Lewis. Contrastive decoding: Open-ended text generation as optimization. CoRR abs/2210.15097 2022.
[bib.bib12] [12] Jiaming Ji Mickel Liu Josef Dai Xuehai Pan Chi Zhang Ce Bian Boyuan Chen Ruiyang Sun Yizhou Wang and Yaodong Yang. Beavertails: Towards improved safety alignment of LLM via a human-preference dataset. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023 NeurIPS 2023 New Orleans LA USA December 10 - 16 2023 2023.