3.1.3 The Rollback and Intervention Loop
The core of our framework’s novelty lies in how it responds to the guard model’s signal. The token buffer enables the following seamless correction loop:
 (Step 1)
 Safety Check: As the LLM generates text the guard model continuously monitors the content (Stream + Buffer).
 (Step 2)
 Rollback: If the guard model detects unsafe content (i.e. G(qr(t))=1) a rollback is triggered. The system clears the b tokens in the buffer and crucially reverts the LLM’s internal generation state (e.g. the KV cache) by b steps. This effectively erases the faulty generation path.
 (Step 3)
 Intervention: After rolling back the framework applies a chosen intervention strategy to regenerate a new safe buffer of b tokens. This can be any method such as Contrastive Decoding Temperature Rescaling or our proposed Introspection method.
 This loop repeats until the Guard Model confirms that the newly generated content is safe or until a predefined maximum number of attempts N is reached. Furthermore the total intervention budget B is defined as: B=N\times b representing the maximum number of tokens subject to intervention. This entire process is invisible to the end-user who only experiences a safe coherent stream of text.