4.1 Experimental Setup
Dataset and Models. We test our framework on the BeaverTails dataset~\cite{bib.bib12} a benchmark specifically designed to test the safety and quality of LLM responses in diverse scenarios. We use Qwen2.5-7B-Instruct as the base model to perform generation on the dataset.
 Metrics We evaluate the performance of each intervention method using three key metrics:
 (1) Harmful Response Rate: To provide a unified safety metric across all methods (including vanilla baselines) we measure the Harmful Response Rate (HRR). This is defined as the proportion of all generated responses that are flagged as unsafe by an open source Guard Model specifically the cais/HarmBench-Llama-2-13b-cls model3https://huggingface.co/cais/HarmBench-Llama-2-13b-cls. A lower HRR indicates better safety performance.
 (2) Response Quality: We assess response quality using the LLM-as-a-judge paradigm following the evaluation criteria from Arena-Hard~\cite{bib.bib15}. For each query a response from each method under evaluation is compared against an output from a reference model (GPT-4o-11-20) for the same query. The same GPT-4o-11-20 model also serves as the judge for this pairwise comparison. The final reported quality score is the win-rate of a given method against the reference model.
 (3) Average Wait Tokens: To avoid the impact of specific implementation details we measure the user-perceived latency caused by interventions using the Average Wait Tokens. In our framework the Average Wait Tokens consist of two parts. Firstly at the beginning of a generation users have to wait for b tokens before seeing the first streamed token. Secondly each time we apply an intervention to generate a new buffer users will wait for another b tokens until the intervention is complete. We report the Average Waiting Tokens across the dataset.
 Intervention Strategies. We evaluate several intervention strategies within our framework. We test Contrastive Decoding~\cite{bib.bib16 bib.bib33} a method that directly modifies the output logits by penalizing distributions similar to an unsafe model and Args Decoding~\cite{bib.bib14} which uses a reward signal from our Guard Model to guide the sampling process toward safer outputs. We also evaluate our proposed Introspection Intervention. To ensure a fair comparison we tune the key hyperparameters for the baseline methods to map out their full capability boundaries. Further implementation details for each strategy are available in Appendix C[ref_id]A3.
 Safety Check Frequency. Performing a safety check after every single generated token would be computationally prohibitive. To balance safety responsiveness with efficiency we perform checks in batches. Specifically for a buffer of size b a new safety check is conducted each time \frac{b}{2} additional tokens are generated. This strategy amortizes the evaluation cost while still ensuring that any unsafe content is detected and corrected in a timely manner.
 For further details on the experimental setup and metric implementations please refer to the Appendix D[ref_id]A4.


## Section References
[bib.bib12] [12] Jiaming Ji Mickel Liu Josef Dai Xuehai Pan Chi Zhang Ce Bian Boyuan Chen Ruiyang Sun Yizhou Wang and Yaodong Yang. Beavertails: Towards improved safety alignment of LLM via a human-preference dataset. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023 NeurIPS 2023 New Orleans LA USA December 10 - 16 2023 2023.
[bib.bib15] [15] Tianle Li Wei-Lin Chiang Evan Frick Lisa Dunlap Tianhao Wu Banghua Zhu Joseph E. Gonzalez and Ion Stoica. From crowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline. CoRR abs/2406.11939 2024.
[bib.bib16] [16] Xiang Lisa Li Ari Holtzman Daniel Fried Percy Liang Jason Eisner Tatsunori Hashimoto Luke Zettlemoyer and Mike Lewis. Contrastive decoding: Open-ended text generation as optimization. CoRR abs/2210.15097 2022.
[bib.bib33] [33] Qihuang Zhong Liang Ding Juhua Liu Bo Du and Dacheng Tao. ROSE doesn’t do that: Boosting the safety of instruction-tuned large language models with reverse prompt contrastive decoding. In Findings of the Association for Computational Linguistics ACL 2024 Bangkok Thailand and virtual meeting August 11-16 2024 pages 13721–13736 2024.
[bib.bib14] [14] Maxim Khanov Jirayu Burapacheep and Yixuan Li. ARGS: alignment as reward-guided search. In The Twelfth International Conference on Learning Representations ICLR 2024 Vienna Austria May 7-11 2024 2024.