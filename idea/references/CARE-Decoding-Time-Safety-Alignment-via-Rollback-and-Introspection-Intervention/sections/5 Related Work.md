5 Related Work
LLM Safety Alignment. Ensuring the safety alignment of large language models (LLMs) has been a critical area of research with existing methods broadly categorized into Training-time Safety Alignment and Test-time Safety Alignment. Training-time Safety Alignment focuses on fine-tuning pre-trained models~\cite{bib.bib2 bib.bib3 bib.bib13 bib.bib22} to align their outputs with human values. While this approach has demonstrated significant success it is often costly inefficient and vulnerable to downstream adversarial attacks such as jailbreaks~\cite{bib.bib35 bib.bib17 bib.bib4 bib.bib20 bib.bib1}. In contrast Test-time Safety Alignment operates at two levels: input-level and output-level. At the input level safety alignment typically involves employing guard models or designing classification algorithms~\cite{bib.bib26 bib.bib8 bib.bib11 bib.bib24} to detect and filter prompts that violate safety constraints. Representative examples include LLamaGuard~\cite{bib.bib26} and OpenAI’s Moderation API4https://openai.com/index/upgrading-the-moderation-api-with-our-new-multimodal-moderation-model/. Another line of work perturbs user queries to mitigate potential safety risks~\cite{bib.bib10 bib.bib23} effectively enhancing robustness against harmful inputs. At the output level the filterings are applied after the model generates responses ensuring that only safe content is delivered to users.
 Decoding-time Intervention. Decoding-time intervention techniques operate at three distinct levels: Sampling Configuration Level: This involves adjusting sampling parameters such as top-p top-k and temperature~\cite{bib.bib6}. These configurations influence the diversity and likelihood of generated tokens. Logits Level: At this level interventions modify the logits produced during the decoding process. A representative method is contrastive decoding~\cite{bib.bib16} where an auxiliary model is used to calibrate the logits of the primary LLM at each generation step. Another prominent approach is Guided Decoding~\cite{bib.bib14} which leverages a reward model to evaluate candidate tokens during sampling. The rewards guide the search process steering the model toward more aligned outputs. Context Level: Context-level interventions involve modifying the input context provided to the LLM. Techniques include adjusting system prompts~\cite{bib.bib30 bib.bib1} or applying small text perturbations (nudges)~\cite{bib.bib7} to replace or refine already-generated text. Additionally thinking interventions~\cite{bib.bib34 bib.bib25 bib.bib29} aim to introduce reasoning patterns that guide the LLM to correct its mistakes or mitigate potential risks.


## Section References
[bib.bib2] [2] Amanda Askell Yuntao Bai Anna Chen Dawn Drain Deep Ganguli Tom Henighan Andy Jones Nicholas Joseph Benjamin Mann Nova DasSarma Nelson Elhage Zac Hatfield-Dodds Danny Hernandez Jackson Kernion Kamal Ndousse Catherine Olsson Dario Amodei Tom B. Brown Jack Clark Sam McCandlish Chris Olah and Jared Kaplan. A general language assistant as a laboratory for alignment. CoRR abs/2112.00861 2021.
[bib.bib3] [3] Yuntao Bai Andy Jones Kamal Ndousse Amanda Askell Anna Chen Nova DasSarma Dawn Drain Stanislav Fort Deep Ganguli Tom Henighan Nicholas Joseph Saurav Kadavath Jackson Kernion Tom Conerly Sheer El Showk Nelson Elhage Zac Hatfield-Dodds Danny Hernandez Tristan Hume Scott Johnston Shauna Kravec Liane Lovitt Neel Nanda Catherine Olsson Dario Amodei Tom B. Brown Jack Clark Sam McCandlish Chris Olah Benjamin Mann and Jared Kaplan. Training a helpful and harmless assistant with reinforcement learning from human feedback. CoRR abs/2204.05862 2022.
[bib.bib13] [13] Atoosa Kasirzadeh and Iason Gabriel. In conversation with artificial intelligence: aligning language models with human values. CoRR abs/2209.00731 2022.
[bib.bib22] [22] Long Ouyang Jeffrey Wu Xu Jiang Diogo Almeida Carroll L. Wainwright Pamela Mishkin Chong Zhang Sandhini Agarwal Katarina Slama Alex Ray John Schulman Jacob Hilton Fraser Kelton Luke Miller Maddie Simens Amanda Askell Peter Welinder Paul F. Christiano Jan Leike and Ryan Lowe. Training language models to follow instructions with human feedback. In Sanmi Koyejo S. Mohamed A. Agarwal Danielle Belgrave K. Cho and A. Oh editors Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022 NeurIPS 2022 New Orleans LA USA November 28 - December 9 2022 2022.
[bib.bib35] [35] Andy Zou Zifan Wang J. Zico Kolter and Matt Fredrikson. Universal and transferable adversarial attacks on aligned language models. CoRR abs/2307.15043 2023.
[bib.bib17] [17] Xiaogeng Liu Nan Xu Muhao Chen and Chaowei Xiao. Autodan: Generating stealthy jailbreak prompts on aligned large language models. CoRR abs/2310.04451 2023.
[bib.bib4] [4] Patrick Chao Alexander Robey Edgar Dobriban Hamed Hassani George J. Pappas and Eric Wong. Jailbreaking black box large language models in twenty queries. CoRR abs/2310.08419 2023.
[bib.bib20] [20] Anay Mehrotra Manolis Zampetakis Paul Kassianik Blaine Nelson Hyrum Anderson Yaron Singer and Amin Karbasi. Tree of attacks: Jailbreaking black-box llms automatically. CoRR abs/2312.02119 2023.
[bib.bib1] [1] Cem Anil Esin Durmus Nina Panickssery Mrinank Sharma Joe Benton Sandipan Kundu Joshua Batson Meg Tong Jesse Mu Daniel Ford Francesco Mosconi Rajashree Agrawal Rylan Schaeffer Naomi Bashkansky Samuel Svenningsen Mike Lambert Ansh Radhakrishnan Carson Denison Evan Hubinger Yuntao Bai Trenton Bricken Timothy Maxwell Nicholas Schiefer James Sully Alex Tamkin Tamera Lanham Karina Nguyen Tomek Korbak Jared Kaplan Deep Ganguli Samuel R. Bowman Ethan Perez Roger B. Grosse and David Kristjanson Duvenaud. Many-shot jailbreaking. In Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024 NeurIPS 2024 Vancouver BC Canada December 10 - 15 2024 2024.
[bib.bib26] [26] Llama Team. Meta llama guard 2. https://github.com/meta-llama/PurpleLlama/blob/main/Llama-Guard2/MODEL_CARD.md 2024.
[bib.bib8] [8] Xiaomeng Hu Pin-Yu Chen and Tsung-Yi Ho. Gradient cuff: Detecting jailbreak attacks on large language models by exploring refusal loss landscapes. In Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024 NeurIPS 2024 Vancouver BC Canada December 10 - 15 2024 2024.
[bib.bib11] [11] Neel Jain Avi Schwarzschild Yuxin Wen Gowthami Somepalli John Kirchenbauer Ping-yeh Chiang Micah Goldblum Aniruddha Saha Jonas Geiping and Tom Goldstein. Baseline defenses for adversarial attacks against aligned language models. CoRR abs/2309.00614 2023.
[bib.bib24] [24] Anthropic Safeguards Research Team. Constitutional classifiers: Defending against universal jailbreaks across thousands of hours of red teaming. CoRR abs/2501.18837 2025.
[bib.bib10] [10] Xiaomeng Hu Pin-Yu Chen and Tsung-Yi Ho. Token highlighter: Inspecting and mitigating jailbreak prompts for large language models. In AAAI-25 Sponsored by the Association for the Advancement of Artificial Intelligence February 25 - March 4 2025 Philadelphia PA USA pages 27330–27338 2025.
[bib.bib23] [23] Alexander Robey Eric Wong Hamed Hassani and George J. Pappas. Smoothllm: Defending large language models against jailbreaking attacks. CoRR abs/2310.03684 2023.
[bib.bib6] [6] Ximing Dong Dayi Lin Shaowei Wang and Ahmed E. Hassan. A framework for real-time safeguarding the text generation of large language model. CoRR abs/2404.19048 2024.
[bib.bib16] [16] Xiang Lisa Li Ari Holtzman Daniel Fried Percy Liang Jason Eisner Tatsunori Hashimoto Luke Zettlemoyer and Mike Lewis. Contrastive decoding: Open-ended text generation as optimization. CoRR abs/2210.15097 2022.
[bib.bib14] [14] Maxim Khanov Jirayu Burapacheep and Yixuan Li. ARGS: alignment as reward-guided search. In The Twelfth International Conference on Learning Representations ICLR 2024 Vienna Austria May 7-11 2024 2024.
[bib.bib30] [30] Yueqi Xie Jingwei Yi Jiawei Shao Justin Curl Lingjuan Lyu Qifeng Chen Xing Xie and Fangzhao Wu. Defending chatgpt against jailbreak attack via self-reminders. Nat. Mac. Intell. 5(12):1486–1496 2023.
[bib.bib7] [7] João Fonseca Andrew Bell and Julia Stoyanovich. Safeguarding large language models in real-time with tunable safety-performance trade-offs. CoRR abs/2501.02018 2025.
[bib.bib34] [34] Junda Zhu Lingyong Yan Shuaiqiang Wang Dawei Yin and Lei Sha. Reasoning-to-defend: Safety-aware reasoning can defend large language models from jailbreaking. CoRR abs/2502.12970 2025.
[bib.bib25] [25] Shengyun Si Xinpeng Wang Guangyao Zhai Nassir Navab and Barbara Plank. Think before refusal : Triggering safety reflection in llms to mitigate false refusal behavior. CoRR abs/2503.17882 2025.
[bib.bib29] [29] Tong Wu Chong Xiang Jiachen T. Wang and Prateek Mittal. Effectively controlling reasoning models through thinking intervention. CoRR abs/2503.24370 2025.