References
[bib.bib1] [1] Cem Anil Esin Durmus Nina Panickssery Mrinank Sharma Joe Benton Sandipan Kundu Joshua Batson Meg Tong Jesse Mu Daniel Ford Francesco Mosconi Rajashree Agrawal Rylan Schaeffer Naomi Bashkansky Samuel Svenningsen Mike Lambert Ansh Radhakrishnan Carson Denison Evan Hubinger Yuntao Bai Trenton Bricken Timothy Maxwell Nicholas Schiefer James Sully Alex Tamkin Tamera Lanham Karina Nguyen Tomek Korbak Jared Kaplan Deep Ganguli Samuel R. Bowman Ethan Perez Roger B. Grosse and David Kristjanson Duvenaud. Many-shot jailbreaking. In Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024 NeurIPS 2024 Vancouver BC Canada December 10 - 15 2024 2024.
 [bib.bib2] [2] Amanda Askell Yuntao Bai Anna Chen Dawn Drain Deep Ganguli Tom Henighan Andy Jones Nicholas Joseph Benjamin Mann Nova DasSarma Nelson Elhage Zac Hatfield-Dodds Danny Hernandez Jackson Kernion Kamal Ndousse Catherine Olsson Dario Amodei Tom B. Brown Jack Clark Sam McCandlish Chris Olah and Jared Kaplan. A general language assistant as a laboratory for alignment. CoRR abs/2112.00861 2021.
 [bib.bib3] [3] Yuntao Bai Andy Jones Kamal Ndousse Amanda Askell Anna Chen Nova DasSarma Dawn Drain Stanislav Fort Deep Ganguli Tom Henighan Nicholas Joseph Saurav Kadavath Jackson Kernion Tom Conerly Sheer El Showk Nelson Elhage Zac Hatfield-Dodds Danny Hernandez Tristan Hume Scott Johnston Shauna Kravec Liane Lovitt Neel Nanda Catherine Olsson Dario Amodei Tom B. Brown Jack Clark Sam McCandlish Chris Olah Benjamin Mann and Jared Kaplan. Training a helpful and harmless assistant with reinforcement learning from human feedback. CoRR abs/2204.05862 2022.
 [bib.bib4] [4] Patrick Chao Alexander Robey Edgar Dobriban Hamed Hassani George J. Pappas and Eric Wong. Jailbreaking black box large language models in twenty queries. CoRR abs/2310.08419 2023.
 [bib.bib5] [5] DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. CoRR abs/2501.12948 2025.
 [bib.bib6] [6] Ximing Dong Dayi Lin Shaowei Wang and Ahmed E. Hassan. A framework for real-time safeguarding the text generation of large language model. CoRR abs/2404.19048 2024.
 [bib.bib7] [7] João Fonseca Andrew Bell and Julia Stoyanovich. Safeguarding large language models in real-time with tunable safety-performance trade-offs. CoRR abs/2501.02018 2025.
 [bib.bib8] [8] Xiaomeng Hu Pin-Yu Chen and Tsung-Yi Ho. Gradient cuff: Detecting jailbreak attacks on large language models by exploring refusal loss landscapes. In Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024 NeurIPS 2024 Vancouver BC Canada December 10 - 15 2024 2024.
 [bib.bib9] [9] Xiaomeng Hu Pin-Yu Chen and Tsung-Yi Ho. Attention slipping: A mechanistic understanding of jailbreak attacks and defenses in llms. CoRR abs/2507.04365 2025.
 [bib.bib10] [10] Xiaomeng Hu Pin-Yu Chen and Tsung-Yi Ho. Token highlighter: Inspecting and mitigating jailbreak prompts for large language models. In AAAI-25 Sponsored by the Association for the Advancement of Artificial Intelligence February 25 - March 4 2025 Philadelphia PA USA pages 27330–27338 2025.
 [bib.bib11] [11] Neel Jain Avi Schwarzschild Yuxin Wen Gowthami Somepalli John Kirchenbauer Ping-yeh Chiang Micah Goldblum Aniruddha Saha Jonas Geiping and Tom Goldstein. Baseline defenses for adversarial attacks against aligned language models. CoRR abs/2309.00614 2023.
 [bib.bib12] [12] Jiaming Ji Mickel Liu Josef Dai Xuehai Pan Chi Zhang Ce Bian Boyuan Chen Ruiyang Sun Yizhou Wang and Yaodong Yang. Beavertails: Towards improved safety alignment of LLM via a human-preference dataset. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023 NeurIPS 2023 New Orleans LA USA December 10 - 16 2023 2023.
 [bib.bib13] [13] Atoosa Kasirzadeh and Iason Gabriel. In conversation with artificial intelligence: aligning language models with human values. CoRR abs/2209.00731 2022.
 [bib.bib14] [14] Maxim Khanov Jirayu Burapacheep and Yixuan Li. ARGS: alignment as reward-guided search. In The Twelfth International Conference on Learning Representations ICLR 2024 Vienna Austria May 7-11 2024 2024.
 [bib.bib15] [15] Tianle Li Wei-Lin Chiang Evan Frick Lisa Dunlap Tianhao Wu Banghua Zhu Joseph E. Gonzalez and Ion Stoica. From crowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline. CoRR abs/2406.11939 2024.
 [bib.bib16] [16] Xiang Lisa Li Ari Holtzman Daniel Fried Percy Liang Jason Eisner Tatsunori Hashimoto Luke Zettlemoyer and Mike Lewis. Contrastive decoding: Open-ended text generation as optimization. CoRR abs/2210.15097 2022.
 [bib.bib17] [17] Xiaogeng Liu Nan Xu Muhao Chen and Chaowei Xiao. Autodan: Generating stealthy jailbreak prompts on aligned large language models. CoRR abs/2310.04451 2023.
 [bib.bib18] [18] AI @ Meta Llama Team. The llama 3 herd of models 2024.
 [bib.bib19] [19] Xingjun Ma Yifeng Gao Yixu Wang Ruofan Wang Xin Wang Ye Sun Yifan Ding Hengyuan Xu Yunhao Chen Yunhan Zhao Hanxun Huang Yige Li Jiaming Zhang Xiang Zheng Yang Bai Zuxuan Wu Xipeng Qiu Jingfeng Zhang Yiming Li Jun Sun Cong Wang Jindong Gu Baoyuan Wu Siheng Chen Tianwei Zhang Yang Liu Mingming Gong Tongliang Liu Shirui Pan Cihang Xie Tianyu Pang Yinpeng Dong Ruoxi Jia Yang Zhang Shiqing Ma Xiangyu Zhang Neil Gong Chaowei Xiao Sarah M. Erfani Bo Li Masashi Sugiyama Dacheng Tao James Bailey and Yu-Gang Jiang. Safety at scale: A comprehensive survey of large model safety. CoRR abs/2502.05206 2025.
 [bib.bib20] [20] Anay Mehrotra Manolis Zampetakis Paul Kassianik Blaine Nelson Hyrum Anderson Yaron Singer and Amin Karbasi. Tree of attacks: Jailbreaking black-box llms automatically. CoRR abs/2312.02119 2023.
 [bib.bib21] [21] OpenAI. GPT-4 technical report. CoRR abs/2303.08774 2023.
 [bib.bib22] [22] Long Ouyang Jeffrey Wu Xu Jiang Diogo Almeida Carroll L. Wainwright Pamela Mishkin Chong Zhang Sandhini Agarwal Katarina Slama Alex Ray John Schulman Jacob Hilton Fraser Kelton Luke Miller Maddie Simens Amanda Askell Peter Welinder Paul F. Christiano Jan Leike and Ryan Lowe. Training language models to follow instructions with human feedback. In Sanmi Koyejo S. Mohamed A. Agarwal Danielle Belgrave K. Cho and A. Oh editors Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022 NeurIPS 2022 New Orleans LA USA November 28 - December 9 2022 2022.
 [bib.bib23] [23] Alexander Robey Eric Wong Hamed Hassani and George J. Pappas. Smoothllm: Defending large language models against jailbreaking attacks. CoRR abs/2310.03684 2023.
 [bib.bib24] [24] Anthropic Safeguards Research Team. Constitutional classifiers: Defending against universal jailbreaks across thousands of hours of red teaming. CoRR abs/2501.18837 2025.
 [bib.bib25] [25] Shengyun Si Xinpeng Wang Guangyao Zhai Nassir Navab and Barbara Plank. Think before refusal : Triggering safety reflection in llms to mitigate false refusal behavior. CoRR abs/2503.17882 2025.
 [bib.bib26] [26] Llama Team. Meta llama guard 2. https://github.com/meta-llama/PurpleLlama/blob/main/Llama-Guard2/MODEL_CARD.md 2024.
 [bib.bib27] [27] Qwen Team. Qwen2.5 technical report. CoRR abs/2412.15115 2024.
 [bib.bib28] [28] Kun Wang Guibin Zhang Zhenhong Zhou Jiahao Wu Miao Yu Shiqian Zhao Chenlong Yin Jinhu Fu Yibo Yan Hanjun Luo Liang Lin Zhihao Xu Haolang Lu Xinye Cao Xinyun Zhou Weifei Jin Fanci Meng Junyuan Mao Hao Wu Minghe Wang Fan Zhang Junfeng Fang Chengwei Liu Yifan Zhang Qiankun Li Chongye Guo Yalan Qin Yi Ding Donghai Hong Jiaming Ji Xinfeng Li Yifan Jiang Dongxia Wang Yihao Huang Yufei Guo Jen tse Huang Yanwei Yue Wenke Huang Guancheng Wan Tianlin Li Lei Bai Jie Zhang Qing Guo Jingyi Wang Tianlong Chen Joey Tianyi Zhou Xiaojun Jia Weisong Sun Cong Wu Jing Chen Xuming Hu Yiming Li Xiao Wang Ningyu Zhang Luu Anh Tuan Guowen Xu Tianwei Zhang Xingjun Ma Xiang Wang Bo An Jun Sun Mohit Bansal Shirui Pan Yuval Elovici Bhavya Kailkhura Bo Li Yaodong Yang Hongwei Li Wenyuan Xu Yizhou Sun Wei Wang Qing Li Ke Tang Yu-Gang Jiang Felix Juefei-Xu Hui Xiong Xiaofeng Wang Shuicheng Yan Dacheng Tao Philip S. Yu Qingsong Wen and Yang Liu. A comprehensive survey in llm(-agent) full stack safety: Data training and deployment 2025.
 [bib.bib29] [29] Tong Wu Chong Xiang Jiachen T. Wang and Prateek Mittal. Effectively controlling reasoning models through thinking intervention. CoRR abs/2503.24370 2025.
 [bib.bib30] [30] Yueqi Xie Jingwei Yi Jiawei Shao Justin Curl Lingjuan Lyu Qifeng Chen Xing Xie and Fangzhao Wu. Defending chatgpt against jailbreak attack via self-reminders. Nat. Mac. Intell. 5(12):1486–1496 2023.
 [bib.bib31] [31] Zhangchen Xu Fengqing Jiang Luyao Niu Jinyuan Jia Bill Yuchen Lin and Radha Poovendran. Safedecoding: Defending against jailbreak attacks via safety-aware decoding. 2024.
 [bib.bib32] [32] Sibo Yi Yule Liu Zhen Sun Tianshuo Cong Xinlei He Jiaxing Song Ke Xu and Qi Li. Jailbreak attacks and defenses against large language models: A survey. CoRR abs/2407.04295 2024.
 [bib.bib33] [33] Qihuang Zhong Liang Ding Juhua Liu Bo Du and Dacheng Tao. ROSE doesn’t do that: Boosting the safety of instruction-tuned large language models with reverse prompt contrastive decoding. In Findings of the Association for Computational Linguistics ACL 2024 Bangkok Thailand and virtual meeting August 11-16 2024 pages 13721–13736 2024.
 [bib.bib34] [34] Junda Zhu Lingyong Yan Shuaiqiang Wang Dawei Yin and Lei Sha. Reasoning-to-defend: Safety-aware reasoning can defend large language models from jailbreaking. CoRR abs/2502.12970 2025.
 [bib.bib35] [35] Andy Zou Zifan Wang J. Zico Kolter and Matt Fredrikson. Universal and transferable adversarial attacks on aligned language models. CoRR abs/2307.15043 2023.