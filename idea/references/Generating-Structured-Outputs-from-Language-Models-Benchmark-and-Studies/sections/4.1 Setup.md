4.1 Setup
The efficiency experiment depends on both the size of the model and the tokenizer’s vocabulary size. We used Llama-3.1-8B-Instruct with the Llamacpp inference engine as backend for Outlines Guidance and Llamacpp. As XGrammar doesn’t support Llamacpp as backend  we add an additional experiment with the Hugging Face Transformers inference engine for XGrammar. All experiments are conducted on a single NVIDIA A100-SXM4-80GB GPU with AMD EPYC 7543 (12 cores) CPU. The batch size is set to 1 for all experiments. Additional details about setup are provided in the Appendix E[ref_id]A5. We also provide a snippet of how we call each engine in the Appendix G[ref_id]A7.