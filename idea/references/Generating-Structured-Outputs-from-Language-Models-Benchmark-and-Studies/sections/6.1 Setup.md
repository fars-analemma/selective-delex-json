6.1 Setup
~\cite{bib.bib21 bib.bib31} have introduced a series of tasks to investigate potential quality concerns in constrained decoding which we leverage and extend in this benchmark. Specifically we adopt the three reasoning tasks from these studies to evaluate the impact of constrained decoding on task accuracy as detailed in TableLABEL:tab:task_description. The simple output structure of these tasks was designed to isolate the effects of constrained decoding on reasoning as outlined by ~\cite{bib.bib31}.
 For our experiments we use the Llama-3.1-8B-Instruct model to measure task performance. We follow the original setup and prompt specifications from ~\cite{bib.bib21} with full details provided in Appendix F[ref_id]A6.
 We implement the following constraints for the first three tasks: (1) Last Letter the output needs to be a concatenation of letters from a-z; (2) Shuffle Objects the output needs to be a single letter from A-E enclosed in parentheses; (3) GSM8K the output is an valid integer or float number. The outputs for all three tasks are structured as JSON objects with two fields: "reasoning" and "answer" formatted as {"reasoning": <reasoning about the answer> "answer": <final answer>}.


## Section References
[bib.bib21] Kurt (2024b) Will Kurt. Say what you mean: A response to ’let me speak freely’ 2024b. URL https://.txt.co/blog/say-what-you-mean-a-response-to-let-me-speak-freely.
[bib.bib31] Tam et al. (2024) Zhi Rui Tam Cheng-Kuang Wu Yi-Lin Tsai Chieh-Yen Lin Hung-yi Lee and Yun-Nung Chen. Let me speak freely? a study on the impact of format restrictions on large language model performance. In Franck Dernoncourt Daniel Preoţiuc-Pietro and Anastasia Shimorina (eds.) Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track pp. 1218–1236 Miami Florida US November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-industry.91. URL https://aclanthology.org/2024.emnlp-industry.91/.