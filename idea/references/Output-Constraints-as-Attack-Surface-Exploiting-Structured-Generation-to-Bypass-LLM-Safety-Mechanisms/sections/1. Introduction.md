1. Introduction
Large Language Models (LLMs) have revolutionized the field of Natural Language Processing (NLP) by achieving state-of-the-art performance across a wide range of tasks including language modeling~\cite{bib.bib66 bib.bib15 bib.bib53} text generation~\cite{bib.bib31 bib.bib54} question answering~\cite{bib.bib28} and agent systems~\cite{bib.bib56 bib.bib72}. These advancements have profoundly transformed daily life and work with billions of interactions~\cite{bib.bib17} with LLM services deployed both online and locally. Popular examples include proprietary models such as GPT-4~\cite{bib.bib2} GPT-4 Omni~\cite{bib.bib45} Claude~\cite{bib.bib4} and Gemini~\cite{bib.bib18} as well as open-weight models like Llama~\cite{bib.bib63 bib.bib64 bib.bib40} Qwen~\cite{bib.bib52} Phi~\cite{bib.bib21} and Deepseek~\cite{bib.bib13 bib.bib12} etc.
 Despite their impressive capabilities LLMs also raise serious safety concerns. Malicious actors may exploit these models to generate misinformation promote conspiracy theories scale spear-phishing attacks or facilitate hate campaigns~\cite{bib.bib38 bib.bib73}. Consequently there is an increasing attention on ensuring ethical and legal alignment of LLMs giving rise to a dedicated research field known as LLM safety~\cite{bib.bib75 bib.bib59}.
 \Description
 Illustration of jailbreak attacks
 To illustrate these safety concerns more concretely we consider jailbreak attacks as a representative example. Jailbreak attacks aim to circumvent internal safety alignments or external safeguards of LLMs to generate harmful or malicious outputs. As depicted in Figure1 although direct harmful queries are typically denied by safety aligned models attackers can successfully engineer prompts through various methods. These include Privilege Escalation (e.g. role-playing in developer mode)~\cite{bib.bib35} adversarial prefix/suffix manipulation via brute-force search~\cite{bib.bib7} gradient-based optimization~\cite{bib.bib84} or rule-based construction~\cite{bib.bib34 bib.bib3}. These jailbreak attacks represent a significant security threat as they enable the generation of harmful content and misinformation that could potentially lead to social unrest.
 Given these vulnerabilities ensuring the security of LLMs has become a critical topic within both NLP~\cite{bib.bib75 bib.bib74 bib.bib80 bib.bib32 bib.bib3 bib.bib50} and security communities~\cite{bib.bib57 bib.bib79 bib.bib81 bib.bib14}. Current defense strategies can be broadly categorized into two main approaches:~\cite{bib.bib76 bib.bib75}: internal safety alignment and external safety guardrails. On one hand foundational LLM providers have made significant progress in enhancing internal safety alignment through extensive research on alignment methodologies~\cite{bib.bib21 bib.bib50 bib.bib26 bib.bib25 bib.bib82}. These efforts aim to ensure that LLMs adhere to ethical and safety standards during deployment. However even well-aligned models remain vulnerable to a variety of sophisticated attack methods. These include adversarial prompting~\cite{bib.bib49 bib.bib7 bib.bib84 bib.bib57} which manipulates input prompts to elicit harmful outputs; encoding malicious instructions within code snippets~\cite{bib.bib55} or ciphered inputs~\cite{bib.bib69}; manipulating decoding parameters to influence model behavior~\cite{bib.bib22}; and exploiting vulnerabilities in enforced decoding processes~\cite{bib.bib80}. Such attacks demonstrate that internal safety alignment alone is insufficient to fully safeguard LLMs from adversarial exploitation.
 On the other hand various external defense mechanisms have been proposed to enhance LLM safety each with differing costs and targets. Examples include employing sophisticated classifiers for attack detection~\cite{bib.bib68 bib.bib24} guarding decoding steps with auxiliary expert models~\cite{bib.bib74} and identifying emerging jailbreak prompts through content moderation systems~\cite{bib.bib71}. However all defense strategies inherently introduce trade-offs. Using classifiers~\cite{bib.bib68 bib.bib24} or additional auditing models~\cite{bib.bib27 bib.bib37} incurs extra computational overhead~\cite{bib.bib73}. Typically output-based auditing is more costly compared to prompt auditing due to the sequential nature of LLM generation which can not be audited in advance~\cite{bib.bib14 bib.bib71}. Besides overly cautious prompt auditing may trigger false positives~\cite{bib.bib71 bib.bib36} leading to degraded user experience and reduced service quality.
 Modern LLM services increasingly expose structured output APIs (e.g. JSON schema support~\cite{bib.bib47 bib.bib67}) to enable reliable tool integration. These APIs implicitly rely on constrained decoding techniques~\cite{bib.bib70 bib.bib16} during generation because even strong LLMs may hallucinate or misinterpret complex output formats without explicit constraints~\cite{bib.bib5 bib.bib19}. Constrained decoding combines grammar-level rules with LLM decoding process ensuring a grammar-compatible output. This technique enables structured output features within LLM APIs—such as regular expressions JSON schema or arbitrary EBNF grammars—that are increasingly critical for integrating LLMs into mature industrial applications~\cite{bib.bib43}.
 In this work we identify a novel and particularly high-risk class of jailbreak attacks: Constrained Decoding Attack (CDA). Unlike conventional prompt-based attacks targeting input-side LLM chatbot vulnerabilities CDA exploits the emerging paradigm of LLM APIs as tooling platforms~\cite{bib.bib58 bib.bib51}. As constrained decoding exposes a control-plane to directly control LLM output it is a potential attack surface for adversaries to exploit. CDA operates by manipulating the control-plane of LLM generation hiding the intent of malicious questions within the schemas or grammars exposed by constrained decoding while maintaining a benign data-plane prompt. The effectiveness of CDAs stems from the significant vulnerability in the design of LLM API services utilizing constrained decoding techniques which is revealed in this work. It operates at a fundamental level of LLM architecture that current safety mechanisms don’t monitor.
 Specifically we introduce Chain Enum Attack a simple yet highly effective instantiation of the CDA methodology which leverages two complementary components that can operate either independently or in powerful combination. The foundational Enum Attack component leverages JSON Schema’s enum feature to exploit structured output constraints in modern LLM APIs allowing attackers to bypass safety mechanisms and generate harmful outputs. This is enhanced by the Chain component which uses partial outputs from an easy-to-jailbreak LLM as input to compromise more strongly aligned LLMs creating a cascade effect that amplifies the attack’s effectiveness across different security levels of language models and presenting significant challenges for existing safety mechanisms.
 We empirically demonstrate that Chain Enum Attack can successfully jailbreak a wide range of proprietary and open-source LLMs in a single shot. It achieves very high Attack Success Rate (ASR) and StrongREJECT score~\cite{bib.bib60}: a metric for evaluating the harmfulness of jailbreak outputs—with a single query. Our evaluation spans five widely recognized benchmarks: AdvBench~\cite{bib.bib84} HarmBench~\cite{bib.bib38} JailbreakBench~\cite{bib.bib9} SorryBench~\cite{bib.bib73} and StrongREJECT~\cite{bib.bib60}.
 Furthermore Chain Enum Attack bypasses most existing prompt-based external safeguards including industrially deployed defenses from OpenAI and Gemini. This capability highlights the significant threat posed by CDA to current LLM safety frameworks.
 More broadly CDA directly challenges the security assumptions underlying structured output technologies—a core enabling feature critical for seamless integration between LLMs and existing software stacks. Given structured output’s growing importance in mature industrial applications of LLMs this vulnerability represents a severe threat vector that demands immediate attention.
 In summary our contributions are:
 •
 We propose Constrained Decoding Attack (CDA) a novel dimension of black-box jailbreak attacks that uniquely operates by manipulating the control-plane exposed by constrained decoding techniques (through output generation constraints) rather than the data-plane where most jailbreak attempts occur (through input prompts) thereby exploiting the structured output features prevalent among modern LLM services while maintaining the appearance of benign requests.
 •
 We introduce Chain Enum Attack a simple yet powerful instance of CDA utilizing JSON Schema’s enum feature to hide malicious content in the control-plane. Our pass@1 attack achieves an average of 96.2% Attack Success Rate (ASR) and 82.6% StrongREJECT score~\cite{bib.bib60} across models including GPT-4o and Gemini-2.0-flash demonstrating its high effectiveness across diverse commercial/open-source models with minimal query complexity.
 •
 We analyze why Chain Enum Attack successfully bypasses both internal and external defenses. By targeting decoding-stage vulnerabilities Chain Enum Attack bypasses prompt-based external safeguards while exploiting inherent limitations in token prediction-based internal alignments. These findings aim to inspire future research and redteaming efforts within the broader LLM safety community.
[IMAGE START] [IMAGE URL: /mnt/bmcpfs-29000zjpjtl6xjmjiifyk/xkhu/ideation_workspace/papers/reference_figures/796673fd60e6c309db4f63374e161aa7.png] Figure 1. Illustration of jailbreak attacks: attackers employ various methods to bypass internal safety alignment or external safty checks.[IMAGE END]



## Section References
[bib.bib66] Vaswani et al. (2017) Ashish Vaswani Noam Shazeer Niki Parmar Jakob Uszkoreit Llion Jones Aidan N Gomez Łukasz Kaiser and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems 30 (2017).
[bib.bib15] Devlin et al. (2019) Jacob Devlin Ming-Wei Chang Kenton Lee and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies volume 1 (long and short papers). 4171–4186.
[bib.bib53] Radford et al. (2018) Alec Radford Karthik Narasimhan Tim Salimans and Ilya Sutskever. 2018. Improving language understanding by generative pre-training. (2018).
[bib.bib31] Lewis et al. (2020) Mike Lewis Yinhan Liu Naman Goyal Marjan Ghazvininejad Abdelrahman Mohamed Omer Levy Veselin Stoyanov and Luke Zettlemoyer. 2020. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation Translation and Comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics Dan Jurafsky Joyce Chai Natalie Schluter and Joel Tetreault (Eds.). Association for Computational Linguistics Online 7871–7880. doi:10.18653/v1/2020.acl-main.703 [https://doi.org/10.18653/v1/2020.acl-main.703]
[bib.bib54] Radford et al. (2019) Alec Radford Jeffrey Wu Rewon Child David Luan Dario Amodei Ilya Sutskever et al. 2019. Language models are unsupervised multitask learners. OpenAI blog 1 8 (2019) 9.
[bib.bib28] Kamalloo et al. (2023) Ehsan Kamalloo Nouha Dziri Charles Clarke and Davood Rafiei. 2023. Evaluating Open-Domain Question Answering in the Era of Large Language Models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) Anna Rogers Jordan Boyd-Graber and Naoaki Okazaki (Eds.). Association for Computational Linguistics Toronto Canada 5591–5606. doi:10.18653/v1/2023.acl-long.307 [https://doi.org/10.18653/v1/2023.acl-long.307]
[bib.bib56] Richards and Significant Gravitas (2023) Toran Bruce Richards and Significant Gravitas. 2023. AutoGPT: An Autonomous GPT-4 Based Agent. https://github.com/Significant-Gravitas/AutoGPT. Released on March 30 2023.
[bib.bib72] Wu et al. (2024a) Qingyun Wu Gagan Bansal Jieyu Zhang Yiran Wu Beibin Li Erkang Zhu Li Jiang Xiaoyun Zhang Shaokun Zhang Jiale Liu Ahmed Hassan Awadallah Ryen W White Doug Burger and Chi Wang. 2024a. AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversations. In First Conference on Language Modeling. https://openreview.net/forum?id=BAakY1hNKS
[bib.bib17] Duarte (2025) Fabio Duarte. 2025. Number of ChatGPT Users (March 2025). https://explodingtopics.com/blog/chatgpt-users. Accessed: March 31 2025.
[bib.bib2] Achiam et al. (2023) Josh Achiam Steven Adler Sandhini Agarwal Lama Ahmad Ilge Akkaya Florencia Leoni Aleman Diogo Almeida Janko Altenschmidt Sam Altman Shyamal Anadkat et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023).
[bib.bib45] OpenAI (2024b) OpenAI. 2024b. GPT-4o System Card. https://openai.com/index/gpt-4o-system-card/ Accessed: March 07 2025.
[bib.bib4] Anthropic (2024) Anthropic. 2024. Introducing the next generation of Claude. https://www.anthropic.com/news/claude-3-family
[bib.bib18] Gemini Team Google (2023) Gemini Team Google. 2023. Gemini: A Family of Highly Capable Multimodal Models. arXiv preprint arXiv:2312.11805 (2023).
[bib.bib63] Touvron et al. (2023a) Hugo Touvron Thibaut Lavril Gautier Izacard Xavier Martinet Marie-Anne Lachaux Timothée Lacroix Baptiste Rozière Naman Goyal Eric Hambro Faisal Azhar Aurelien Rodriguez Armand Joulin Edouard Grave and Guillaume Lample. 2023a. LLaMA: Open and Efficient Foundation Language Models. arXiv preprint arXiv:2302.13971 (2023).
[bib.bib64] Touvron et al. (2023b) Hugo Touvron Louis Martin Kevin Stone Peter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra Prajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen Guillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller Cynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou Hakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev Punit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich Yinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra Igor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang Angela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic Sergey Edunov and Thomas Scialom. 2023b. Llama 2: Open Foundation and Fine-Tuned Chat Models. arXiv preprint arXiv:2307.09288 (2023).
[bib.bib40] Meta AI (2024) Meta AI. 2024. Introducing Meta Llama 3: The most capable openly available LLM to date. https://ai.meta.com/blog/meta-llama-3
[bib.bib52] Qwen et al. (2025) Qwen : An Yang Baosong Yang Beichen Zhang Binyuan Hui Bo Zheng Bowen Yu Chengyuan Li Dayiheng Liu Fei Huang Haoran Wei Huan Lin Jian Yang Jianhong Tu Jianwei Zhang Jianxin Yang Jiaxi Yang Jingren Zhou Junyang Lin Kai Dang Keming Lu Keqin Bao Kexin Yang Le Yu Mei Li Mingfeng Xue Pei Zhang Qin Zhu Rui Men Runji Lin Tianhao Li Tianyi Tang Tingyu Xia Xingzhang Ren Xuancheng Ren Yang Fan Yang Su Yichang Zhang Yu Wan Yuqiong Liu Zeyu Cui Zhenru Zhang and Zihan Qiu. 2025. Qwen2.5 Technical Report. arXiv:2412.15115 [cs.CL] https://arxiv.org/abs/2412.15115
[bib.bib21] Haider et al. (2024) Emman Haider Daniel Perez-Becker Thomas Portet Piyush Madan Amit Garg Atabak Ashfaq David Majercak Wen Wen Dongwoo Kim Ziyi Yang Jianwen Zhang Hiteshi Sharma Blake Bullwinkel Martin Pouliot Amanda Minnich Shiven Chawla Solianna Herrera Shahed Warreth Maggie Engler Gary Lopez Nina Chikanov Raja Sekhar Rao Dheekonda Bolor-Erdene Jagdagdorj Roman Lutz Richard Lundeen Tori Westerhoff Pete Bryan Christian Seifert Ram Shankar Siva Kumar Andrew Berkley and Alex Kessler. 2024. Phi-3 Safety Post-Training: Aligning Language Models with a ”Break-Fix” Cycle. arXiv:2407.13833 [cs.CL] https://arxiv.org/abs/2407.13833
[bib.bib13] DeepSeek-AI et al. (2025b) DeepSeek-AI Aixin Liu Bei Feng Bing Xue Bingxuan Wang Bochao Wu Chengda Lu Chenggang Zhao Chengqi Deng Chenyu Zhang Chong Ruan Damai Dai Daya Guo Dejian Yang Deli Chen Dongjie Ji Erhang Li Fangyun Lin Fucong Dai Fuli Luo Guangbo Hao Guanting Chen Guowei Li H. Zhang Han Bao Hanwei Xu Haocheng Wang Haowei Zhang Honghui Ding Huajian Xin Huazuo Gao Hui Li Hui Qu J. L. Cai Jian Liang Jianzhong Guo Jiaqi Ni Jiashi Li Jiawei Wang Jin Chen Jingchang Chen Jingyang Yuan Junjie Qiu Junlong Li Junxiao Song Kai Dong Kai Hu Kaige Gao Kang Guan Kexin Huang Kuai Yu Lean Wang Lecong Zhang Lei Xu Leyi Xia Liang Zhao Litong Wang Liyue Zhang Meng Li Miaojun Wang Mingchuan Zhang Minghua Zhang Minghui Tang Mingming Li Ning Tian Panpan Huang Peiyi Wang Peng Zhang Qiancheng Wang Qihao Zhu Qinyu Chen Qiushi Du R. J. Chen R. L. Jin Ruiqi Ge Ruisong Zhang Ruizhe Pan Runji Wang Runxin Xu Ruoyu Zhang Ruyi Chen S. S. Li Shanghao Lu Shangyan Zhou Shanhuang Chen Shaoqing Wu Shengfeng Ye Shengfeng Ye Shirong Ma Shiyu Wang Shuang Zhou Shuiping Yu Shunfeng Zhou Shuting Pan T. Wang Tao Yun Tian Pei Tianyu Sun W. L. Xiao Wangding Zeng Wanjia Zhao Wei An Wen Liu Wenfeng Liang Wenjun Gao Wenqin Yu Wentao Zhang X. Q. Li Xiangyue Jin Xianzu Wang Xiao Bi Xiaodong Liu Xiaohan Wang Xiaojin Shen Xiaokang Chen Xiaokang Zhang Xiaosha Chen Xiaotao Nie Xiaowen Sun Xiaoxiang Wang Xin Cheng Xin Liu Xin Xie Xingchao Liu Xingkai Yu Xinnan Song Xinxia Shan Xinyi Zhou Xinyu Yang Xinyuan Li Xuecheng Su Xuheng Lin Y. K. Li Y. Q. Wang Y. X. Wei Y. X. Zhu Yang Zhang Yanhong Xu Yanhong Xu Yanping Huang Yao Li Yao Zhao Yaofeng Sun Yaohui Li Yaohui Wang Yi Yu Yi Zheng Yichao Zhang Yifan Shi Yiliang Xiong Ying He Ying Tang Yishi Piao Yisong Wang Yixuan Tan Yiyang Ma Yiyuan Liu Yongqiang Guo Yu Wu Yuan Ou Yuchen Zhu Yuduan Wang Yue Gong Yuheng Zou Yujia He Yukun Zha Yunfan Xiong Yunxian Ma Yuting Yan Yuxiang Luo Yuxiang You Yuxuan Liu Yuyang Zhou Z. F. Wu Z. Z. Ren Zehui Ren Zhangli Sha Zhe Fu Zhean Xu Zhen Huang Zhen Zhang Zhenda Xie Zhengyan Zhang Zhewen Hao Zhibin Gou Zhicheng Ma Zhigang Yan Zhihong Shao Zhipeng Xu Zhiyu Wu Zhongyu Zhang Zhuoshu Li Zihui Gu Zijia Zhu Zijun Liu Zilin Li Ziwei Xie Ziyang Song Ziyi Gao and Zizheng Pan. 2025b. DeepSeek-V3 Technical Report. arXiv:2412.19437 [cs.CL] https://arxiv.org/abs/2412.19437
[bib.bib12] DeepSeek-AI et al. (2025a) DeepSeek-AI Daya Guo Dejian Yang Haowei Zhang Junxiao Song Ruoyu Zhang Runxin Xu Qihao Zhu Shirong Ma Peiyi Wang Xiao Bi Xiaokang Zhang Xingkai Yu Yu Wu Z. F. Wu Zhibin Gou Zhihong Shao Zhuoshu Li Ziyi Gao Aixin Liu Bing Xue Bingxuan Wang Bochao Wu Bei Feng Chengda Lu Chenggang Zhao Chengqi Deng Chenyu Zhang Chong Ruan Damai Dai Deli Chen Dongjie Ji Erhang Li Fangyun Lin Fucong Dai Fuli Luo Guangbo Hao Guanting Chen Guowei Li H. Zhang Han Bao Hanwei Xu Haocheng Wang Honghui Ding Huajian Xin Huazuo Gao Hui Qu Hui Li Jianzhong Guo Jiashi Li Jiawei Wang Jingchang Chen Jingyang Yuan Junjie Qiu Junlong Li J. L. Cai Jiaqi Ni Jian Liang Jin Chen Kai Dong Kai Hu Kaige Gao Kang Guan Kexin Huang Kuai Yu Lean Wang Lecong Zhang Liang Zhao Litong Wang Liyue Zhang Lei Xu Leyi Xia Mingchuan Zhang Minghua Zhang Minghui Tang Meng Li Miaojun Wang Mingming Li Ning Tian Panpan Huang Peng Zhang Qiancheng Wang Qinyu Chen Qiushi Du Ruiqi Ge Ruisong Zhang Ruizhe Pan Runji Wang R. J. Chen R. L. Jin Ruyi Chen Shanghao Lu Shangyan Zhou Shanhuang Chen Shengfeng Ye Shiyu Wang Shuiping Yu Shunfeng Zhou Shuting Pan S. S. Li Shuang Zhou Shaoqing Wu Shengfeng Ye Tao Yun Tian Pei Tianyu Sun T. Wang Wangding Zeng Wanjia Zhao Wen Liu Wenfeng Liang Wenjun Gao Wenqin Yu Wentao Zhang W. L. Xiao Wei An Xiaodong Liu Xiaohan Wang Xiaokang Chen Xiaotao Nie Xin Cheng Xin Liu Xin Xie Xingchao Liu Xinyu Yang Xinyuan Li Xuecheng Su Xuheng Lin X. Q. Li Xiangyue Jin Xiaojin Shen Xiaosha Chen Xiaowen Sun Xiaoxiang Wang Xinnan Song Xinyi Zhou Xianzu Wang Xinxia Shan Y. K. Li Y. Q. Wang Y. X. Wei Yang Zhang Yanhong Xu Yao Li Yao Zhao Yaofeng Sun Yaohui Wang Yi Yu Yichao Zhang Yifan Shi Yiliang Xiong Ying He Yishi Piao Yisong Wang Yixuan Tan Yiyang Ma Yiyuan Liu Yongqiang Guo Yuan Ou Yuduan Wang Yue Gong Yuheng Zou Yujia He Yunfan Xiong Yuxiang Luo Yuxiang You Yuxuan Liu Yuyang Zhou Y. X. Zhu Yanhong Xu Yanping Huang Yaohui Li Yi Zheng Yuchen Zhu Yunxian Ma Ying Tang Yukun Zha Yuting Yan Z. Z. Ren Zehui Ren Zhangli Sha Zhe Fu Zhean Xu Zhenda Xie Zhengyan Zhang Zhewen Hao Zhicheng Ma Zhigang Yan Zhiyu Wu Zihui Gu Zijia Zhu Zijun Liu Zilin Li Ziwei Xie Ziyang Song Zizheng Pan Zhen Huang Zhipeng Xu Zhongyu Zhang and Zhen Zhang. 2025a. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. arXiv:2501.12948 [cs.CL] https://arxiv.org/abs/2501.12948
[bib.bib38] Mazeika et al. (2024) Mantas Mazeika Long Phan Xuwang Yin Andy Zou Zifan Wang Norman Mu Elham Sakhaee Nathaniel Li Steven Basart Bo Li David Forsyth and Dan Hendrycks. 2024. HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal. arXiv:2402.04249 [cs.LG] https://arxiv.org/abs/2402.04249
[bib.bib73] Xie et al. (2025) Tinghao Xie Xiangyu Qi Yi Zeng Yangsibo Huang Udari Madhushani Sehwag Kaixuan Huang Luxi He Boyi Wei Dacheng Li Ying Sheng Ruoxi Jia Bo Li Kai Li Danqi Chen Peter Henderson and Prateek Mittal. 2025. SORRY-Bench: Systematically Evaluating Large Language Model Safety Refusal. In The Thirteenth International Conference on Learning Representations. https://openreview.net/forum?id=YfKNaRktan
[bib.bib75] Xu et al. (2024b) Zihao Xu Yi Liu Gelei Deng Yuekang Li and Stjepan Picek. 2024b. A Comprehensive Study of Jailbreak Attack versus Defense for Large Language Models. In Findings of the Association for Computational Linguistics: ACL 2024 Lun-Wei Ku Andre Martins and Vivek Srikumar (Eds.). Association for Computational Linguistics Bangkok Thailand 7432–7449. doi:10.18653/v1/2024.findings-acl.443 [https://doi.org/10.18653/v1/2024.findings-acl.443]
[bib.bib59] Shi et al. (2024) Dan Shi Tianhao Shen Yufei Huang Zhigen Li Yongqi Leng Renren Jin Chuang Liu Xinwei Wu Zishan Guo Linhao Yu Ling Shi Bojian Jiang and Deyi Xiong. 2024. Large Language Model Safety: A Holistic Survey. arXiv:2412.17686 [cs.AI] https://arxiv.org/abs/2412.17686
[bib.bib35] Liu et al. (2024a) Yi Liu Gelei Deng Zhengzi Xu Yuekang Li Yaowen Zheng Ying Zhang Lida Zhao Tianwei Zhang Kailong Wang and Yang Liu. 2024a. Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study. arXiv:2305.13860 [cs.SE] https://arxiv.org/abs/2305.13860
[bib.bib7] Carlini et al. (2023) Nicholas Carlini Milad Nasr Christopher A. Choquette-Choo Matthew Jagielski Irena Gao Pang Wei Koh Daphne Ippolito Florian Tramèr and Ludwig Schmidt. 2023. Are aligned neural networks adversarially aligned?. In Thirty-seventh Conference on Neural Information Processing Systems. https://openreview.net/forum?id=OQQoD8Vc3B
[bib.bib84] Zou et al. (2023) Andy Zou Zifan Wang Nicholas Carlini Milad Nasr J. Zico Kolter and Matt Fredrikson. 2023. Universal and Transferable Adversarial Attacks on Aligned Language Models. arXiv:2307.15043 [cs.CL] https://arxiv.org/abs/2307.15043
[bib.bib34] Liu et al. (2024b) Xiaogeng Liu Nan Xu Muhao Chen and Chaowei Xiao. 2024b. AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models. In The Twelfth International Conference on Learning Representations. https://openreview.net/forum?id=7Jwpw4qKkb
[bib.bib3] Andriushchenko et al. (2025) Maksym Andriushchenko Francesco Croce and Nicolas Flammarion. 2025. Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks. In The Thirteenth International Conference on Learning Representations. https://openreview.net/forum?id=hXA8wqRdyV
[bib.bib74] Xu et al. (2024a) Zhangchen Xu Fengqing Jiang Luyao Niu Jinyuan Jia Bill Yuchen Lin and Radha Poovendran. 2024a. SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) Lun-Wei Ku Andre Martins and Vivek Srikumar (Eds.). Association for Computational Linguistics Bangkok Thailand 5587–5605. doi:10.18653/v1/2024.acl-long.303 [https://doi.org/10.18653/v1/2024.acl-long.303]
[bib.bib80] Zhang et al. (2024) Hangfan Zhang Zhimeng Guo Huaisheng Zhu Bochuan Cao Lu Lin Jinyuan Jia Jinghui Chen and Dinghao Wu. 2024. Jailbreak Open-Sourced Large Language Models via Enforced Decoding. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) Lun-Wei Ku Andre Martins and Vivek Srikumar (Eds.). Association for Computational Linguistics Bangkok Thailand 5475–5493. doi:10.18653/v1/2024.acl-long.299 [https://doi.org/10.18653/v1/2024.acl-long.299]
[bib.bib32] Li et al. (2024) Xirui Li Ruochen Wang Minhao Cheng Tianyi Zhou and Cho-Jui Hsieh. 2024. DrAttack: Prompt Decomposition and Reconstruction Makes Powerful LLMs Jailbreakers. In Findings of the Association for Computational Linguistics: EMNLP 2024 Yaser Al-Onaizan Mohit Bansal and Yun-Nung Chen (Eds.). Association for Computational Linguistics Miami Florida USA 13891–13913. doi:10.18653/v1/2024.findings-emnlp.813 [https://doi.org/10.18653/v1/2024.findings-emnlp.813]
[bib.bib50] Qi et al. (2025) Xiangyu Qi Ashwinee Panda Kaifeng Lyu Xiao Ma Subhrajit Roy Ahmad Beirami Prateek Mittal and Peter Henderson. 2025. Safety Alignment Should be Made More Than Just a Few Tokens Deep. In The Thirteenth International Conference on Learning Representations. https://openreview.net/forum?id=6Mxhg9PtDE
[bib.bib57] Shen et al. (2024) Xinyue Shen Zeyuan Chen Michael Backes Yun Shen and Yang Zhang. 2024. ”Do Anything Now”: Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models. In Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security (Salt Lake City UT USA) (CCS ’24). Association for Computing Machinery New York NY USA 1671–1685. doi:10.1145/3658644.3670388 [https://doi.org/10.1145/3658644.3670388]
[bib.bib79] Yu et al. (2024) Jiahao Yu Xingwei Lin Zheng Yu and Xinyu Xing. 2024. LLM-Fuzzer: Scaling Assessment of Large Language Model Jailbreaks. In 33rd USENIX Security Symposium (USENIX Security 24). USENIX Association Philadelphia PA 4657–4674. https://www.usenix.org/conference/usenixsecurity24/presentation/yu-jiahao
[bib.bib81] Zhang et al. (2025) Shenyi Zhang Yuchen Zhai Keyan Guo Hongxin Hu Shengnan Guo Zheng Fang Lingchen Zhao Chao Shen Cong Wang and Qian Wang. 2025. JBShield: Defending Large Language Models from Jailbreak Attacks through Activated Concept Analysis and Manipulation. arXiv:2502.07557 [cs.CR] https://arxiv.org/abs/2502.07557
[bib.bib14] Deng et al. (2024) Gelei Deng Yi Liu Yuekang Li Kailong Wang Ying Zhang Zefeng Li Haoyu Wang Tianwei Zhang and Yang Liu. 2024. MASTERKEY: Automated Jailbreaking of Large Language Model Chatbots. In NDSS. https://www.ndss-symposium.org/ndss-paper/masterkey-automated-jailbreaking-of-large-language-model-chatbots/
[bib.bib76] Yi et al. (2024) Sibo Yi Yule Liu Zhen Sun Tianshuo Cong Xinlei He Jiaxing Song Ke Xu and Qi Li. 2024. Jailbreak attacks and defenses against large language models: A survey. arXiv preprint arXiv:2407.04295 (2024).
[bib.bib26] Ji et al. (2023) Jiaming Ji Mickel Liu Josef Dai Xuehai Pan Chi Zhang Ce Bian Boyuan Chen Ruiyang Sun Yizhou Wang and Yaodong Yang. 2023. Beavertails: Towards improved safety alignment of llm via a human-preference dataset. Advances in Neural Information Processing Systems 36 (2023) 24678–24704.
[bib.bib25] Ji et al. (2024) Jiaming Ji Donghai Hong Borong Zhang Boyuan Chen Josef Dai Boren Zheng Tianyi Qiu Boxun Li and Yaodong Yang. 2024. Pku-saferlhf: A safety alignment preference dataset for llama family models. arXiv e-prints (2024) arXiv–2406.
[bib.bib82] Zhao et al. (2025) Xuandong Zhao Will Cai Tianneng Shi David Huang Licong Lin Song Mei and Dawn Song. 2025. Improving LLM Safety Alignment with Dual-Objective Optimization. arXiv preprint arXiv:2503.03710 (2025).
[bib.bib49] Qi et al. (2023) Xiangyu Qi Kaixuan Huang Ashwinee Panda Peter Henderson Mengdi Wang and Prateek Mittal. 2023. Visual Adversarial Examples Jailbreak Aligned Large Language Models. arXiv:2306.13213 [cs.CR] https://arxiv.org/abs/2306.13213
[bib.bib55] Ren et al. (2024) Qibing Ren Chang Gao Jing Shao Junchi Yan Xin Tan Wai Lam and Lizhuang Ma. 2024. CodeAttack: Revealing Safety Generalization Challenges of Large Language Models via Code Completion. In Findings of the Association for Computational Linguistics: ACL 2024 Lun-Wei Ku Andre Martins and Vivek Srikumar (Eds.). Association for Computational Linguistics Bangkok Thailand 11437–11452. doi:10.18653/v1/2024.findings-acl.679 [https://doi.org/10.18653/v1/2024.findings-acl.679]
[bib.bib69] Wei et al. (2023) Alexander Wei Nika Haghtalab and Jacob Steinhardt. 2023. Jailbroken: How Does LLM Safety Training Fail?. In Thirty-seventh Conference on Neural Information Processing Systems. https://openreview.net/forum?id=jA235JGM09
[bib.bib22] Huang et al. (2024a) Yangsibo Huang Samyak Gupta Mengzhou Xia Kai Li and Danqi Chen. 2024a. Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation. In The Twelfth International Conference on Learning Representations. https://openreview.net/forum?id=r42tSSCHPh
[bib.bib68] Wang et al. (2023) Han Wang Ming Shan Hee Md Rabiul Awal Kenny Tsu Wei Choo and Roy Ka-Wei Lee. 2023. Evaluating GPT-3 generated explanations for hateful content moderation. In Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence (Macao P.R.China) (IJCAI ’23). Article 694 9 pages. doi:10.24963/ijcai.2023/694 [https://doi.org/10.24963/ijcai.2023/694]
[bib.bib24] Inan et al. (2023) Hakan Inan Kartikeya Upasani Jianfeng Chi Rashi Rungta Krithika Iyer Yuning Mao Michael Tontchev Qing Hu Brian Fuller Davide Testuggine and Madian Khabsa. 2023. Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations. arXiv:2312.06674 [cs.CL] https://arxiv.org/abs/2312.06674
[bib.bib71] Wu et al. (2024b) Jialin Wu Jiangyi Deng Shengyuan Pang Yanjiao Chen Jiayang Xu Xinfeng Li and Wenyuan Xu. 2024b. Legilimens: Practical and Unified Content Moderation for Large Language Model Services. In Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security (Salt Lake City UT USA) (CCS ’24). Association for Computing Machinery New York NY USA 1151–1165. doi:10.1145/3658644.3690322 [https://doi.org/10.1145/3658644.3690322]
[bib.bib27] Jigsaw and Google Counter Abuse Technology (2017) Jigsaw and Google Counter Abuse Technology. 2017. Perspective API. https://www.perspectiveapi.com/. Accessed: March 31 2025.
[bib.bib37] Markov et al. (2023) Todor Markov Chong Zhang Sandhini Agarwal Florentine Eloundou Nekoul Theodore Lee Steven Adler Angela Jiang and Lilian Weng. 2023. A holistic approach to undesired content detection in the real world. In Proceedings of the Thirty-Seventh AAAI Conference on Artificial Intelligence and Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence and Thirteenth Symposium on Educational Advances in Artificial Intelligence (AAAI’23/IAAI’23/EAAI’23). AAAI Press Article 1683 10 pages. doi:10.1609/aaai.v37i12.26752 [https://doi.org/10.1609/aaai.v37i12.26752]
[bib.bib36] Mai et al. (2025) Wuyuao Mai Geng Hong Pei Chen Xudong Pan Baojun Liu Yuan Zhang Haixin Duan and Min Yang. 2025. You Can’t Eat Your Cake and Have It Too: The Performance Degradation of LLMs with Jailbreak Defense. In THE WEB CONFERENCE 2025. https://openreview.net/forum?id=ETyLTCkvfT
[bib.bib47] OpenAI (2025b) OpenAI. 2025b. Structured Outputs. https://platform.openai.com/docs/guides/structured-outputs. Accessed: March 2025.
[bib.bib67] vLLM Team (2025) vLLM Team. 2025. Structured Output Generation with vLLM. https://docs.vllm.ai/en/latest/serving/structured_outputs.html. Accessed: March 2025.
[bib.bib70] Willard and Louf (2023) Brandon T. Willard and Rémi Louf. 2023. Efficient Guided Generation for Large Language Models. arXiv:2307.09702 [cs.CL] https://arxiv.org/abs/2307.09702
[bib.bib16] Dong et al. (2024) Yixin Dong Charlie F. Ruan Yaxing Cai Ruihang Lai Ziyi Xu Yilong Zhao and Tianqi Chen. 2024. XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models. arXiv:2411.15100 [cs.CL] https://arxiv.org/abs/2411.15100
[bib.bib5] Ayala and Bechard (2024) Orlando Ayala and Patrice Bechard. 2024. Reducing hallucination in structured outputs via Retrieval-Augmented Generation. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 6: Industry Track) Yi Yang Aida Davani Avi Sil and Anoop Kumar (Eds.). Association for Computational Linguistics Mexico City Mexico 228–238. doi:10.18653/v1/2024.naacl-industry.19 [https://doi.org/10.18653/v1/2024.naacl-industry.19]
[bib.bib19] Geng et al. (2025) Saibo Geng Hudson Cooper Michał Moskal Samuel Jenkins Julian Berman Nathan Ranchin Robert West Eric Horvitz and Harsha Nori. 2025. JSONSchemaBench: A Rigorous Benchmark of Structured Outputs for Language Models. arXiv:2501.10868 [cs.CL] https://arxiv.org/abs/2501.10868
[bib.bib43] Netz et al. (2024) Lukas Netz Jan Reimer and Bernhard Rumpe. 2024. Using Grammar Masking to Ensure Syntactic Validity in LLM-based Modeling Tasks. In ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems.
[bib.bib58] Shen (2024) Zhuocheng Shen. 2024. LLM With Tools: A Survey. arXiv:2409.18807 [cs.AI] https://arxiv.org/abs/2409.18807
[bib.bib51] Qu et al. (2025) Changle Qu Sunhao Dai Xiaochi Wei Hengyi Cai Shuaiqiang Wang Dawei Yin Jun Xu and Ji-rong Wen. 2025. Tool learning with large language models: a survey. Frontiers of Computer Science 19 8 (2025).
[bib.bib60] Souly et al. (2024) Alexandra Souly Qingyuan Lu Dillon Bowen Tu Trinh Elvis Hsieh Sana Pandey Pieter Abbeel Justin Svegliato Scott Emmons Olivia Watkins and Sam Toyer. 2024. A StrongREJECT for Empty Jailbreaks. In ICLR 2024 Workshop on Reliable and Responsible Foundation Models. https://openreview.net/forum?id=al303JJkGO
[bib.bib9] Chao et al. (2024a) Patrick Chao Edoardo Debenedetti Alexander Robey Maksym Andriushchenko Francesco Croce Vikash Sehwag Edgar Dobriban Nicolas Flammarion George J. Pappas Florian Tramèr Hamed Hassani and Eric Wong. 2024a. JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track. https://openreview.net/forum?id=urjPCYZt0I