2.1. Jailbreak Attacks on LLMs
Jailbreak attacks are designed to create malicious inputs that prompt target LLMs to generate outputs that violate predefined safety or ethical guidelines. Carlini et al.~\cite{bib.bib7} first suggested that improved NLP adversarial attacks could achieve jailbreaking on aligned LLMs and encouraged further research in this area. Since then various jailbreak attack methods have emerged. We adopt the categorization of jailbreak attacks proposed by ~\cite{bib.bib81} which has divided jailbreak attacks into the following five categories: manual-designed optimization-based template-based linguistics-based and encoding-based. As depicted in Table1 we extend this taxonomy by adding a new dimension: output-based attacks which are closely related to our work.
 Manual-designed Jailbreaks represent the most straightforward approach where human designers craft malicious inputs to elicit undesirable outputs from LLMs. The most notable examples are the In-the-wild Jailbreak Prompts (IJP)~\cite{bib.bib57} which document real-world jailbreak attempts observed in actual deployments and shared across social media platforms.
 Optimization-based Jailbreaks leverage automated algorithms to exploit the internal gradients of LLMs for crafting malicious soft prompts. GCG~\cite{bib.bib84} employs a greedy algorithm to modify input prompts by adding an adversarial suffix prompting the LLM to start its response with “Sure” as its optimization goal. SAA~\cite{bib.bib3} building on GCG combining hand-crafted templates with random search strategy to find adversarial suffixes. These attacks are automated but they require white-box access to the model or at least the logit probabilities access besides their adversarial processes require a large number of queries to the target LLM.
 Template-based Jailbreaks generate jailbreak prompts by optimizing sophisticated templates and embedding the original harmful requests within them. MasterKey~\cite{bib.bib14} trains a jailbreak-oriented LLM to generate adversarial inputs. LLMFuzzer~\cite{bib.bib79} use an LLM to mutate human-written templates into new jailbreak prompts. AutoDAN~\cite{bib.bib34} applies hierarchical genetic algorithm for both sentence and word level jailbreak prompts optimizations. PAIR~\cite{bib.bib10} and TAP~\cite{bib.bib39} use similar idea to adopt an attacker LLM to generate/mutate jailbreak prompts and an evaluator LLM to score the generated prompts to enable refinement and pruning. StructTransform~\cite{bib.bib78} is a concurrent work which also targets the structured generation as attack surface it studies the vulnerability within structured style generation which aligns with our findings.
 Linguistics-based Jailbreaks exploit linguistic properties to conceal malicious intentions within seemingly benign inputs also known as indirect jailbreaks. DrAttack~\cite{bib.bib32} decomposes and reconstructs malicious prompts embedding the intent within the reassembled context to evade detection. Puzzler~\cite{bib.bib8} applies combinations of diverse clues to bypass LLM’s safety alignment mechanisms.
 Encoding-based Jailbreaks translate malicious prompts into less commonly used languages or encoding formats that may not be well-aligned thereby bypassing LLM security measures. Zulu~\cite{bib.bib77} encodes malicious prompts into low-resource languages while Base64~\cite{bib.bib69} encodes malicious prompts in Base64 format to obfuscate the intent.
 Output-based Jailbreaks is an emerging category that uses completely different methods to jailbreak LLMs. EnDec~\cite{bib.bib80} manipulates white-box LLMs’ logits to directly perform enforced decoding to construct desired output like ”Sure” in the beginning. APT~\cite{bib.bib33} is a concurrent work which uses the GuidedRegex feature in the structured output API to eventually build up a prefix tree where refusal tokens are naturally banned from selection through regular expression constraints.
[TABLE START]<table>
	<tr>
		<td>Categories</td>
		<td>Jailbreaks</td>
		<td>Extra Assist</td>
		<td>White/Black box</td>
		<td>Target LLM Queries</td>
		<td>I/O-Based</td>
	</tr>
	<tr>
		<td>Manually-designed</td>
		<td>IJP~\cite{bib.bib57}</td>
		<td>Human</td>
		<td>\bullet</td>
		<td>-</td>
		<td>Input</td>
	</tr>
	<tr>
		<td rowspan="2">Optimization-based</td>
		<td>GCG~\cite{bib.bib84}</td>
		<td>-</td>
		<td>\circ</td>
		<td>\sim2K</td>
		<td>Input</td>
	</tr>
	<tr>
		<td>SAA~\cite{bib.bib3}</td>
		<td>-</td>
		<td>\circ</td>
		<td>\sim10K</td>
		<td>Input</td>
	</tr>
	<tr>
		<td rowspan="6">Template-based</td>
		<td>MasterKey~\cite{bib.bib14}</td>
		<td>LLM</td>
		<td>\bullet</td>
		<td>\sim200</td>
		<td>Input</td>
	</tr>
	<tr>
		<td>LLMFuzzer~\cite{bib.bib79}</td>
		<td>LLM</td>
		<td>\bullet</td>
		<td>\sim500</td>
		<td>Input</td>
	</tr>
	<tr>
		<td>AutoDAN~\cite{bib.bib34}</td>
		<td>LLM</td>
		<td>\circ</td>
		<td>\sim200</td>
		<td>Input</td>
	</tr>
	<tr>
		<td>PAIR~\cite{bib.bib10}</td>
		<td>LLM</td>
		<td>\bullet</td>
		<td>\sim20</td>
		<td>Input</td>
	</tr>
	<tr>
		<td>TAP~\cite{bib.bib39}</td>
		<td>LLM</td>
		<td>\bullet</td>
		<td>\sim20</td>
		<td>Input</td>
	</tr>
	<tr>
		<td>StructTransform~\cite{bib.bib78}</td>
		<td>LLM</td>
		<td>\bullet</td>
		<td>\sim3</td>
		<td>Input</td>
	</tr>
	<tr>
		<td rowspan="2">Linguistics-based</td>
		<td>DrAttack~\cite{bib.bib32}</td>
		<td>LLM</td>
		<td>\bullet</td>
		<td>\sim10</td>
		<td>Input</td>
	</tr>
	<tr>
		<td>Puzzler~\cite{bib.bib8}</td>
		<td>LLM</td>
		<td>\bullet</td>
		<td>-</td>
		<td>Input</td>
	</tr>
	<tr>
		<td rowspan="2">Encoding-based</td>
		<td>Zulu~\cite{bib.bib77}</td>
		<td>-</td>
		<td>\bullet</td>
		<td>-</td>
		<td>Input</td>
	</tr>
	<tr>
		<td>Base64~\cite{bib.bib69}</td>
		<td>-</td>
		<td>\bullet</td>
		<td>-</td>
		<td>Input</td>
	</tr>
	<tr>
		<td rowspan="5">Output-based</td>
		<td>EnDec~\cite{bib.bib80}</td>
		<td>-</td>
		<td>\circ</td>
		<td>-</td>
		<td>Output</td>
	</tr>
	<tr>
		<td>APT~\cite{bib.bib33}</td>
		<td>eval LLM</td>
		<td>\bullet</td>
		<td>O(output len)</td>
		<td>Output</td>
	</tr>
	<tr>
		<td>EnumAttack(ours)</td>
		<td>-</td>
		<td>\bullet</td>
		<td>\sim1</td>
		<td>Output</td>
	</tr>
	<tr>
		<td>ChainEnumAttack(ours)</td>
		<td>weak LLM</td>
		<td>\bullet</td>
		<td>\sim1</td>
		<td>Output</td>
	</tr>
	<tr>
		<td>BenignEnumAttack(ours)</td>
		<td>-</td>
		<td>\bullet</td>
		<td>\sim1</td>
		<td>Output</td>
	</tr>
</table>
Table 1. 
Summary of existing jailbreak attacks adapted from  ~\cite{bib.bib81}, - indicates the method does not use the listed resource or lacks that capability, \circ denotes white-box attack and \bullet denotes black-box attack.[TABLE END]



## Section References
[bib.bib7] Carlini et al. (2023) Nicholas Carlini Milad Nasr Christopher A. Choquette-Choo Matthew Jagielski Irena Gao Pang Wei Koh Daphne Ippolito Florian Tramèr and Ludwig Schmidt. 2023. Are aligned neural networks adversarially aligned?. In Thirty-seventh Conference on Neural Information Processing Systems. https://openreview.net/forum?id=OQQoD8Vc3B
[bib.bib81] Zhang et al. (2025) Shenyi Zhang Yuchen Zhai Keyan Guo Hongxin Hu Shengnan Guo Zheng Fang Lingchen Zhao Chao Shen Cong Wang and Qian Wang. 2025. JBShield: Defending Large Language Models from Jailbreak Attacks through Activated Concept Analysis and Manipulation. arXiv:2502.07557 [cs.CR] https://arxiv.org/abs/2502.07557
[bib.bib57] Shen et al. (2024) Xinyue Shen Zeyuan Chen Michael Backes Yun Shen and Yang Zhang. 2024. ”Do Anything Now”: Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models. In Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security (Salt Lake City UT USA) (CCS ’24). Association for Computing Machinery New York NY USA 1671–1685. doi:10.1145/3658644.3670388 [https://doi.org/10.1145/3658644.3670388]
[bib.bib84] Zou et al. (2023) Andy Zou Zifan Wang Nicholas Carlini Milad Nasr J. Zico Kolter and Matt Fredrikson. 2023. Universal and Transferable Adversarial Attacks on Aligned Language Models. arXiv:2307.15043 [cs.CL] https://arxiv.org/abs/2307.15043
[bib.bib3] Andriushchenko et al. (2025) Maksym Andriushchenko Francesco Croce and Nicolas Flammarion. 2025. Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks. In The Thirteenth International Conference on Learning Representations. https://openreview.net/forum?id=hXA8wqRdyV
[bib.bib14] Deng et al. (2024) Gelei Deng Yi Liu Yuekang Li Kailong Wang Ying Zhang Zefeng Li Haoyu Wang Tianwei Zhang and Yang Liu. 2024. MASTERKEY: Automated Jailbreaking of Large Language Model Chatbots. In NDSS. https://www.ndss-symposium.org/ndss-paper/masterkey-automated-jailbreaking-of-large-language-model-chatbots/
[bib.bib79] Yu et al. (2024) Jiahao Yu Xingwei Lin Zheng Yu and Xinyu Xing. 2024. LLM-Fuzzer: Scaling Assessment of Large Language Model Jailbreaks. In 33rd USENIX Security Symposium (USENIX Security 24). USENIX Association Philadelphia PA 4657–4674. https://www.usenix.org/conference/usenixsecurity24/presentation/yu-jiahao
[bib.bib34] Liu et al. (2024b) Xiaogeng Liu Nan Xu Muhao Chen and Chaowei Xiao. 2024b. AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models. In The Twelfth International Conference on Learning Representations. https://openreview.net/forum?id=7Jwpw4qKkb
[bib.bib10] Chao et al. (2024b) Patrick Chao Alexander Robey Edgar Dobriban Hamed Hassani George J. Pappas and Eric Wong. 2024b. Jailbreaking Black Box Large Language Models in Twenty Queries. arXiv:2310.08419 [cs.LG] https://arxiv.org/abs/2310.08419
[bib.bib39] Mehrotra et al. (2024) Anay Mehrotra Manolis Zampetakis Paul Kassianik Blaine Nelson Hyrum S Anderson Yaron Singer and Amin Karbasi. 2024. Tree of Attacks: Jailbreaking Black-Box LLMs Automatically. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. https://openreview.net/forum?id=SoM3vngOH5
[bib.bib78] Yoosuf et al. (2025) Shehel Yoosuf Temoor Ali Ahmed Lekssays Mashael AlSabah and Issa Khalil. 2025. StructTransform: A Scalable Attack Surface for Safety-Aligned Large Language Models. arXiv:2502.11853 [cs.LG] https://arxiv.org/abs/2502.11853
[bib.bib32] Li et al. (2024) Xirui Li Ruochen Wang Minhao Cheng Tianyi Zhou and Cho-Jui Hsieh. 2024. DrAttack: Prompt Decomposition and Reconstruction Makes Powerful LLMs Jailbreakers. In Findings of the Association for Computational Linguistics: EMNLP 2024 Yaser Al-Onaizan Mohit Bansal and Yun-Nung Chen (Eds.). Association for Computational Linguistics Miami Florida USA 13891–13913. doi:10.18653/v1/2024.findings-emnlp.813 [https://doi.org/10.18653/v1/2024.findings-emnlp.813]
[bib.bib8] Chang et al. (2024) Zhiyuan Chang Mingyang Li Yi Liu Junjie Wang Qing Wang and Yang Liu. 2024. Play Guessing Game with LLM: Indirect Jailbreak Attack with Implicit Clues. In Findings of the Association for Computational Linguistics: ACL 2024 Lun-Wei Ku Andre Martins and Vivek Srikumar (Eds.). Association for Computational Linguistics Bangkok Thailand 5135–5147. doi:10.18653/v1/2024.findings-acl.304 [https://doi.org/10.18653/v1/2024.findings-acl.304]
[bib.bib77] Yong et al. (2024) Zheng-Xin Yong Cristina Menghini and Stephen H. Bach. 2024. Low-Resource Languages Jailbreak GPT-4. arXiv:2310.02446 [cs.CL] https://arxiv.org/abs/2310.02446
[bib.bib69] Wei et al. (2023) Alexander Wei Nika Haghtalab and Jacob Steinhardt. 2023. Jailbroken: How Does LLM Safety Training Fail?. In Thirty-seventh Conference on Neural Information Processing Systems. https://openreview.net/forum?id=jA235JGM09
[bib.bib80] Zhang et al. (2024) Hangfan Zhang Zhimeng Guo Huaisheng Zhu Bochuan Cao Lu Lin Jinyuan Jia Jinghui Chen and Dinghao Wu. 2024. Jailbreak Open-Sourced Large Language Models via Enforced Decoding. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) Lun-Wei Ku Andre Martins and Vivek Srikumar (Eds.). Association for Computational Linguistics Bangkok Thailand 5475–5493. doi:10.18653/v1/2024.acl-long.299 [https://doi.org/10.18653/v1/2024.acl-long.299]
[bib.bib33] Li et al. (2025) Yanzeng Li Yunfan Xiong Jialun Zhong Jinchao Zhang Jie Zhou and Lei Zou. 2025. Exploiting Prefix-Tree in Structured Output Interfaces for Enhancing Jailbreak Attacking. arXiv:2502.13527 [cs.CR] https://arxiv.org/abs/2502.13527