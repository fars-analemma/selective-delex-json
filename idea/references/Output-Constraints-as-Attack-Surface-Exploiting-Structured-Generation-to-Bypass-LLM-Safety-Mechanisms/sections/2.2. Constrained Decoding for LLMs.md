2.2. Constrained Decoding for LLMs
Constrained decoding is a technique that guides LLM generation by incorporating grammar-based constraints into the decoding process. As depicted in Figure2 in addition to normal LLM generation process a grammar rule is applied to guide the generation process. It first goes through a similar lexer-parser workflow in compiler design where tokenization is treated as a lexer process using LLM tokenizer rules then the grammar rule is applied to existing tokens as a parsing process generating a per-token mask vector. Tokens that do not match the grammar rule are masked out modifying their logits to -\infty (in practice -100 is used in PyTorch-based implementation) so that their probabilities will be zero. Then a standard multinomial sampling process is followed to generate the next token ensuring the output is constrained to the grammar rule.
 Formally we characterize the constrained decoding process as follows.
 Problem Setup We consider a LLM f which maps a sequence of input tokens x_{1:n} to the logits vector of next token z_{n+1}\in\{R}^{|V|} where V is the vocabulary set of tokens and z_{n+1}[i]\in\{R} represents the logits value for the token with index i in V formally: z_{n+1}=f(x_{1:n})
 The logits values are transformed into a probability distribution using the softmax function usually normalized by a temperature parameter T then LLM utilizes a multinomial sampling process to generate the next token x_{n+1} choosing next token based on the normalized probabilities with configurable parameters like T top_p and top_k etc. x_{n+1}\sim p(x_{n+1}[i]\mid x_{1:n})=softmax(\frac{z_{n+1}[i]}{T})=\frac{e^{\frac{z_{n+1}[i]}{T}}}{\sum_{j=1}^{|V|}{e^{\frac{z_{n+1}[j]}{T}}}}
 Constrained Decoding Given a context-free grammar (CFG) G and a sequence of tokens x_{1:n} constrained decoding aims to generate the next token x_{n+1} that satisfies the grammar rule. Similar to compiler principles a context-free grammar rule G creates both a lexer L and a parser P. In the LLM context L corresponds directly to the LLM tokenizer as we already operate on tokenstreams rather than raw characters. The parser P could be implemented in multiple ways like LL(1) LR(1) etc. In general Pushdown Automata (PDA) are typically used to recognize languages generated by CFGs as they employ a stack to manage nested structures. Assuming we have an automaton A generated from grammar G which processes the current tokenstream x_{1:n} and produces a token-level mask m_{n+1}. For each token i\in V if there exists a valid transition in A for i then m_{n+1}[i]=1 otherwise m_{n+1}[i]=0. This mask is applied to the logits z_{n+1} to prevent invalid tokens from being generated: \hat{z}_{n+1}[i]=\begin{cases}z_{n+1}[i]&\text{if }m_{n+1}[i]=1\\ -\infty&\text{if }m_{n+1}[i]=0\end{cases}
 The masked logits \hat{z}_{n+1} are then used just as in Equation2[ref_id]S2.E2: x_{n+1}\sim p(x_{n+1}\mid x_{1:n})=\frac{e^{\frac{\hat{z}_{n+1}[i]}{T}}}{\sum_{j=1}^{|V|}e^{\frac{\hat{z}_{n+1}[j]}{T}}}
 \Description
 Constrained decoding illustration
 \Description
 A simplified JSON grammar example for mask generation
 Mask Generation We use a simple example to illustrate how the mask is generated in the above process. Note that the process is simplified and the actual implementation could be more complex and faces different challenges. As depicted in Figure3 JSON grammar typically handles key-value pairs where key must be a string and value could be either string number boolean array or other non-recursive JSON objects made up of key-value pairs we use a simplified JSON grammar to illustrate the mask generation process.
 Consider a tokenstream x_{1:n} = {”Hello . Based on the JSON grammar the parser currently resides in a state forming a STRING value. Consequently the acceptable token space becomes a strict subset of vocabulary V containing only tokens that can legally continue or terminate the string. This constraint derives from the application of these grammar rules: \{pair}::=\{STRING}\;\{:}\;\{value} \{STRING}::=\{"}\;(^{\prime}\{\textbackslash\textbackslash}^{\prime}\cdot\mid\sim[\{"}\{\textbackslash\textbackslash\textbackslash r \textbackslash n}])\text{*}\;\{"}
 As illustrated in Figure3 current tokenstream will set the automaton A to the state of forming a string executing rules of Equation6[ref_id]S2.E6 the rules are to accept either an escape sequence (’\\’ with any single follow-up character) or any character except quotation marks (’”’) backslashes (’\\’) carriage returns (’\r’) and newlines (’\n’) and the automaton will transit to the next state when encountering a closing quotation mark (’”’) which terminates the string and continues with the rules in pair.
 Consequently when generating tokens after ’”Hello’ the valid continuations are either terminating the string (with ’”’) or extending it with additional characters. Tokens beginning with \r or \n are masked out preventing their generation. Upon encountering ’”’ the automaton transitions back to the rule in Equation5[ref_id]S2.E5 exiting the STRING state. In this subsequent state the parser constrains the next token exclusively to a colon (’:’) after which the automaton advances to the value component generation.
 By applying such mask generation process the output of LLM is constrained to the grammar rule ensuring the output is valid and interpretable in the context of the grammar rule.
 Implementation Approaches To achieve effective constrained decoding implementation Outlines~\cite{bib.bib70} and SynCode~\cite{bib.bib65} utilize a lexer and parser to handle output and generate the token mask but they suffer from boundary mismatch problem raised by ~\cite{bib.bib29} as character-level PDA and token-level PDA have a large gap to fix. Synchromesh~\cite{bib.bib48} and llama.cpp~\cite{bib.bib20} use runtime checking for all tokens in their implementations which leads to significant overhead. XGrammar~\cite{bib.bib16} is current the state-of-the-art implementation of constrained decoding utilizing system optimizations to reduce runtime check via context-independent caching and it also enables co-optimizations to enable end-to-end LLM inference speedup in structured generation settings. By co-working with various LLM serving engines~\cite{bib.bib30 bib.bib83 bib.bib42} constrained-decoding techniques have been widely adapted in real-world applications to support structured output like Guided Choice Guided Regex Guided JSON and Guided Grammar etc.
 Proprietary solutions from providers like OpenAI~\cite{bib.bib2} and Genimi~\cite{bib.bib18} have also adapted similar techniques to support structured output generation but the details are not publicly available. Nevertheless as long as their APIs are open to the public with structured output support we can use their services to generate structured outputs. Recent work ~\cite{bib.bib19} evaluates the performance and quality of existing APIs and frameworks providing a comprehensive comparison of their structured output generation capabilities and limitations.
[IMAGE START] [IMAGE URL: /mnt/bmcpfs-29000zjpjtl6xjmjiifyk/xkhu/ideation_workspace/papers/reference_figures/101db279bb4eda2b9cf983ef13474e21.png] Figure 2. Constrained decoding illustration. The per-token mask is generated like traditional lexer-parser workflow in compiler design, where prior output is treated as tokenstream and matches the grammar rule as a parsing process, generating the mask, then the mask is applied to the LLM generation process, ensuring the output is constrained to grammar rules.[IMAGE END]

[IMAGE START] [IMAGE URL: /mnt/bmcpfs-29000zjpjtl6xjmjiifyk/xkhu/ideation_workspace/papers/reference_figures/e2b590f4fa5d00c4232d0f9e9042b5f1.png] Figure 3. JSON Grammar-Based Mask Generation. This illustrates how a constrained decoding system applies grammar rules to the current tokenstream, creating a binary mask over the vocabulary. The mask identifies valid tokens (which can advance the parsing automaton) versus invalid tokens (which are prohibited in the current grammatical state).[IMAGE END]



## Section References
[bib.bib70] Willard and Louf (2023) Brandon T. Willard and Rémi Louf. 2023. Efficient Guided Generation for Large Language Models. arXiv:2307.09702 [cs.CL] https://arxiv.org/abs/2307.09702
[bib.bib65] Ugare et al. (2024) Shubham Ugare Tarun Suresh Hangoo Kang Sasa Misailovic and Gagandeep Singh. 2024. SynCode: LLM Generation with Grammar Augmentation. arXiv:2403.01632 [cs.LG] https://arxiv.org/abs/2403.01632
[bib.bib29] Koo et al. (2024) Terry Koo Frederick Liu and Luheng He. 2024. Automata-based constraints for language model decoding. In First Conference on Language Modeling. https://openreview.net/forum?id=BDBdblmyzY
[bib.bib48] Poesia et al. (2022) Gabriel Poesia Oleksandr Polozov Vu Le Ashish Tiwari Gustavo Soares Christopher Meek and Sumit Gulwani. 2022. Synchromesh: Reliable code generation from pre-trained language models. arXiv:2201.11227 [cs.LG] https://arxiv.org/abs/2201.11227
[bib.bib20] Gerganov (2023) Georgi Gerganov. 2023. llama.cpp: LLM inference in C/C++. https://github.com/ggml-org/llama.cpp. Started development in March 2023.
[bib.bib16] Dong et al. (2024) Yixin Dong Charlie F. Ruan Yaxing Cai Ruihang Lai Ziyi Xu Yilong Zhao and Tianqi Chen. 2024. XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models. arXiv:2411.15100 [cs.CL] https://arxiv.org/abs/2411.15100
[bib.bib30] Kwon et al. (2023) Woosuk Kwon Zhuohan Li Siyuan Zhuang Ying Sheng Lianmin Zheng Cody Hao Yu Joseph Gonzalez Hao Zhang and Ion Stoica. 2023. Efficient Memory Management for Large Language Model Serving with PagedAttention. In Proceedings of the 29th Symposium on Operating Systems Principles (Koblenz Germany) (SOSP ’23). Association for Computing Machinery New York NY USA 611–626. doi:10.1145/3600006.3613165 [https://doi.org/10.1145/3600006.3613165]
[bib.bib83] Zheng et al. (2024) Lianmin Zheng Liangsheng Yin Zhiqiang Xie Chuyue Sun Jeff Huang Cody Hao Yu Shiyi Cao Christos Kozyrakis Ion Stoica Joseph E. Gonzalez Clark Barrett and Ying Sheng. 2024. SGLang: Efficient Execution of Structured Language Model Programs. arXiv:2312.07104 [cs.AI] https://arxiv.org/abs/2312.07104
[bib.bib42] ModelTC (2025) ModelTC. 2025. LightLLM: A Python-based LLM inference and serving framework. https://github.com/ModelTC/lightllm. Latest version 1.0.0 released in February 2025.
[bib.bib2] Achiam et al. (2023) Josh Achiam Steven Adler Sandhini Agarwal Lama Ahmad Ilge Akkaya Florencia Leoni Aleman Diogo Almeida Janko Altenschmidt Sam Altman Shyamal Anadkat et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023).
[bib.bib18] Gemini Team Google (2023) Gemini Team Google. 2023. Gemini: A Family of Highly Capable Multimodal Models. arXiv preprint arXiv:2312.11805 (2023).
[bib.bib19] Geng et al. (2025) Saibo Geng Hudson Cooper Michał Moskal Samuel Jenkins Julian Berman Nathan Ranchin Robert West Eric Horvitz and Harsha Nori. 2025. JSONSchemaBench: A Rigorous Benchmark of Structured Outputs for Language Models. arXiv:2501.10868 [cs.CL] https://arxiv.org/abs/2501.10868