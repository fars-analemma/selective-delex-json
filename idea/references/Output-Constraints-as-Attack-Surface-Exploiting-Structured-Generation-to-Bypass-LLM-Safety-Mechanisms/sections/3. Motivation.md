3. Motivation
\Description
 Direct jailbreak attempts to GPT-4o model evaluating on AdvBench.
 We first examine existing prompt-based attacks on GPT-4o model where we use AdvBench~\cite{bib.bib84} as the attack prompts. We find that the model is well protected by their defenses and the attack success rate is very low (1.1%). As depicted in Figure4 most of the refusal sequence generated by GPT-4o is quite short and deterministic by our guess an external safety guardrail may triggers the refusal stops the actual generation and return that short refusal response. In rare case the external guardrail fails then the GPT-4o model itself may sometimes refuses the question using its own alignment preference but sometimes it just answers showing jailbreak behaviors.
 Given these robust defenses against traditional input-based attacks we shifted our focus toward output-side vulnerabilities. Specifically we identified structured output capabilities as a promising attack surface that could potentially circumvent existing safety mechanisms by manipulating the decoding process itself rather than relying solely on prompt engineering.
 The primary structured output methods currently available in LLM systems include the following:
 \Description
 Illustration of multiple structured output features
 GuidedChoice. GuidedChoice restricts model output to selections from predefined choices useful for multiple-choice questions and classification tasks.
 GuidedRegex. GuidedRegex requires output to match regular expressions. Research has shown this feature can be exploited to generate malicious content through tree-based search techniques~\cite{bib.bib33}.
 GuidedJSON. GuidedJSON initially ensured valid JSON outputs now provides robust JSON Schema support with predefined structures and fields. This feature is particularly valuable for LLM-powered agent systems interfacing with existing software.
 GuidedGrammar. GuidedGrammar Requires output to conform to a context-free grammar essential for tasks like code generation. This more generalized approach encompasses all previous structured output types. While not widely supported by all vendors open-source communities like vllm~\cite{bib.bib30} and sglang~\cite{bib.bib83} offer GuidedGrammar support through backends such as Outlines~\cite{bib.bib70} and XGrammar~\cite{bib.bib16}.
 Structured output has become essential for developers integrating LLMs into existing software ecosystems. These capabilities facilitate reliable function calling API interactions and external integrations where format adherence is critical. Studies indicate structured output can reduce hallucinations in model responses~\cite{bib.bib5}.
 However while offering unprecedented control and reliability these features also introduce potential security considerations if not properly implemented. The fundamental power of constrained decoding—its ability to precisely manipulate an LLM’s output space—creates a double-edged sword. If misused this mechanism could potentially circumvent safety guardrails that would normally prevent harmful outputs. Constrained decoding could present security implications as it shifts control from the model’s internal safety alignment to external grammar specifications that dictate the decoding process.
 \Description
 The figure illustrates how LLM decoding works and combining with a grammar-guided mask to generate constrained text.
 Traditional jailbreak approaches~\cite{bib.bib84 bib.bib3} focus on crafting prompts that elicit affirmative or harmful response prefixes from LLMs. With constrained decoding similar objectives could potentially be achieved through different mechanisms. Rather than complex prompt engineering an attacker might define grammar constraints that influence the model to produce content that bypasses safety mechanisms. As shown in Figure6 the guided decoding process can control the model’s output within a user-defined token space which could be misused if not properly secured. The extra control plane of constrained decoding allows malicious attackers to hide their intentions within the grammar constraints which is not exposed to LLM itself.
 To better understand the underlying token distribution dynamics we use the Phi-3.5-MoE model~\cite{bib.bib21} for illustration and we also use beam search to track the most possible contents generated by the model the absolute probability also reflects the quality of its safety alignment. As illustrated in Figure7 direct prompt attacks are mostly ineffective for safety aligned models. In contrast techniques that modify output distribution through constrained decoding demonstrate different behavior patterns. In our visualization we identify four distinct token types with their associated generation probabilities: refusal tokens safe tokens weak jailbreak tokens and strong jailbreak tokens in ascending threat level to conduct jailbreak behaviors. Our analysis reveals a progression of attack effectiveness across different approaches: direct jailbreak prompts face nearly 100% refusal from well-aligned models while adding affirmative prefixes produces only negligible improvements for attackers. JSON format constraints show more impact reducing refusal probability by half though the model still predominantly refuses harmful requests. Our Constrained Decoding Attack (CDA formally introduced in §4[ref_id]S4) eliminates direct refusals in the Phi-3.5 MoE model yet the system continues generating safe tokens with high probability demonstrating how different decoding constraints influence token selection while also revealing persistent safety mechanisms even under constrained decoding conditions. These token type classifications will be referenced throughout subsequent sections to explain attack vectors and defense strategies.
 \Description
 Token distribution comparison
 This distinction is important: while conventional jailbreak methods attempt to circumvent safety guardrails through input manipulation CDA could directly influence what the model is allowed to generate by leveraging the constrained decoding space itself. As shown in Table1 we have added this new attack category to capture this emerging potential vulnerability.
 Research regarding potential security implications of constrained decoding remains limited creating an opportunity to identify and address any vulnerabilities before they could be widely exploited. This paper aims to fill this gap by exploring the security considerations of constrained decoding mechanisms analyzing potential vulnerabilities within current LLM services and proposing mitigation strategies to protect against possible Constrained Decoding Attacks.
[TABLE START]<table>
	<tr>
		<td>Categories</td>
		<td>Jailbreaks</td>
		<td>Extra Assist</td>
		<td>White/Black box</td>
		<td>Target LLM Queries</td>
		<td>I/O-Based</td>
	</tr>
	<tr>
		<td>Manually-designed</td>
		<td>IJP~\cite{bib.bib57}</td>
		<td>Human</td>
		<td>\bullet</td>
		<td>-</td>
		<td>Input</td>
	</tr>
	<tr>
		<td rowspan="2">Optimization-based</td>
		<td>GCG~\cite{bib.bib84}</td>
		<td>-</td>
		<td>\circ</td>
		<td>\sim2K</td>
		<td>Input</td>
	</tr>
	<tr>
		<td>SAA~\cite{bib.bib3}</td>
		<td>-</td>
		<td>\circ</td>
		<td>\sim10K</td>
		<td>Input</td>
	</tr>
	<tr>
		<td rowspan="6">Template-based</td>
		<td>MasterKey~\cite{bib.bib14}</td>
		<td>LLM</td>
		<td>\bullet</td>
		<td>\sim200</td>
		<td>Input</td>
	</tr>
	<tr>
		<td>LLMFuzzer~\cite{bib.bib79}</td>
		<td>LLM</td>
		<td>\bullet</td>
		<td>\sim500</td>
		<td>Input</td>
	</tr>
	<tr>
		<td>AutoDAN~\cite{bib.bib34}</td>
		<td>LLM</td>
		<td>\circ</td>
		<td>\sim200</td>
		<td>Input</td>
	</tr>
	<tr>
		<td>PAIR~\cite{bib.bib10}</td>
		<td>LLM</td>
		<td>\bullet</td>
		<td>\sim20</td>
		<td>Input</td>
	</tr>
	<tr>
		<td>TAP~\cite{bib.bib39}</td>
		<td>LLM</td>
		<td>\bullet</td>
		<td>\sim20</td>
		<td>Input</td>
	</tr>
	<tr>
		<td>StructTransform~\cite{bib.bib78}</td>
		<td>LLM</td>
		<td>\bullet</td>
		<td>\sim3</td>
		<td>Input</td>
	</tr>
	<tr>
		<td rowspan="2">Linguistics-based</td>
		<td>DrAttack~\cite{bib.bib32}</td>
		<td>LLM</td>
		<td>\bullet</td>
		<td>\sim10</td>
		<td>Input</td>
	</tr>
	<tr>
		<td>Puzzler~\cite{bib.bib8}</td>
		<td>LLM</td>
		<td>\bullet</td>
		<td>-</td>
		<td>Input</td>
	</tr>
	<tr>
		<td rowspan="2">Encoding-based</td>
		<td>Zulu~\cite{bib.bib77}</td>
		<td>-</td>
		<td>\bullet</td>
		<td>-</td>
		<td>Input</td>
	</tr>
	<tr>
		<td>Base64~\cite{bib.bib69}</td>
		<td>-</td>
		<td>\bullet</td>
		<td>-</td>
		<td>Input</td>
	</tr>
	<tr>
		<td rowspan="5">Output-based</td>
		<td>EnDec~\cite{bib.bib80}</td>
		<td>-</td>
		<td>\circ</td>
		<td>-</td>
		<td>Output</td>
	</tr>
	<tr>
		<td>APT~\cite{bib.bib33}</td>
		<td>eval LLM</td>
		<td>\bullet</td>
		<td>O(output len)</td>
		<td>Output</td>
	</tr>
	<tr>
		<td>EnumAttack(ours)</td>
		<td>-</td>
		<td>\bullet</td>
		<td>\sim1</td>
		<td>Output</td>
	</tr>
	<tr>
		<td>ChainEnumAttack(ours)</td>
		<td>weak LLM</td>
		<td>\bullet</td>
		<td>\sim1</td>
		<td>Output</td>
	</tr>
	<tr>
		<td>BenignEnumAttack(ours)</td>
		<td>-</td>
		<td>\bullet</td>
		<td>\sim1</td>
		<td>Output</td>
	</tr>
</table>
Table 1. 
Summary of existing jailbreak attacks adapted from  ~\cite{bib.bib81}, - indicates the method does not use the listed resource or lacks that capability, \circ denotes white-box attack and \bullet denotes black-box attack.[TABLE END]

[IMAGE START] [IMAGE URL: /mnt/bmcpfs-29000zjpjtl6xjmjiifyk/xkhu/ideation_workspace/papers/reference_figures/ada8999bb86facdbaa9ce1bcf26ead0d.png] Figure 7. Visualization of 5-token size beams with highest generation probability for Phi-3.5-MoE model. Direct prompt attacks are easily detected and rejected by the model’s auditing mechanisms, rendering them ineffective. In contrast, Constrained Decoding Attack (detailed in   § 4[ref_id]S4) demonstrates its effectiveness by modifying the output distribution through constrained decoding specified by JSON Schema, making it harder for the model to identify and prevent harmful content generation.[IMAGE END]

[IMAGE START] [IMAGE URL: /mnt/bmcpfs-29000zjpjtl6xjmjiifyk/xkhu/ideation_workspace/papers/reference_figures/0da14c3c5ef45f3c6f757044050f466b.png] Figure 6. Motivation example of Constrained Decoding Attack(CDA): Unlike traditional prompt-based attacks where LLMs process the input prompts as data plane to generate a probability distribution over the vocabulary set, where potential jailbreak behaviors may occur. CDA exploits the LLM control plane through grammar constraints (a malicious EBNF grammar here), making LLM forced to generate desired outputs by manipulating the decoding process while leaving a harmless input data plane to bypass safety guardrails.[IMAGE END]

[IMAGE START] [IMAGE URL: /mnt/bmcpfs-29000zjpjtl6xjmjiifyk/xkhu/ideation_workspace/papers/reference_figures/60cd55caba81061a0dbbee7c2976453a.png] Figure 4. Direct jailbreak attempts to GPT-4o model evaluating on AdvBench(520 cases), GPT-4o model rejects 489 cases with direct refusal, rejects 25 cases with longer model responses, and 6 cases somewhat jailbroken.[IMAGE END]



## Section References
[bib.bib84] Zou et al. (2023) Andy Zou Zifan Wang Nicholas Carlini Milad Nasr J. Zico Kolter and Matt Fredrikson. 2023. Universal and Transferable Adversarial Attacks on Aligned Language Models. arXiv:2307.15043 [cs.CL] https://arxiv.org/abs/2307.15043
[bib.bib33] Li et al. (2025) Yanzeng Li Yunfan Xiong Jialun Zhong Jinchao Zhang Jie Zhou and Lei Zou. 2025. Exploiting Prefix-Tree in Structured Output Interfaces for Enhancing Jailbreak Attacking. arXiv:2502.13527 [cs.CR] https://arxiv.org/abs/2502.13527
[bib.bib30] Kwon et al. (2023) Woosuk Kwon Zhuohan Li Siyuan Zhuang Ying Sheng Lianmin Zheng Cody Hao Yu Joseph Gonzalez Hao Zhang and Ion Stoica. 2023. Efficient Memory Management for Large Language Model Serving with PagedAttention. In Proceedings of the 29th Symposium on Operating Systems Principles (Koblenz Germany) (SOSP ’23). Association for Computing Machinery New York NY USA 611–626. doi:10.1145/3600006.3613165 [https://doi.org/10.1145/3600006.3613165]
[bib.bib83] Zheng et al. (2024) Lianmin Zheng Liangsheng Yin Zhiqiang Xie Chuyue Sun Jeff Huang Cody Hao Yu Shiyi Cao Christos Kozyrakis Ion Stoica Joseph E. Gonzalez Clark Barrett and Ying Sheng. 2024. SGLang: Efficient Execution of Structured Language Model Programs. arXiv:2312.07104 [cs.AI] https://arxiv.org/abs/2312.07104
[bib.bib70] Willard and Louf (2023) Brandon T. Willard and Rémi Louf. 2023. Efficient Guided Generation for Large Language Models. arXiv:2307.09702 [cs.CL] https://arxiv.org/abs/2307.09702
[bib.bib16] Dong et al. (2024) Yixin Dong Charlie F. Ruan Yaxing Cai Ruihang Lai Ziyi Xu Yilong Zhao and Tianqi Chen. 2024. XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models. arXiv:2411.15100 [cs.CL] https://arxiv.org/abs/2411.15100
[bib.bib5] Ayala and Bechard (2024) Orlando Ayala and Patrice Bechard. 2024. Reducing hallucination in structured outputs via Retrieval-Augmented Generation. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 6: Industry Track) Yi Yang Aida Davani Avi Sil and Anoop Kumar (Eds.). Association for Computational Linguistics Mexico City Mexico 228–238. doi:10.18653/v1/2024.naacl-industry.19 [https://doi.org/10.18653/v1/2024.naacl-industry.19]
[bib.bib3] Andriushchenko et al. (2025) Maksym Andriushchenko Francesco Croce and Nicolas Flammarion. 2025. Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks. In The Thirteenth International Conference on Learning Representations. https://openreview.net/forum?id=hXA8wqRdyV
[bib.bib21] Haider et al. (2024) Emman Haider Daniel Perez-Becker Thomas Portet Piyush Madan Amit Garg Atabak Ashfaq David Majercak Wen Wen Dongwoo Kim Ziyi Yang Jianwen Zhang Hiteshi Sharma Blake Bullwinkel Martin Pouliot Amanda Minnich Shiven Chawla Solianna Herrera Shahed Warreth Maggie Engler Gary Lopez Nina Chikanov Raja Sekhar Rao Dheekonda Bolor-Erdene Jagdagdorj Roman Lutz Richard Lundeen Tori Westerhoff Pete Bryan Christian Seifert Ram Shankar Siva Kumar Andrew Berkley and Alex Kessler. 2024. Phi-3 Safety Post-Training: Aligning Language Models with a ”Break-Fix” Cycle. arXiv:2407.13833 [cs.CL] https://arxiv.org/abs/2407.13833
[bib.bib57] Shen et al. (2024) Xinyue Shen Zeyuan Chen Michael Backes Yun Shen and Yang Zhang. 2024. ”Do Anything Now”: Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models. In Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security (Salt Lake City UT USA) (CCS ’24). Association for Computing Machinery New York NY USA 1671–1685. doi:10.1145/3658644.3670388 [https://doi.org/10.1145/3658644.3670388]
[bib.bib14] Deng et al. (2024) Gelei Deng Yi Liu Yuekang Li Kailong Wang Ying Zhang Zefeng Li Haoyu Wang Tianwei Zhang and Yang Liu. 2024. MASTERKEY: Automated Jailbreaking of Large Language Model Chatbots. In NDSS. https://www.ndss-symposium.org/ndss-paper/masterkey-automated-jailbreaking-of-large-language-model-chatbots/
[bib.bib79] Yu et al. (2024) Jiahao Yu Xingwei Lin Zheng Yu and Xinyu Xing. 2024. LLM-Fuzzer: Scaling Assessment of Large Language Model Jailbreaks. In 33rd USENIX Security Symposium (USENIX Security 24). USENIX Association Philadelphia PA 4657–4674. https://www.usenix.org/conference/usenixsecurity24/presentation/yu-jiahao
[bib.bib34] Liu et al. (2024b) Xiaogeng Liu Nan Xu Muhao Chen and Chaowei Xiao. 2024b. AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models. In The Twelfth International Conference on Learning Representations. https://openreview.net/forum?id=7Jwpw4qKkb
[bib.bib10] Chao et al. (2024b) Patrick Chao Alexander Robey Edgar Dobriban Hamed Hassani George J. Pappas and Eric Wong. 2024b. Jailbreaking Black Box Large Language Models in Twenty Queries. arXiv:2310.08419 [cs.LG] https://arxiv.org/abs/2310.08419
[bib.bib39] Mehrotra et al. (2024) Anay Mehrotra Manolis Zampetakis Paul Kassianik Blaine Nelson Hyrum S Anderson Yaron Singer and Amin Karbasi. 2024. Tree of Attacks: Jailbreaking Black-Box LLMs Automatically. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. https://openreview.net/forum?id=SoM3vngOH5
[bib.bib78] Yoosuf et al. (2025) Shehel Yoosuf Temoor Ali Ahmed Lekssays Mashael AlSabah and Issa Khalil. 2025. StructTransform: A Scalable Attack Surface for Safety-Aligned Large Language Models. arXiv:2502.11853 [cs.LG] https://arxiv.org/abs/2502.11853
[bib.bib32] Li et al. (2024) Xirui Li Ruochen Wang Minhao Cheng Tianyi Zhou and Cho-Jui Hsieh. 2024. DrAttack: Prompt Decomposition and Reconstruction Makes Powerful LLMs Jailbreakers. In Findings of the Association for Computational Linguistics: EMNLP 2024 Yaser Al-Onaizan Mohit Bansal and Yun-Nung Chen (Eds.). Association for Computational Linguistics Miami Florida USA 13891–13913. doi:10.18653/v1/2024.findings-emnlp.813 [https://doi.org/10.18653/v1/2024.findings-emnlp.813]
[bib.bib8] Chang et al. (2024) Zhiyuan Chang Mingyang Li Yi Liu Junjie Wang Qing Wang and Yang Liu. 2024. Play Guessing Game with LLM: Indirect Jailbreak Attack with Implicit Clues. In Findings of the Association for Computational Linguistics: ACL 2024 Lun-Wei Ku Andre Martins and Vivek Srikumar (Eds.). Association for Computational Linguistics Bangkok Thailand 5135–5147. doi:10.18653/v1/2024.findings-acl.304 [https://doi.org/10.18653/v1/2024.findings-acl.304]
[bib.bib77] Yong et al. (2024) Zheng-Xin Yong Cristina Menghini and Stephen H. Bach. 2024. Low-Resource Languages Jailbreak GPT-4. arXiv:2310.02446 [cs.CL] https://arxiv.org/abs/2310.02446
[bib.bib69] Wei et al. (2023) Alexander Wei Nika Haghtalab and Jacob Steinhardt. 2023. Jailbroken: How Does LLM Safety Training Fail?. In Thirty-seventh Conference on Neural Information Processing Systems. https://openreview.net/forum?id=jA235JGM09
[bib.bib80] Zhang et al. (2024) Hangfan Zhang Zhimeng Guo Huaisheng Zhu Bochuan Cao Lu Lin Jinyuan Jia Jinghui Chen and Dinghao Wu. 2024. Jailbreak Open-Sourced Large Language Models via Enforced Decoding. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) Lun-Wei Ku Andre Martins and Vivek Srikumar (Eds.). Association for Computational Linguistics Bangkok Thailand 5475–5493. doi:10.18653/v1/2024.acl-long.299 [https://doi.org/10.18653/v1/2024.acl-long.299]
[bib.bib81] Zhang et al. (2025) Shenyi Zhang Yuchen Zhai Keyan Guo Hongxin Hu Shengnan Guo Zheng Fang Lingchen Zhao Chao Shen Cong Wang and Qian Wang. 2025. JBShield: Defending Large Language Models from Jailbreak Attacks through Activated Concept Analysis and Manipulation. arXiv:2502.07557 [cs.CR] https://arxiv.org/abs/2502.07557