4. Constrained Decoding Attacks
Building on these observations we introduce the concept of Constrained Decoding Attack (CDA) as a novel potential vulnerability and dimension in LLM security. Unlike traditional jailbreak approaches that focus on crafting specialized prompts CDA represents a different attack vector—targeting the decoding stage through grammar rules that constrain output generation.
 We formally define the threat model for potential Constrained Decoding Attacks as follows:
 Attacker Capabilities
 •
 API Access: The attacker has access to an LLM through its public API. It does not require access to model weights gradients or internal representations.
 •
 Grammar-level Control: The attacker can specify grammar rules that will be used to constrain the model’s output which is the attack surface APIs like OpenAI support such features more or less. It can be a direct grammar input or encapsulated in a structured output format like Choice Regex and JSON schema.