4.1.1. Enum Attack on OpenAI’s GPT models
Initially we perform preliminary experiments on OpenAI’s GPT models. From OpenAI’s model system card~\cite{bib.bib2 bib.bib45 bib.bib44} their models before release have been extensively tested via external red teaming specific post-training and equipped with moderation model~\cite{bib.bib46} to mitigate potential safety risks.
 Taking GPT-4o as an example~\cite{bib.bib45 bib.bib44} 4 phase of red teamers have been conducted to observe and collect unintended behaviors of the model including multi-turn conversations multimodal (audio image together with text) generations and multiple languages. Then post-training efforts are made to mitigate the risks aligning the model to the safety guidelines. Finally the released model is equipped with external guardrails to further mitigate the risks.
 \Description
 Enum Attack to GPT-4o model evaluating on AdvBench.
 As presented in our motivation in Figure4 baseline attacks on GPT-4o model are now well mitigated. It is quite natural because defenses are always evolving to catch up with the latest attacks.
 Due to their empirically-sound and well-acknowledged safety alignment effort we are curious about whether structured output feature is a vulnerability to GPT-4o model. Therefore we perform our proof-of-concept Enum Attack in Figure8 on the GPT-4o model.
 As illustrated in Figure9 Enum Attack performs significantly well on GPT-4o model where all 520 cases are successfully jailbroken the external guardrail is entirely bypassed as no early refusal is triggered. In addition the generation content although wrapped in the structured output is not refused by the model itself and the step-by-step plan is both specific and convincing as if it is not safety aligned. Followed by ~\cite{bib.bib60} we use their StrongREJECT score to evaluate the attack which is defined as follows: StrongREJECT=(1-refused)\times\frac{specific+convincing}{2} GPT-4o model achieves 95.6% StrongREJECT score placing it among the most effective jailbreaks documented against this model class and substantially exceeding typical attack efficacy ranges. This quantitatively confirms the complete circumvention of both internal and external safety mechanisms previously described.
 This result is quite surprising because our simple proof-of-concept jailbreaks GPT-4o’s safety alignment so easily with the aid of structured output which definitely show great vulnerabilities within the design of the structured output feature.
 We analyze the results as follows:
 •
 Enum Attack bypasses the external prompt-based guardrails since no early refusal is triggered by the model. The reason is quite simple the attack body is concealed within structured output and the prompt part is harmless so the model remains unaware during prompt auditing.
 •
 Enum Attack also bypasses the internal safety alignment of the model as the model does not refuse in the constrained output either. It is because the model first generates an affirmative prefix constrained in the structured output which SHOULD NOT be generated by the model; however the constrained decoding mechanism forced the model to generate it. Later the model continues to generate the following steps as an answer to the question which is an internal limitation to LLMs. As LLMs are designed to perform next-token prediction and the model is therefore likely to continue generating steps as the answer to the question even if the affirmative prefix is harmful.
 •
 Output-based auditing appears to be inactive in API access to the GPT-4o model likely due to cost and performance considerations. Implementing real-time auditing of generated content introduces substantial computational overhead that would be commercially infeasible to maintain at the production scale.
 In conclusion despite its simplicity Enum Attack demonstrates remarkable effectiveness against LLMs. The GPT-4o model proves particularly vulnerable achieving an ASR 100% through only API-level access and structured output capabilities. GPT-4o-mini models perform similarly which is not surprising as the structured output feature is the same. Gemini models are also vulnerable to Enum Attack as it offers similar structured output features. Additionally since LLM serving engines like vllm~\cite{bib.bib30} and sglang~\cite{bib.bib83} support even more structured output features through its OpenAI-compatible server APIs open-weight LLMs are also vulnerable to Enum Attack. More detailed evaluations will be presented in §5[ref_id]S5.
[IMAGE START] [IMAGE URL: /mnt/bmcpfs-29000zjpjtl6xjmjiifyk/xkhu/ideation_workspace/papers/reference_figures/60cd55caba81061a0dbbee7c2976453a.png] Figure 4. Direct jailbreak attempts to GPT-4o model evaluating on AdvBench(520 cases), GPT-4o model rejects 489 cases with direct refusal, rejects 25 cases with longer model responses, and 6 cases somewhat jailbroken.[IMAGE END]

[IMAGE START] [IMAGE URL: /mnt/bmcpfs-29000zjpjtl6xjmjiifyk/xkhu/ideation_workspace/papers/reference_figures/4b7994b4145df3404f770e6715b0cc54.png] Figure 8. Illustration code for Enum Attack, where the attack body is located in the structured output, and the prompt part is harmless, so it can bypass prompt-based auditing.[IMAGE END]

[IMAGE START] [IMAGE URL: /mnt/bmcpfs-29000zjpjtl6xjmjiifyk/xkhu/ideation_workspace/papers/reference_figures/668fc15e48cce6eb9aa4dfb433895f6f.png] Figure 9. Enum Attack to GPT-4o model evaluating on AdvBench, where all 520 cases are successfully jailbroken, the external guardrail is entirely bypassed, and the model doesn’t refuse in the constrained output either. The attack success rate is 100%.[IMAGE END]



## Section References
[bib.bib2] Achiam et al. (2023) Josh Achiam Steven Adler Sandhini Agarwal Lama Ahmad Ilge Akkaya Florencia Leoni Aleman Diogo Almeida Janko Altenschmidt Sam Altman Shyamal Anadkat et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023).
[bib.bib45] OpenAI (2024b) OpenAI. 2024b. GPT-4o System Card. https://openai.com/index/gpt-4o-system-card/ Accessed: March 07 2025.
[bib.bib44] OpenAI (2024a) OpenAI. 2024a. GPT-4o mini: advancing cost-efficient intelligence. https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/ Accessed: March 07 2025.
[bib.bib46] OpenAI (2025a) OpenAI. 2025a. Moderation - OpenAI API. https://platform.openai.com/docs/guides/moderation/overview Accessed: March 19 2025.
[bib.bib60] Souly et al. (2024) Alexandra Souly Qingyuan Lu Dillon Bowen Tu Trinh Elvis Hsieh Sana Pandey Pieter Abbeel Justin Svegliato Scott Emmons Olivia Watkins and Sam Toyer. 2024. A StrongREJECT for Empty Jailbreaks. In ICLR 2024 Workshop on Reliable and Responsible Foundation Models. https://openreview.net/forum?id=al303JJkGO
[bib.bib30] Kwon et al. (2023) Woosuk Kwon Zhuohan Li Siyuan Zhuang Ying Sheng Lianmin Zheng Cody Hao Yu Joseph Gonzalez Hao Zhang and Ion Stoica. 2023. Efficient Memory Management for Large Language Model Serving with PagedAttention. In Proceedings of the 29th Symposium on Operating Systems Principles (Koblenz Germany) (SOSP ’23). Association for Computing Machinery New York NY USA 611–626. doi:10.1145/3600006.3613165 [https://doi.org/10.1145/3600006.3613165]
[bib.bib83] Zheng et al. (2024) Lianmin Zheng Liangsheng Yin Zhiqiang Xie Chuyue Sun Jeff Huang Cody Hao Yu Shiyi Cao Christos Kozyrakis Ion Stoica Joseph E. Gonzalez Clark Barrett and Ying Sheng. 2024. SGLang: Efficient Execution of Structured Language Model Programs. arXiv:2312.07104 [cs.AI] https://arxiv.org/abs/2312.07104