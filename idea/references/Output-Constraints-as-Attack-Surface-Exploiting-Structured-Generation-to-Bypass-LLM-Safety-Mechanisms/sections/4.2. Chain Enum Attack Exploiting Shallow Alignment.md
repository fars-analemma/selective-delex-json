4.2. Chain Enum Attack: Exploiting Shallow Alignment
Enum Attack demonstrates the potential of Constrained Decoding Attacks to jailbreak LLMs. It is a simple yet effective attack that can be applied to a wide range of models and infrastructures. The fundamental vulnerability exploited by Enum Attack stems from what recent research terms shallow safety alignment~\cite{bib.bib50}. Current LLM safety mechanisms predominantly protect only the first few tokens of generation creating a critical weakness that constrained decoding attacks can exploit.
 However Enum Attack may be not effective against deeply aligned models as they can realize the harmfulness of the generated content and start to refuse or disclaim the harmful content.
 Therefore we introduce a more sophisticated variant: Chain Enum Attack: a multi-stage attack that exploits the shallow alignment of one model to compromise the safety of another model with stronger alignment.
 (1)
 A shallowly-aligned model is used to generate harmful prefix answers via Enum Attack
 (2)
 These prefixes are then forwarded to a more deeply-aligned model which are forced to be generated by grammar constraints
 (3)
 The deeply-aligned model despite its stronger safety mechanisms inherits the compromised token distribution continues to generate harmful content
 \Description
 Token distribution comparison
 To understand this vulnerability we continue to analyze the token probability distribution with quantitative evaluation which has been conducted in Figure7. Previously we have motivated how Enum Attack breaks its safety alignment through affirmative prefix JSON format and question hiding in output side. Now we continue the analysis. As shown in Figure10 there are a dramatic logit shift towards jailbreaking with enhanced methods and we will explain them each:
 (1)
 Using Enum Attack our PoC attack on the well aligned Phi-3.5-MoE model although direct refusals are gone. The model now still generates safe tokens in relatively high probability
 (2)
 With a benign system prompt the tendency of safe token generation is further depressed under Enum Attack and the model is now more likely to be jailbroken as jailbreak tokens are now most likely to be generated
 (3)
 Performing Chain Enum Attack: with a response generated from a weakly aligned model prefilled through Enum Attack to setup the jailbroken context even strongly aligned model can be fully jailbroken the top-5 beams are now all strong jailbreak tokens
 This distribution shift confirms the hypothesis in recent literature~\cite{bib.bib50} that safety alignment in current LLMs is predominantly concentrated in initial token selection. By forcing the generation of affirmative prefixes through constrained decoding Chain Enum Attack effectively bypasses this shallow alignment mechanism by compromising the token distribution from a safety aligned one (with significantly high refusal probability) to a balanced context-sensitive one.
 Chain Enum Attack also demonstrates how shallow alignment in one model can undermine safety in another model with stronger alignment. After all the next-token-prediction nature limits the capability of the model to refuse continuing the harmful content generation.
 Additionally our analysis reveals that the vulnerability extends beyond initial token selection. Once forced to generate affirmative prefixes models exhibit a cascading failure pattern where subsequent tokens increasingly favor harmful content completion. As presented by ~\cite{bib.bib50} we re-confirm that the probability mass shifts dramatically toward harmful completions after a few forced tokens. This phenomenon can be quantified as:
 P(w_{t}^{\text{harmful}}|w_{1:t-1}^{\text{forced}})\gg P(w_{t}^{\text{harmful}}|w_{1:t-1}^{\text{natural}})
 Where w_{1:t-1}^{\text{forced}} represents the sequence of tokens generated through constrained decoding and w_{t}^{\text{harmful}} represents harmful completion tokens.
 This provides quantitative evidence that current safety alignments are too limited on operating at the generation start rather than maintaining safety awareness throughout the entire generation process.
[IMAGE START] [IMAGE URL: /mnt/bmcpfs-29000zjpjtl6xjmjiifyk/xkhu/ideation_workspace/papers/reference_figures/ada8999bb86facdbaa9ce1bcf26ead0d.png] Figure 7. Visualization of 5-token size beams with highest generation probability for Phi-3.5-MoE model. Direct prompt attacks are easily detected and rejected by the model’s auditing mechanisms, rendering them ineffective. In contrast, Constrained Decoding Attack (detailed in   § 4[ref_id]S4) demonstrates its effectiveness by modifying the output distribution through constrained decoding specified by JSON Schema, making it harder for the model to identify and prevent harmful content generation.[IMAGE END]

[IMAGE START] [IMAGE URL: /mnt/bmcpfs-29000zjpjtl6xjmjiifyk/xkhu/ideation_workspace/papers/reference_figures/9d6ceada8ba0d396c3e51538c0e5645f.png] Figure 10. Token distribution ablation with progressive attack methods, the exact probability distribution is sampled from Phi-3.5-MoE model. Refusal tokens, safe tokens,weak jailbreak tokens and strong jailbreak tokens are marked explicitly with their generation probability, less-than-1e-16 values are omitted.[IMAGE END]



## Section References
[bib.bib50] Qi et al. (2025) Xiangyu Qi Ashwinee Panda Kaifeng Lyu Xiao Ma Subhrajit Roy Ahmad Beirami Prateek Mittal and Peter Henderson. 2025. Safety Alignment Should be Made More Than Just a Few Tokens Deep. In The Thirteenth International Conference on Learning Representations. https://openreview.net/forum?id=6Mxhg9PtDE