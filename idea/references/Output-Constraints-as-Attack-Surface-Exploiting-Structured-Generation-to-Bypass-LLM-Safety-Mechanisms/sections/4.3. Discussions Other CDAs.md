4.3. Discussions: Other CDAs
Except attacks we presented there are also other CDAs that can be achieved. For example Although EnDec~\cite{bib.bib80} uses white-box attack to directly control output logits to perform jailbreaks it can be achieved in an indirect way through guided grammar to perform a Constrained Decoding Attack. APT~\cite{bib.bib33} utilizes GuidedRegex features to iteratively generate desired text by blocking refusal tokens identified through an evaluator model. Although it requires significant numbers of queries to jailbreak the model as it requires step-by-step GuidedRegex generation like a tree construction it can be combined with Enum Attack to perform more effective jailbreaks as Enum Attack can hide the harmful content in the structured output and APT can avoid refusals in the generation process.
 Besides existing prompt-based jailbreak methods are also orthogonal to our work and can be combined with Enum Attack to perform more effective jailbreaks. After all Constrained Decoding Attacks make the output generation stage a new attack surface and previous prompt-stage attack methods can also work in this new attack surface.
 Template-based attacks like MasterKey~\cite{bib.bib14} LLMFuzzer~\cite{bib.bib79} PAIR~\cite{bib.bib10} and TAP~\cite{bib.bib39} can be used in Enum Attack to generate a more complicated ‘yes prefix’ field so that the model can be hijacked more effectively.
 Linguistic-based attacks~\cite{bib.bib32 bib.bib8} and encoding-based attacks~\cite{bib.bib77 bib.bib69} can also be combined with Enum Attack. The combination can hide the harmful content in the structured output deeper so that even auditing the grammar rules may not be effective to detect the attack.
 Finally there can be potentially more CDAs that utilize the constrained decoding mechanisms as the design space of an EBNF grammar is unlimited malicious attackers can create a flexible pretend-to-be-harmless grammar to jailbreak the model. Therefore it is essential to study the security risks of the constrained decoding mechanism and develop effective mechanisms to mitigate the model from Constrained Decoding Attacks.


## Section References
[bib.bib80] Zhang et al. (2024) Hangfan Zhang Zhimeng Guo Huaisheng Zhu Bochuan Cao Lu Lin Jinyuan Jia Jinghui Chen and Dinghao Wu. 2024. Jailbreak Open-Sourced Large Language Models via Enforced Decoding. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) Lun-Wei Ku Andre Martins and Vivek Srikumar (Eds.). Association for Computational Linguistics Bangkok Thailand 5475–5493. doi:10.18653/v1/2024.acl-long.299 [https://doi.org/10.18653/v1/2024.acl-long.299]
[bib.bib33] Li et al. (2025) Yanzeng Li Yunfan Xiong Jialun Zhong Jinchao Zhang Jie Zhou and Lei Zou. 2025. Exploiting Prefix-Tree in Structured Output Interfaces for Enhancing Jailbreak Attacking. arXiv:2502.13527 [cs.CR] https://arxiv.org/abs/2502.13527
[bib.bib14] Deng et al. (2024) Gelei Deng Yi Liu Yuekang Li Kailong Wang Ying Zhang Zefeng Li Haoyu Wang Tianwei Zhang and Yang Liu. 2024. MASTERKEY: Automated Jailbreaking of Large Language Model Chatbots. In NDSS. https://www.ndss-symposium.org/ndss-paper/masterkey-automated-jailbreaking-of-large-language-model-chatbots/
[bib.bib79] Yu et al. (2024) Jiahao Yu Xingwei Lin Zheng Yu and Xinyu Xing. 2024. LLM-Fuzzer: Scaling Assessment of Large Language Model Jailbreaks. In 33rd USENIX Security Symposium (USENIX Security 24). USENIX Association Philadelphia PA 4657–4674. https://www.usenix.org/conference/usenixsecurity24/presentation/yu-jiahao
[bib.bib10] Chao et al. (2024b) Patrick Chao Alexander Robey Edgar Dobriban Hamed Hassani George J. Pappas and Eric Wong. 2024b. Jailbreaking Black Box Large Language Models in Twenty Queries. arXiv:2310.08419 [cs.LG] https://arxiv.org/abs/2310.08419
[bib.bib39] Mehrotra et al. (2024) Anay Mehrotra Manolis Zampetakis Paul Kassianik Blaine Nelson Hyrum S Anderson Yaron Singer and Amin Karbasi. 2024. Tree of Attacks: Jailbreaking Black-Box LLMs Automatically. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. https://openreview.net/forum?id=SoM3vngOH5
[bib.bib32] Li et al. (2024) Xirui Li Ruochen Wang Minhao Cheng Tianyi Zhou and Cho-Jui Hsieh. 2024. DrAttack: Prompt Decomposition and Reconstruction Makes Powerful LLMs Jailbreakers. In Findings of the Association for Computational Linguistics: EMNLP 2024 Yaser Al-Onaizan Mohit Bansal and Yun-Nung Chen (Eds.). Association for Computational Linguistics Miami Florida USA 13891–13913. doi:10.18653/v1/2024.findings-emnlp.813 [https://doi.org/10.18653/v1/2024.findings-emnlp.813]
[bib.bib8] Chang et al. (2024) Zhiyuan Chang Mingyang Li Yi Liu Junjie Wang Qing Wang and Yang Liu. 2024. Play Guessing Game with LLM: Indirect Jailbreak Attack with Implicit Clues. In Findings of the Association for Computational Linguistics: ACL 2024 Lun-Wei Ku Andre Martins and Vivek Srikumar (Eds.). Association for Computational Linguistics Bangkok Thailand 5135–5147. doi:10.18653/v1/2024.findings-acl.304 [https://doi.org/10.18653/v1/2024.findings-acl.304]
[bib.bib77] Yong et al. (2024) Zheng-Xin Yong Cristina Menghini and Stephen H. Bach. 2024. Low-Resource Languages Jailbreak GPT-4. arXiv:2310.02446 [cs.CL] https://arxiv.org/abs/2310.02446
[bib.bib69] Wei et al. (2023) Alexander Wei Nika Haghtalab and Jacob Steinhardt. 2023. Jailbroken: How Does LLM Safety Training Fail?. In Thirty-seventh Conference on Neural Information Processing Systems. https://openreview.net/forum?id=jA235JGM09