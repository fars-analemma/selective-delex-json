5.1. Experimental Setup
Datasets Following previous works ~\cite{bib.bib7 bib.bib84 bib.bib80} we evaluate the performance of constrained decoding based attacks (CDAs) in five well-known benchmarks whose statistics are shown in Table2.
 Large Language Models We evaluate LLMs with structured output support capabilities. For proprietary models we evaluate GPT-4o GPT-4o-mini and Gemini-2.0-flash1We use the following available checkpoints of these models including gpt-4o-2024-11-20 gpt-4o-mini-2024-07-18 gemini-2.0-flash-001. using OpenAI and Gemini’s API access. For open-weight models we evaluate 5 latest open-weight LLMs served on an OpenAI compatible server with structured output features including Phi-3.5-MoE~\cite{bib.bib21} Mistral Nemo~\cite{bib.bib41} Qwen-2.5-32B~\cite{bib.bib52} Llama-3.1-8B~\cite{bib.bib40} and Gemma-2-9B~\cite{bib.bib61}. Legacy models like Vicuna~\cite{bib.bib11} and Llama2~\cite{bib.bib64} are extensively studied and considered easy to be jailbroken~\cite{bib.bib9} so we don’t include them in the evaluation. In total we evaluate 3 proprietary models and 5 open-weight models using black-box settings for attacks evaluation.
 Evaluation Metrics Following previous works ~\cite{bib.bib84 bib.bib7 bib.bib80} we use attack success rate (ASR) as the evaluation metric. However considering ASR is a binary metric we follow recent work~\cite{bib.bib60} who use LLM-as-a-judge to evaluate in multiple dimensions including refusal convincingness and specificity to evaluate the harmfulness of the generated text. We also apply the same workflow as StrongREJECT~\cite{bib.bib60} to evaluate ASR with their proposed StrongREJECT Score combining refusal convincingness and specificity which is defined in Equation7[ref_id]S4.E7. Surprisingly we find out the original version of ~\cite{bib.bib60} has a significant ratio (over 20% using GPT-4o as evaluator) to refuse the evaluation request whichever judge LLM is used which limits the quality of LLM-as-a-judge because both the question and answer could be too offensive and malicious to the model which may falsely trigger the refusal to evaluate the Q-A pair.
 \Description
 StrongREJECT evaluation process
 Intriguingly we found that the same constrained decoding mechanism exploited in our attack can be repurposed to improve evaluation methodology. We apply a derived guided-grammar to constrain the output format of the judge model forcing structured evaluation responses that prevent refusal to assess harmful content. This technique illustrated in Figure11 leverages the core insight of Enum Attack—that constrained decoding can bypass internal safety alignment—to create more reliable measurement tools for safety research. The improved StrongREJECT evaluation has zero refusal rate we use the improved StrongREJECT Score to evaluate as additional metrics to ASR and the evaluation model we use is GPT-4o. Keep in mind of this specific crafted format in Figure11 it represents another challenge for proper content auditing which we will discuss later.
 Additional Experimental Setup For local LLMs we use vllm version 0.7.2 for serving we serve these models on a Ubuntu 22.04 server running Linux Kernel 5.15 with Xeon Gold 6430 (128) @ 3.40GHz 4 NVIDIA A100 80GB GPUs and 512GB RAM. We set the ”max_model_len” to 3072 and ”tensor_parallel_size” to 4 for all these models. We use the vllm’s OpenAI compatible server to call the local LLMs so that the evaluation for local models is consistent with the evaluation for OpenAI models. Although ~\cite{bib.bib23} suggests that the temperature setting can affect the evaluation results it is orthogonal to our attack surface and not the focus of this paper. Therefore We use temperature=0.6 for all models.
[TABLE START]<table>
	<tr>
		<th>Dataset</th>
		<th>Size</th>
		<th>Category</th>
		<th>Extra Attack</th>
	</tr>
	<tr>
		<td>AdvBench~\cite{bib.bib84}</td>
		<td>520</td>
		<td>\circ</td>
		<td>\circ</td>
	</tr>
	<tr>
		<td>StrongREJECT~\cite{bib.bib60}</td>
		<td>311</td>
		<td>6</td>
		<td>\circ</td>
	</tr>
	<tr>
		<td>JailbreakBench~\cite{bib.bib9}</td>
		<td>100</td>
		<td>10</td>
		<td>\circ</td>
	</tr>
	<tr>
		<td>HarmBench~\cite{bib.bib38}</td>
		<td>100</td>
		<td>3</td>
		<td>\circ</td>
	</tr>
	<tr>
		<td>SorryBench~\cite{bib.bib73}</td>
		<td>440</td>
		<td>44</td>
		<td>21</td>
	</tr>
</table>
Table 2. Summary of Datasets used for jailbreak attack evaluation, \circ denotes w/o such property.[TABLE END]

[IMAGE START] [IMAGE URL: /mnt/bmcpfs-29000zjpjtl6xjmjiifyk/xkhu/ideation_workspace/papers/reference_figures/e56779a074ecde883b0ae212647761e0.png] Figure 11. Improved StrongREJECT evaluation process.[IMAGE END]



## Section References
[bib.bib7] Carlini et al. (2023) Nicholas Carlini Milad Nasr Christopher A. Choquette-Choo Matthew Jagielski Irena Gao Pang Wei Koh Daphne Ippolito Florian Tramèr and Ludwig Schmidt. 2023. Are aligned neural networks adversarially aligned?. In Thirty-seventh Conference on Neural Information Processing Systems. https://openreview.net/forum?id=OQQoD8Vc3B
[bib.bib84] Zou et al. (2023) Andy Zou Zifan Wang Nicholas Carlini Milad Nasr J. Zico Kolter and Matt Fredrikson. 2023. Universal and Transferable Adversarial Attacks on Aligned Language Models. arXiv:2307.15043 [cs.CL] https://arxiv.org/abs/2307.15043
[bib.bib80] Zhang et al. (2024) Hangfan Zhang Zhimeng Guo Huaisheng Zhu Bochuan Cao Lu Lin Jinyuan Jia Jinghui Chen and Dinghao Wu. 2024. Jailbreak Open-Sourced Large Language Models via Enforced Decoding. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) Lun-Wei Ku Andre Martins and Vivek Srikumar (Eds.). Association for Computational Linguistics Bangkok Thailand 5475–5493. doi:10.18653/v1/2024.acl-long.299 [https://doi.org/10.18653/v1/2024.acl-long.299]
[bib.bib21] Haider et al. (2024) Emman Haider Daniel Perez-Becker Thomas Portet Piyush Madan Amit Garg Atabak Ashfaq David Majercak Wen Wen Dongwoo Kim Ziyi Yang Jianwen Zhang Hiteshi Sharma Blake Bullwinkel Martin Pouliot Amanda Minnich Shiven Chawla Solianna Herrera Shahed Warreth Maggie Engler Gary Lopez Nina Chikanov Raja Sekhar Rao Dheekonda Bolor-Erdene Jagdagdorj Roman Lutz Richard Lundeen Tori Westerhoff Pete Bryan Christian Seifert Ram Shankar Siva Kumar Andrew Berkley and Alex Kessler. 2024. Phi-3 Safety Post-Training: Aligning Language Models with a ”Break-Fix” Cycle. arXiv:2407.13833 [cs.CL] https://arxiv.org/abs/2407.13833
[bib.bib41] Mistral AI team (2024) Mistral AI team. 2024. Mistral NeMo: A State-of-the-Art 12B Model with 128k Context Length. https://mistral.ai/news/mistral-nemo Released under Apache 2.0 license.
[bib.bib52] Qwen et al. (2025) Qwen : An Yang Baosong Yang Beichen Zhang Binyuan Hui Bo Zheng Bowen Yu Chengyuan Li Dayiheng Liu Fei Huang Haoran Wei Huan Lin Jian Yang Jianhong Tu Jianwei Zhang Jianxin Yang Jiaxi Yang Jingren Zhou Junyang Lin Kai Dang Keming Lu Keqin Bao Kexin Yang Le Yu Mei Li Mingfeng Xue Pei Zhang Qin Zhu Rui Men Runji Lin Tianhao Li Tianyi Tang Tingyu Xia Xingzhang Ren Xuancheng Ren Yang Fan Yang Su Yichang Zhang Yu Wan Yuqiong Liu Zeyu Cui Zhenru Zhang and Zihan Qiu. 2025. Qwen2.5 Technical Report. arXiv:2412.15115 [cs.CL] https://arxiv.org/abs/2412.15115
[bib.bib40] Meta AI (2024) Meta AI. 2024. Introducing Meta Llama 3: The most capable openly available LLM to date. https://ai.meta.com/blog/meta-llama-3
[bib.bib61] Team et al. (2024) Gemma Team Morgane Riviere Shreya Pathak Pier Giuseppe Sessa Cassidy Hardin Surya Bhupatiraju Léonard Hussenot Thomas Mesnard Bobak Shahriari Alexandre Ramé Johan Ferret Peter Liu Pouya Tafti Abe Friesen Michelle Casbon Sabela Ramos Ravin Kumar Charline Le Lan Sammy Jerome Anton Tsitsulin Nino Vieillard Piotr Stanczyk Sertan Girgin Nikola Momchev Matt Hoffman Shantanu Thakoor Jean-Bastien Grill Behnam Neyshabur Olivier Bachem Alanna Walton Aliaksei Severyn Alicia Parrish Aliya Ahmad Allen Hutchison Alvin Abdagic Amanda Carl Amy Shen Andy Brock Andy Coenen Anthony Laforge Antonia Paterson Ben Bastian Bilal Piot Bo Wu Brandon Royal Charlie Chen Chintu Kumar Chris Perry Chris Welty Christopher A. Choquette-Choo Danila Sinopalnikov David Weinberger Dimple Vijaykumar Dominika Rogozińska Dustin Herbison Elisa Bandy Emma Wang Eric Noland Erica Moreira Evan Senter Evgenii Eltyshev Francesco Visin Gabriel Rasskin Gary Wei Glenn Cameron Gus Martins Hadi Hashemi Hanna Klimczak-Plucińska Harleen Batra Harsh Dhand Ivan Nardini Jacinda Mein Jack Zhou James Svensson Jeff Stanway Jetha Chan Jin Peng Zhou Joana Carrasqueira Joana Iljazi Jocelyn Becker Joe Fernandez Joost van Amersfoort Josh Gordon Josh Lipschultz Josh Newlan Ju yeong Ji Kareem Mohamed Kartikeya Badola Kat Black Katie Millican Keelin McDonell Kelvin Nguyen Kiranbir Sodhia Kish Greene Lars Lowe Sjoesund Lauren Usui Laurent Sifre Lena Heuermann Leticia Lago Lilly McNealus Livio Baldini Soares Logan Kilpatrick Lucas Dixon Luciano Martins Machel Reid Manvinder Singh Mark Iverson Martin Görner Mat Velloso Mateo Wirth Matt Davidow Matt Miller Matthew Rahtz Matthew Watson Meg Risdal Mehran Kazemi Michael Moynihan Ming Zhang Minsuk Kahng Minwoo Park Mofi Rahman Mohit Khatwani Natalie Dao Nenshad Bardoliwalla Nesh Devanathan Neta Dumai Nilay Chauhan Oscar Wahltinez Pankil Botarda Parker Barnes Paul Barham Paul Michel Pengchong Jin Petko Georgiev Phil Culliton Pradeep Kuppala Ramona Comanescu Ramona Merhej Reena Jana Reza Ardeshir Rokni Rishabh Agarwal Ryan Mullins Samaneh Saadat Sara Mc Carthy Sarah Cogan Sarah Perrin Sébastien M. R. Arnold Sebastian Krause Shengyang Dai Shruti Garg Shruti Sheth Sue Ronstrom Susan Chan Timothy Jordan Ting Yu Tom Eccles Tom Hennigan Tomas Kocisky Tulsee Doshi Vihan Jain Vikas Yadav Vilobh Meshram Vishal Dharmadhikari Warren Barkley Wei Wei Wenming Ye Woohyun Han Woosuk Kwon Xiang Xu Zhe Shen Zhitao Gong Zichuan Wei Victor Cotruta Phoebe Kirk Anand Rao Minh Giang Ludovic Peran Tris Warkentin Eli Collins Joelle Barral Zoubin Ghahramani Raia Hadsell D. Sculley Jeanine Banks Anca Dragan Slav Petrov Oriol Vinyals Jeff Dean Demis Hassabis Koray Kavukcuoglu Clement Farabet Elena Buchatskaya Sebastian Borgeaud Noah Fiedel Armand Joulin Kathleen Kenealy Robert Dadashi and Alek Andreev. 2024. Gemma 2: Improving Open Language Models at a Practical Size. arXiv:2408.00118 [cs.CL] https://arxiv.org/abs/2408.00118
[bib.bib11] Chiang et al. (2023) Wei-Lin Chiang Zhuohan Li Zi Lin Ying Sheng Zhanghao Wu Hao Zhang Lianmin Zheng Siyuan Zhuang Yonghao Zhuang Joseph E. Gonzalez Ion Stoica and Eric P. Xing. 2023. Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality. https://lmsys.org/blog/2023-03-30-vicuna/
[bib.bib64] Touvron et al. (2023b) Hugo Touvron Louis Martin Kevin Stone Peter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra Prajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen Guillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller Cynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou Hakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev Punit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich Yinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra Igor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang Angela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic Sergey Edunov and Thomas Scialom. 2023b. Llama 2: Open Foundation and Fine-Tuned Chat Models. arXiv preprint arXiv:2307.09288 (2023).
[bib.bib9] Chao et al. (2024a) Patrick Chao Edoardo Debenedetti Alexander Robey Maksym Andriushchenko Francesco Croce Vikash Sehwag Edgar Dobriban Nicolas Flammarion George J. Pappas Florian Tramèr Hamed Hassani and Eric Wong. 2024a. JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track. https://openreview.net/forum?id=urjPCYZt0I
[bib.bib60] Souly et al. (2024) Alexandra Souly Qingyuan Lu Dillon Bowen Tu Trinh Elvis Hsieh Sana Pandey Pieter Abbeel Justin Svegliato Scott Emmons Olivia Watkins and Sam Toyer. 2024. A StrongREJECT for Empty Jailbreaks. In ICLR 2024 Workshop on Reliable and Responsible Foundation Models. https://openreview.net/forum?id=al303JJkGO
[bib.bib23] Huang et al. (2024b) Yangsibo Huang Samyak Gupta Mengzhou Xia Kai Li and Danqi Chen. 2024b. Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation. In The Twelfth International Conference on Learning Representations. https://openreview.net/forum?id=r42tSSCHPh
[bib.bib38] Mazeika et al. (2024) Mantas Mazeika Long Phan Xuwang Yin Andy Zou Zifan Wang Norman Mu Elham Sakhaee Nathaniel Li Steven Basart Bo Li David Forsyth and Dan Hendrycks. 2024. HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal. arXiv:2402.04249 [cs.LG] https://arxiv.org/abs/2402.04249
[bib.bib73] Xie et al. (2025) Tinghao Xie Xiangyu Qi Yi Zeng Yangsibo Huang Udari Madhushani Sehwag Kaixuan Huang Luxi He Boyi Wei Dacheng Li Ying Sheng Ruoxi Jia Bo Li Kai Li Danqi Chen Peter Henderson and Prateek Mittal. 2025. SORRY-Bench: Systematically Evaluating Large Language Model Safety Refusal. In The Thirteenth International Conference on Learning Representations. https://openreview.net/forum?id=YfKNaRktan