6. Discussions
Based on previous evaluation and the great success of Constrained Decoding Attacks we discuss the implications of this attack surface on the current LLM safety landscape and propose potential defense strategies to mitigate the threat.
 \Description
 safety guardrails
 As depicted in Figure12 despite internal LLM safety alignment current LLM auditing practices predominantly focus on two distinct phases of the generation pipeline: input prompt auditing and output auditing~\cite{bib.bib6}. We will discuss each of these approaches:
 Input-focused auditing strategies typically involve extra classifiers small LLMs or rule-based filters. This approach is generally cost-effective as the generation process and validation can be run in parallel and early refusal can be triggered to stop harmful content generation. However it may fail to capture the complex dynamics of the generation process itself particularly when sophisticated jailbreak techniques are employed by adversaries.
 As for Enum Attack and more generally Constrained Decoding Attacks they can easily bypass prompt auditing by hiding malicious content within the structured output specification rather than the visible prompt. As shown in Figure8 no prompt auditing methods will refuse the query because the prompt part is benign and the harmful question is hidden in the enum field somewhere in the structured output constraints. Therefore grammar constraints are now a vulnerable path to jailbreak LLMs as their content is not audited by prompt auditing methods shown as the red line in Figure12.
 Finding 1: Auditing the decoding constraints represents a critical blind spot in current LLM safety architectures.
 On the opposite end of the generation pipeline output auditing represents a more comprehensive but resource-intensive approach to safety. This methodology involves verifying the correctness and safety of LLM-generated content while implementing sanitization processes to remove or neutralize potentially harmful elements from the output. It can happen either during generation or post generation. It seems that jailbreak is a false research problem if a sound and effective output auditing can be achieved as we can choose not to return the harmful content to the user whenever harmful content is detected.
 However output auditing faces several fundamental limitations. Unlike prompt auditing it cannot be run in parallel with the generation process as content evaluation must occur sequentially after text production. Furthermore output auditing is computationally expensive requiring an extra analysis of the generated content. Additionally even sophisticated output auditing systems are susceptible to false positives potentially resulting in the inappropriate filtering of regular harmless content.
 These constraints—latency penalties computational costs and accuracy challenges—have significantly limited the implementation of comprehensive output auditing in commercial settings because the introduced response delay can substantially degrade user experience particularly in interactive applications requiring real-time performance. Both OpenAI and Gemini APIs we evaluated appear to not implement output auditing probably due to commercial considerations.
 \Description
 BenignEnumAttack
 One might assume that implementing robust output auditing would be sufficient for a safety-first LLM service but we demonstrate this is not the case. We question whether output auditing is a silver bullet for solving LLM jailbreaking through our proposed BenignEnumAttack variant depicted in Figure13. This attack is designed to generate seemingly benign content that can pass output auditing while still delivering harmful information. This vulnerability exists because the semantic meaning of a full response is not always reducible to the sum of its parts and conventional output auditing may fail to capture the harmfulness of the full response in context. BenignEnumAttack can generate a response that is benign as a whole but contains harmful content newly generated through constrained decoding techniques.
 By strategically embedding harmful content within benign contexts this attack creates a fundamental deadlock problem in current auditing systems: the generated responses for both BenignEnumAttack and StrongREJECT evaluation appear structurally identical but one is actually harmful and the other is not. So the output auditing will fail as it will inevitably either trigger a false positive (refusing legitimate StrongREJECT evaluations) or a false negative (failing to detect BenignEnumAttack jailbreaks).
 Finding 2: Output auditing is not a silver bullet to jailbreaking as it is dead-locked by BenignEnumAttack.
 While constrained decoding enables powerful structured output capabilities in LLMs our research demonstrates that its security implications have been critically underexplored. Based on our systematic analysis of EnumAttack and its variants we propose several possible mitigation strategies that address the fundamental vulnerabilities in current implementations:
 (1)
 Safety-Preserving Grammar Constraints: The core vulnerability exploited by EnumAttack is the ability to override LLM’s safety alignment by constrained output space within user-defined grammar constraints. We propose implementing ”safety-preserving constraints” that maintain a whitelist of tokens like ”I’m sorry” which cannot be constrained by user-defined grammars. This approach would preserve the model’s ability to generate safety-critical refusals even within structured outputs. Implementation challenges include balancing strict grammar adherence with safety requirements as excessive safety tokens might disrupt valid structured outputs in certain contexts.
 (2)
 Context-Aware Token Attribution: Current output auditing processes cannot distinguish between user-prefilled content and model-generated content—a vulnerability exploited by BenignEnumAttack. A more robust approach would implement token-level provenance tracking during the generation process enabling auditing systems to differentiate between tokens originating from enum constraints (or others specified by grammars) versus those freely generated by the model. This would enable more accurate detection of attempts to manipulate the generation trajectory through constrained fields.
 (3)
 Integrated Safety Signaling: Co-design LLM with auditing signal tokens: Similar to (1) we still whitelist some tokens in the constrained decoding process but we whitelist special tokens that indicate potential auditing signals such as “¡sexual¿” when encounter sex-related context or “¡political¿” with political-related context. Models can be finetuned to generate these tokens in certain scenarios so that model generation itself is aware of possible jailbreaking and can either flag the auditing process or stop jailbroken generation.
[IMAGE START] [IMAGE URL: /mnt/bmcpfs-29000zjpjtl6xjmjiifyk/xkhu/ideation_workspace/papers/reference_figures/4b7994b4145df3404f770e6715b0cc54.png] Figure 8. Illustration code for Enum Attack, where the attack body is located in the structured output, and the prompt part is harmless, so it can bypass prompt-based auditing.[IMAGE END]

[IMAGE START] [IMAGE URL: /mnt/bmcpfs-29000zjpjtl6xjmjiifyk/xkhu/ideation_workspace/papers/reference_figures/cc6645a488433da18e5aa33f8beb1d0d.png] Figure 12. Potential External Safety Guardrails in LLMs, adopted from  ~\cite{bib.bib14}.[IMAGE END]

[IMAGE START] [IMAGE URL: /mnt/bmcpfs-29000zjpjtl6xjmjiifyk/xkhu/ideation_workspace/papers/reference_figures/82bc9a97886131a3b2f7fff541a50fd2.png] Figure 13. BenignEnumAttack (red) and a benign redteaming Q-A auditing process (blue). Existing output auditing methods cannot identify their differences, so that they will be refused or returned together, causing either false positive or false negative.[IMAGE END]



## Section References
[bib.bib6] Bell and Fonseca (2024) Andrew Bell and Joao Fonseca. 2024. Output Scouting: Auditing Large Language Models for Catastrophic Responses. arXiv:2410.05305 [cs.CL] https://arxiv.org/abs/2410.05305
[bib.bib14] Deng et al. (2024) Gelei Deng Yi Liu Yuekang Li Kailong Wang Ying Zhang Zefeng Li Haoyu Wang Tianwei Zhang and Yang Liu. 2024. MASTERKEY: Automated Jailbreaking of Large Language Model Chatbots. In NDSS. https://www.ndss-symposium.org/ndss-paper/masterkey-automated-jailbreaking-of-large-language-model-chatbots/