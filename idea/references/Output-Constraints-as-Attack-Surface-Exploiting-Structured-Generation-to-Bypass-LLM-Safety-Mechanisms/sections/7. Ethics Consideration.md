7. Ethics Consideration
Our work identifies significant security vulnerabilities in constrained decoding mechanisms for LLMs by demonstrating how guided grammar manipulation can bypass safety guardrails. We empirically show that state-of-the-art models remain vulnerable to Constrained Decoding Attacks despite robust safety alignment efforts. The security implications are particularly concerning as CDAs exploit a fundamental gap in existing safety architectures which primarily focus on input filtering rather than the generation process itself.
 By publishing this research we aim to enable the security community to address these vulnerabilities before they can be widely exploited. Since CDAs target low-level decoding processes exposed through structured output features mitigating these attacks presents unique challenges distinct from traditional jailbreak defenses. We have acknowldged our findings to affected model providers like OpenAI and Gemini and propose several mitigation strategies that could reduce risk without compromising legitimate structured output functionality. We believe our analysis provides crucial insights for developing more comprehensive safety mechanisms that protect the entire generation pipeline rather than just input and output boundaries.