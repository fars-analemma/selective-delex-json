8. Conclusion
In this study we introduce Constrained Decoding Attacks a novel attack surface which fundamentally challenges existing security paradigms for Large Language Models. By manipulating grammar-level constraints particularly through structured output features: we demonstrate how attackers can bypass both internal safety alignment and external defensive measures to achieve jailbreaks. Our proof-of-concept Enum Attack successfully jailbreaks both open-source and proprietary models with minimal queries achieving 96.2% ASR and 82.6% StrongREJECT score across five diverse benchmarks with a single query. These results significantly outperform existing orthogonal jailbreak methods and reveal a significant systematic vulnerability in current LLM safety architectures. By revealing this previously unexplored attack surface our work contributes to developing more comprehensive security paradigms for LLMs that address safety at all stages of LLM generation.