A. Comparison with Prior Work on Steering and Alignment
Table 3 provides a structured comparison between prior work on inference-time steering and training-time customization and our setting of interest. We categorize each line of work along several dimensions: whether the intervention is applied at inference or training time, whether it is motivated by a benign utility objective, whether utility improvement is the primary goal, whether safety side-effects are explicitly studied, and whether the work evaluates robustness under adversarial or jailbreak attacks. This comparison highlights a gap in the existing literature: while several prior studies examine steering as an attack vector or analyze safety regressions as a secondary effect, none focus on developer-side, utility-driven activation steering and its unintended safety externalities under black-box jailbreak evaluation. Our work isolates this previously underexplored regime, where benign, deployment-motivated steering interventions can systematically erode safety margins despite not being adversarially controlled.  ~\cite{b10} Inference ✓ ✗ ✓ ✗ Persona Steering ~\cite{b7} Inference ✗ ✓ ✓ ✓ Fine-tuning Aligned LMs ~\cite{b20} Train ✓ ✗ ✓ ✓ Likelihood Displacement in DPO ~\cite{b22} Train ✓ ✗ ✓ ✗ Steering Externality (Ours) Inference ✓ ✓ ✓ ✓

[TABLE START]Table 3 . Comparison of inference-time steering and training-time customization risks relevant to safety externalities. <table><row><cell>Work</cell><cell>Phase</cell><cell cols="4">Benign objective priority Utility Studied safety Attack/ side-effects JB Eval</cell></row><row><cell>Refusal Direction Interventions (Arditi et al., 2024)</cell><cell>Inference</cell><cell>✗</cell><cell>✗</cell><cell>✓</cell><cell>✓</cell></row><row><cell>Rogue Scalpel</cell><cell/><cell/><cell/><cell/><cell/></row></table>[TABLE END]


Section references:
[b10]: A Korznikov, A Galichin, A Dontsov, O Rogov, I Oseledets, E Tutubalina. The rogue scalpel: Activation steering compromises llm safety. (2025). The rogue scalpel: Activation steering compromises llm safety
[b20]: X Qi, Y Zeng, T Xie, P.-Y Chen, R Jia, P Mittal, P Henderson. Fine-tuning aligned language models compromises safety, even when users do not intend to!. (2023). Fine-tuning aligned language models compromises safety, even when users do not intend to!
[b22]: N Razin, S Malladi, A Bhaskar, D Chen, S Arora, B Hanin. Unintentional unalignment: Likelihood displacement in direct preference optimization. (2025). Unintentional unalignment: Likelihood displacement in direct preference optimization
[b7]: A Ghandeharioun, A Yuan, M Guerard, E Reif, M Lepori, L Dixon. Who's asking? user personas and the mechanics of latent misalignment. (2024). Who's asking? user personas and the mechanics of latent misalignment