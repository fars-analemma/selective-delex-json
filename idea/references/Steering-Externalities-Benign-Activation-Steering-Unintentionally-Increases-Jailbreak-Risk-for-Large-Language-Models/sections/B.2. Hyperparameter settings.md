B.2. Hyperparameter settings
Since we employ two distinct steering methodologies, it is necessary to discuss the specific hyperparameters selected for each. The optimal configuration varies between methods due to the nature of the steering vectors and the tasks they target. Below, we detail the hyperparameter settings chosen for STEER-COMPLIANCE and STEER-JSON: • STEER-COMPLIANCE: This method involves two key hyperparameters: i) Steering strength (Coefficient) and ii) Steering layers. To determine the optimal steering coefficient, we conducted an ablation study on Llama-3-8B-Instruct, sampling the coefficient α from the set [0, 0.5, 1.0, 1.5, 2.0]. We measured compliance utility (defined as the harmless refusal rate) on 100 harmful questions sampled from SorryBench, as well as the model's ability to generate answers for 100 benign questions sampled from Alpaca (measured via win-rate). The results are summarized in Figure 5. Our objective was to identify a coefficient that maximizes the Win-Rate on benign tasks while simultaneously maintaining a low refusal rate. Based on the plot, we selected the coefficient where the performance intersects with the baseline Win-Rate of the original model (indicated by the blue dotted line). This intersection occurs at approximately 1.3. We utilized this coefficient for the remainder of the experiments. For the consistency of experiment, we follow the basic implementation of CAST in terms of steering layers, in which we keep the steering layers as layer 15,17,18,19,20,21,22,23,24, which is consistent with colab implementation of ~\cite{b11} across all models in each experiment. • STEER-JSON: Unlike STEER-COMPLIANCE, our instruction-following approach dynamically determines the strength coefficient according to Eq.4. Regarding the selection of steering layers, we adopt the grid search method used by ~\cite{b23} to identify the optimal layer for maximizing JSON Correctness. Consequently, we utilize layer 15 for Gemma-7B-it, layer 16 for Llama-2-7B-Chat, and layer 6 for Llama-3-8B-Instruct. All selected layers were identified via the instruction-following algorithm.

[IMAGE START]Figure 5 . Figure 5. An ablation study on Llama-3-8B-Instruct by varying the coefficient of steering strength. We plot out two lines, the blue line indicates the Win-Eate which measure the ability of LLMs generating on benign questions sampled from Alpaca after steering and red line indicates the Refusal Rate on harmful questions sampled from SorryBench.[IMAGE END]


Section references:
[b11]: B Lee, I Padhi, K Ramamurthy, E Miehling, P Dognin, M Nagireddy, A Dhurandhar. Programming refusal with conditional activation steering. (2025). Programming refusal with conditional activation steering
[b23]: A Stolfo, V Balachandran, S Yousefi, E Horvitz, B Nushi. Improving instruction-following in language models through activation steering. (2025). Improving instruction-following in language models through activation steering