D. Per-token KL Divergence on Gemma-7B-it model
In Sec. 5, we give some mechanic explanation of why steering externalities will occur. We present per-token KL Divergence graphs on Llama-3-8B-Instruct including both compliance and JSON steering. We found that benign steering causes changes (KL divergence) as significant as utility changes. We also present the KL Divergence graph on Gemma-7B-it model.  Similar observation can be made from Figure 7 and Figure 8, for both STEER-COMPLIANCE and STEER-JSON, the KL divergence peaks during the initial tokens before stabilizing. This indicates that steering primarily impacts the critical "refusal gate", shifting probability mass from refusal templates to non-refusal openers. Because generation is autoregressive, this early distributional shift effectively sets a new mode; once the refusal prefix is bypassed, the model naturally continues along a potentially harmful trajectory. This mechanism applies even to benign formatting tasks like JSON steering, confirming that steering degrades safety by eroding the initial safety margin.

[IMAGE START]Figure 7 . Figure 7. Per-token KL Divergence between Original and Compliance Steered Models on Gemma-7B-it. Red line indicates the KL Divergence on Harmbench responses, blue line highlights the KL Divergence on Alpaca (Benign) responses.[IMAGE END]


[IMAGE START]Figure 8 . Figure 8. Per-token KL Divergence between Original and JSON Steered Models on Gemma-7B-it. Red line indicates the KL Divergence on Harmbench responses, blue line highlights the KL Divergence on Alpaca (Benign) responses.[IMAGE END]
