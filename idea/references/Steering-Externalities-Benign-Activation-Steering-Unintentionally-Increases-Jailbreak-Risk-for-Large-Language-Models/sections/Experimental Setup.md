Experimental Setup
Dataset: To generate STEER-COMPLIANCE vectors, we randomly sampled 100 benign instructions from Alpaca ~\cite{b12} and paired each with both a refusal and a compliant response, following the procedure described in Sec.3.2. For Instruction-Following Steering vectors-specifically for STEER-JSON-we randomly sampled 400 data instances containing JSON-specific instructions from the Instruction-Following Evaluation Dataset (IFEval) (Zhou et al., 2023). To measure benign utility preservation under steering, we (i) evaluated refusal rates on 100 harmless Alpaca prompts (compliance/helpfulness utility), and (ii) evaluated JSON validity on 100 prompts from IFEval (format-following utility). For both sets, we ensured that the evaluation prompts were distinct from those used for steering vector construction. For our safety evaluation, we utilized the full HarmBench dataset ~\cite{b15}, which contains 400 harmful queries that cover a wide range of attack topics. Large Language Models: Our experiments were conducted on three open-weight models: Llama-2-7B-Chat ~\cite{b26}, Llama-3-8B-Instruct ~\cite{b8} and Gemma-7B-it ~\cite{b25}. A steered model refers to the corresponding base model augmented with a fixed steering vector; in all evaluations, this configuration was treated as a black-box target for jailbreak analysis. Llama2-7b-Chat Llama3-8b-Instruct Gemma-7b-it Models 0 10 20 30 40 50 ASR (%) 0.0% 2.0% 3.2% 11.5% 20.0% 15.5% 16.0% 38.5% 25.5% Intrinsic Vulnerability Original JSON Compliance Llama2-7b-Chat Llama3-8b-Instruct Gemma-7b-it Models 0 20 40 60 80 100 ASR (%) 77.0% 71.0% 71.0% 88.8% 79.5% 90.2% 94.2% 81.2% 93.5% Synergistic Vulnerability CoP Original CoP + JSON CoP + Compliance Figure 2. Attack Success Rate (ASR) between original target LLMs and compliance steered LLMs as well as ASR by applying black-box jailbreak attack CoP on original and steered models respectively on 400 HarmBench data. After steering, all LLMs are more vulnerable to jailbreak attacks. Evaluation Protocol: To assess the utility of compliance ability of activation steering, we primarily focused on measuring the Refusal Rate. We evaluated by utilizing the finetuned DistilRoberta-Based model ~\cite{b19}, which is a model that identifies rejections in LLMs' response. For instruction-following utility, we followed the same evaluation protocol as ~\cite{b23}) that explicitly designs an evaluation function to judge whether the output response can be loaded as JSON formats. The details can be found in Appendix B. For measuring the jailbreak success, we evaluated the Attack Success Rate (ASR) metric with the Harmbench classifier, which is a carefully fine-tuned Llama-2-13B model to determine whether the jailbreak response is relevant to the original malicious query and harmful. A detailed description of the hyperparameter settings is provided in Appendix B.2.

Section references:
[b12]: X Li, T Zhang, Y Dubois, R Taori, I Gulrajani, C Guestrin, P Liang, T Hashimoto. Alpacaeval: An automatic evaluator of instruction-following models. (2023). Alpacaeval: An automatic evaluator of instruction-following models
[b15]: M Mazeika, L Phan, X Yin, A Zou, Z Wang, N Mu, E Sakhaee, N Li, S Basart, B Li, D Forsyth, D Hendrycks. A standardized evaluation framework for automated red teaming and robust refusal. (2024). A standardized evaluation framework for automated red teaming and robust refusal
[b19]: Fine-tuned distilroberta-base for rejection in the output detection. (2024). Fine-tuned distilroberta-base for rejection in the output detection
[b23]: A Stolfo, V Balachandran, S Yousefi, E Horvitz, B Nushi. Improving instruction-following in language models through activation steering. (2025). Improving instruction-following in language models through activation steering
[b25]: G Team, T Mesnard, C Hardin, R Dadashi, S Bhupatiraju, S Pathak, L Sifre, M Rivière, M Kale, J Love. Open models based on gemini research and technology. (2024). Open models based on gemini research and technology
[b26]: H Touvron, T Lavril, G Izacard, X Martinet, M Lachaux, T Lacroix, B Rozière, N Goyal, E Hambro, F Azhar, A Rodriguez, A Joulin, E Grave, G Lample. Llama: Open and efficient foundation language models. (2023). Llama: Open and efficient foundation language models
[b8]: A Grattafiori, A Dubey, A Jauhri, A Pandey, A Kadian, A Al-Dahle, A Letman, A Mathur, A Schelten, A Vaughan. The llama 3 herd of models. (2024). The llama 3 herd of models