H. Win-Rate measurement of Compliance Steering
As Sec. 4 shows, adding both STEER-COMPLIANCE and STEER-JSON improves overall utility by lowering the harmless refusal rate and increasing JSON extraction in the response. A natural question arises: how well does the steered model perform in terms of general response quality? In this section, we introduce an additional utility measurement: â€¢ Win-Rate measures whether the responses generated by a given LLM are better than those generated by a reference model. In our evaluation, we use GPT-4 as the reference model (judge). The purpose of this metric is to assess the general capability of LLM responses after steering. We follow the procedures described in Sec.4.1 and perform compliance steering on three target models. We sampled 100 questions from Alpaca and evaluated the Win-Rate, as shown in Table 6. As the compliance behavior vectors are injected into the target models, we observe an overall increasing trend in Win-Rate. In particular, the Llama-2-7B-Chat-hf model originally had a Win-Rate of 0.31, which increased to 4.67 after compliance steering. This further supports the hypothesis that model developers prioritize improving overall generation quality (i.e., utility).

[TABLE START]Table 6 . Length invariant Win-Rate by applying compliance steering on original models and evaluated on 100 Alpaca questions. After steering, all LLMs have higher Win-Rate indicating the overall generation qualities are improved. <table><row><cell>Models</cell><cell cols="2">ORI (Win-Rate) STEER COMPLIANCE (Win-Rate)</cell></row><row><cell>Llama-3-8B-Instruct</cell><cell>2.51</cell><cell>2.79</cell></row><row><cell>Llama-2-7B-Chat-hf</cell><cell>0.31</cell><cell>4.67</cell></row><row><cell>Gemma-7B-it</cell><cell>2.38</cell><cell>3.88</cell></row></table>[TABLE END]
