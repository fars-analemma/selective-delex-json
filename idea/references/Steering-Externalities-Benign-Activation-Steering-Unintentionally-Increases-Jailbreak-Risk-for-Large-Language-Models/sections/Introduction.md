Introduction
Large language models (LLMs) (Vaswani et al., 2023) are commonly deployed as instruction-following assistants (Bai et al., 2022a;c), where users expect helpful, coherent, and instruction-adherent responses, while providers rely on a combination of behavioral controls -such as refusal mech-anisms, persona conditioning, and output-format constraints -to allow flexibility in adapting deployed models. Maintaining this balance is challenging: training-based alignment mechanisms such as supervised fine-tuning (Zhang et al., 2024;~\cite{b24} and preference optimization ~\cite{b1}Bai et al., 2022b;~\cite{b18} can improve refusal behavior, yet models remain vulnerable to prompt-based jailbreaks and automated redteaming procedures (Zou et al., 2023;~\cite{b13}Xiong et al., 2025). At the same time, practitioners increasingly seek methods to control model behavior without retraining, both for cost reasons and for rapid iteration. Activation steering ~\cite{b27} has emerged as a practical post-training control primitive for LLMs. Rather than modifying model weights, activation steering injects vectors into the model's hidden states at inference time, biasing generation toward desired attributes such as increased helpfulness, stylistic consistency, persona conditioning (e.g., adopting a specific role or tone) ~\cite{b6}, or stricter instruction adherence (e.g., enforcing structured outputs like JSON) ~\cite{b23}. Because these interventions operate post hoc and do not require retraining, activation steering enables rapid, cost-effective behavioral customization, making it attractive in real-world deployment settings where model service providers must simultaneously support multiple control requests for different users. Existing studies that examine activation steering and safety largely adopt an attacker or diagnostic perspective: they intentionally apply steering vectors to probe or bypass refusal mechanisms ~\cite{b0,b7,b10}. In these settings, activation steering is treated as an attack vector that can be arbitrarily used by a malicious actor, and therefore, safety degradation is an expected outcome. In contrast, we study activation steering from the perspective of a model developer, where steering is applied to improve benign utility and is not accessible to attackers. As illustrated in Figure 1, our focus is on studying how such developer-side, utility-driven steering can nonetheless introduce unintended safety regressions, increasing susceptibility to jailbreak attacks even though the steering mechanism is not adversarially controlled. Our study complements prior work which shows even benign  et al., 2023;Xiong et al., 2025;~\cite{b16}. We distinguish two evaluation regimes: (i) Benchmark-only, which evaluates on the original harmful prompts provided by the dataset (direct harmful requests; no prompt rewriting), and (ii) Synergistic Vulnerability, which runs an attack algorithm that iteratively revises the harmful request based on the target steered model's feedback. The right section quantifies these averaged externalities across the three tested models (i.e., Llama-2-7B-Chat, Llama-3-8B-Instruct and Gemma-7B-it). The results show that while steering successfully modifies behavior-such as increasing harmless non-refusal rates (i.e. 100% minus refusal rates) for benign queries or improving JSON extraction-it unintentionally compromises safety. This leads to higher Attack Success Rates on harmful queries compared to the original models, an effect that is amplified under jailbreak attacks. operations -post fine-tuning ~\cite{b20} or preference optimization ~\cite{b22}) -can unintentionally degrade safety alignment and increase vulnerability to misuse. These results motivate a broader question: Do steering vectors learned from benign instruction data accidentally have any other side-effects? In this paper, we present a new practical safety risk that we termed steering externalities: unintended safety regressions that arise when activation steering is optimized for benign utility. Concretely, we focus on realistic workflows: a model developer uses a benign dataset and tries to make the model "more compliant" ~\cite{b11} or produce "more formatted responses" ~\cite{b23}) (e.g., more likely to produce direct, helpful answers, or using JSON as output format) by learning a steering direction from either contrastive examples or instruction-following. Despite these benign objectives, we find that the resulting steered models exhibit substantial safety degradation, with attack success rates under State-Of-The-Art (SOTA) jailbreak methods increasing by up to 99% compared to the original aligned models. Why would benign activation steering compromise safety? Our central hypothesis is that benign activation steering compromises safety by systematically biasing the model's early-token distribution toward non-refusal trajectories, thereby reducing the "safety margin" that alignment relies on to refuse harmful requests. Specifically, utilityoriented steering (e.g., compliance or formatting) increases the likelihood of affirmative or structured openings in the first few generated tokens, implicitly suppressing refusalpreferring prefixes that safety training places disproportionate weight on. As a result, even when the steering objective is benign, the model becomes more likely to enter a non-refusal mode at generation onset, making it easier for adversaries to elicit disallowed behavior. Importantly, this effect does not require novel jailbreak techniques: a modest reduction in refusal robustness at the prefix level can substantially amplify the effectiveness of existing automated jailbreak pipelines. This vulnerability is exacerbated by modern jailbreak methods that rely on search (e.g., iterative rewriting, multi-step strategies, or agentic decomposition), where a model that is only slightly more willing to comply can become dramatically easier to jailbreak in practice. Our paper makes three primary contributions: 1. Identification of Steering Externalities: We define and empirically demonstrate "steering externalities", a phenomenon where activation steering optimized solely for benign utility (such as "compliance" or "Instruction-following") systematically degrades safety alignment. We show that this trade-off is not limited to semantic steering but extends to syntactic formatting constraints, challenging the assumption that benign test-time model adaptation is inherently safe. 2. Jailbreak Amplification Effect: We establish that benign steering acts as a "force multiplier" for adversarial attacks. Through comprehensive evaluation on Llama-2-7B-Chat, Llama-3-8B-Instruct, and Gemma-7B-it, we show that steering interventions drastically increase the Attack Success Rate (ASR) of black-box jailbreaks (CoP, PAIR, TAP), in some cases boosting ASR to nearly 99%, by eroding the model's safety margin. 3. Mechanistic Explanation via Hidden Safety Fractures. We provide mechanistic evidence that benign activation steering induces an implicit domain shift in the model's internal state: it benignizes harmful requests by pushing their prompt representations toward harmless subspace, thereby shrinking the representational safety margin that normally triggers refusal. This shift manifests immediately at generation time as a concentrated change in the first few output tokenstoken-wise KL spikes show that steering suppresses refusal-prefixed openings and increases the probability of a non-refusal start. Once the model is "tricked" into beginning in a benign-coded, non-refusal mode, autoregressive generation amplifies the effect and carries the trajectory toward harmful completion, even though no explicit safety mechanism is removed. Overall, our results complement and extend prior warnings that activation steering can compromise alignment safeguards, by showing that even steering learned exclusively from benign data and operated only by the model service provider for utility enhancement -can systematically increase practical jailbreakability. Based on our findings, we also provide discussions on possible mitigation strategies and potential research topics for future studies.

Section references:
[b0]: A Arditi, O Obeso, A Syed, D Paleka, N Panickssery, W Gurnee, N Nanda. Refusal in language models is mediated by a single direction. (2024). Refusal in language models is mediated by a single direction
[b1]: A Askell, Y Bai, A Chen, D Drain, D Ganguli, T Henighan, A Jones, N Joseph, B Mann, N Das-Sarma, N Elhage, Z Hatfield-Dodds, D Hernandez, J Kernion, K Ndousse, C Olsson, D Amodei, T Brown, J Clark, S Mccandlish, C Olah, J Kaplan. . (2021)
[b10]: A Korznikov, A Galichin, A Dontsov, O Rogov, I Oseledets, E Tutubalina. The rogue scalpel: Activation steering compromises llm safety. (2025). The rogue scalpel: Activation steering compromises llm safety
[b11]: B Lee, I Padhi, K Ramamurthy, E Miehling, P Dognin, M Nagireddy, A Dhurandhar. Programming refusal with conditional activation steering. (2025). Programming refusal with conditional activation steering
[b13]: X Liu, P Li, E Suh, Y Vorobeychik, Z Mao, S Jha, P Mcdaniel, H Sun, B Li. Autodanturbo: A lifelong agent for strategy self-exploration to jailbreak llms. (2024). Autodanturbo: A lifelong agent for strategy self-exploration to jailbreak llms
[b16]: A Mehrotra, M Zampetakis, P Kassianik, B Nelson, H Anderson, Y Singer, A Karbasi. Tree of attacks: Jailbreaking black-box llms automatically. (2023). Tree of attacks: Jailbreaking black-box llms automatically
[b18]: L Ouyang, J Wu, X Jiang, D Almeida, C Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, J Schulman, J Hilton, F Kelton, L Miller, M Simens, A Askell, P Welinder, P Christiano, J Leike, R Lowe. . (2022)
[b20]: X Qi, Y Zeng, T Xie, P.-Y Chen, R Jia, P Mittal, P Henderson. Fine-tuning aligned language models compromises safety, even when users do not intend to!. (2023). Fine-tuning aligned language models compromises safety, even when users do not intend to!
[b22]: N Razin, S Malladi, A Bhaskar, D Chen, S Arora, B Hanin. Unintentional unalignment: Likelihood displacement in direct preference optimization. (2025). Unintentional unalignment: Likelihood displacement in direct preference optimization
[b23]: A Stolfo, V Balachandran, S Yousefi, E Horvitz, B Nushi. Improving instruction-following in language models through activation steering. (2025). Improving instruction-following in language models through activation steering
[b24]: F Tajwar, A Singh, A Sharma, R Rafailov, J Schneider, T Xie, S Ermon, C Finn, A Kumar. Preference fine-tuning of llms should leverage suboptimal, on-policy data. (2024). Preference fine-tuning of llms should leverage suboptimal, on-policy data
[b27]: A Turner, L Thiergart, G Leech, D Udell, J Vazquez, U Mini, M Macdiarmid. Steering language models with activation engineering. (2024). Steering language models with activation engineering
[b6]: R Chen, A Arditi, H Sleight, O Evans, J Lindsey. Persona vectors: Monitoring and controlling char-acter traits in language models. (2025). Persona vectors: Monitoring and controlling char-acter traits in language models
[b7]: A Ghandeharioun, A Yuan, M Guerard, E Reif, M Lepori, L Dixon. Who's asking? user personas and the mechanics of latent misalignment. (2024). Who's asking? user personas and the mechanics of latent misalignment