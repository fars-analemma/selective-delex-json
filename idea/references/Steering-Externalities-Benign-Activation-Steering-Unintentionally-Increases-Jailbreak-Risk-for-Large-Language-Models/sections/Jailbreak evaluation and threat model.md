Jailbreak evaluation and threat model
Threat model. To evaluate whether benign activation steering increases adversarial vulnerability, we adopt a blackbox threat model. We treat the steered model-defined by fixed base parameters θ together with a fixed, developercontrolled steering intervention-as the target system. The attacker has no access to or control over the steering mechanism, does not observe internal activations, and cannot modify the steering vector. Instead, the attacker adaptively executes jailbreak attempts by interacting with the model solely through input-output queries, as illustrated in Figure 1 (Attacker's Move). Given a harmful intent x, an attack algorithm produces an adversarial prompt x adv . The model then generates under active steering: y ∼ p steer θ (• | x adv ). (5) We evaluate both (i) intrinsic vulnerability of the steered model (without external attacks) and (ii) synergistic vulnerability when steering is combined with black-box jailbreak pipelines. In the latter setting, we instantiate the attack algorithm using three representative prompt-only methods, all of which operate without access to internal activations or control over the steering mechanism. Unless otherwise stated, the steering vector remains fixed throughout the attack process, and safety is assessed using the same downstream evaluation judge across all conditions. Specifically, we consider: Table 1. Utility checks for benign steering. Compliance steering reduces refusals on harmless Alpaca prompts, and JSON-format steering increases JSON-valid outputs on IFEval. Model Harmless Alpaca refusal (↓ better utility) IFEval JSON correctness (↑ better utility) Original COMPLIANCE STEERING Original JSON STEERING Llama-2-7B-Chat 9% 6% 61% 74% Llama-3-8B-Instruct 2% 0% 63% 69% Gemma-7B-it 18% 1% 69% 81% • Composition-of-Principles (CoP) (Xiong et al., 2025): an agentic workflow that combines multiple persuasive principles to find successful jailbreak prompts. • Prompt Automatic Iterative Refinement (PAIR) ~\cite{b5}: an automatic black-box attack that iteratively optimizes prompts based on model responses. • Tree of Attacks with Pruning (TAP) ~\cite{b16}: an extension of PAIR that explores a tree of candidate jailbreak prompts via search and pruning. In addition to measuring jailbreak success, we explicitly measure utility preservation under steering. This joint evaluation enables us to characterize steering externalities as a trade-off: steering improves benign utility while simultaneously increasing susceptibility to jailbreak attacks.

Section references:
[b16]: A Mehrotra, M Zampetakis, P Kassianik, B Nelson, H Anderson, Y Singer, A Karbasi. Tree of attacks: Jailbreaking black-box llms automatically. (2023). Tree of attacks: Jailbreaking black-box llms automatically
[b5]: P Chao, A Robey, E Dobriban, H Hassani, G Pappas, E Wong. Jailbreaking black box large language models in twenty queries. (2023). Jailbreaking black box large language models in twenty queries