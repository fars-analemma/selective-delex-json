Potential Risks and Mitigations
We acknowledge that this research involves the study of jailbreaking dynamics and demonstrates how steering can act as a "force multiplier" for adversarial attacks. While this knowledge could theoretically be leveraged by malicious actors to bypass safety filters, we believe that the vulnerability exists regardless of its public disclosure. The steering vectors studied here are derived from benign data (e.g., standard instruction tuning), meaning developers might be deploying compromised models without realizing it. Therefore, we believe the benefits of exposing this "steering externality"-to enable the development of defenses such as STEER-BIND-outweigh the risks of disclosure. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention is all you need, 2023. Xie, T., Qi, X., Zeng, Y., Huang, Y., Sehwag, U. M., Huang, K., He, L., Wei, B., Li, D., Sheng, Y., Jia, R., Li, B., Li, K., Chen, D., Henderson, P., and Mittal, P. Sorrybench: Systematically evaluating large language model safety refusal, 2025. URL https://arxiv.org/  abs/2406.14598[https://arxiv.org/abs/2406.14598]. Xiong, C., Chen, P.-Y., and Ho, T.-Y. Cop: Agentic redteaming for large language models using composition of principles, 2025. URL https://arxiv.org/abs/  2506.00781[https://arxiv.org/abs/2506.00781]. Zhang, B., Liu, Z., Cherry, C., and Firat, O. When scaling meets llm finetuning: The effect of data, model and finetuning method, 2024. URL https://arxiv.org/  abs/2402.17193[https://arxiv.org/abs/2402.17193]. Zhou, J., Lu, T., Mishra, S., Brahma, S., Basu, S., Luan, Y., Zhou, D., and Hou, L. Instruction-following evaluation for large language models, 2023. URL https:  //arxiv.org/abs/2311.07911[https://arxiv.org/abs/2311.07911]. Zou, A., Wang, Z., Kolter, J. Z., and Fredrikson, M. Universal and transferable adversarial attacks on aligned language models. CoRR, abs/2307.15043, 2023.