References
[b0]: A Arditi, O Obeso, A Syed, D Paleka, N Panickssery, W Gurnee, N Nanda. Refusal in language models is mediated by a single direction. (2024). Refusal in language models is mediated by a single direction
[b1]: A Askell, Y Bai, A Chen, D Drain, D Ganguli, T Henighan, A Jones, N Joseph, B Mann, N Das-Sarma, N Elhage, Z Hatfield-Dodds, D Hernandez, J Kernion, K Ndousse, C Olsson, D Amodei, T Brown, J Clark, S Mccandlish, C Olah, J Kaplan. . (2021)
[b2]: Y Bai, A Jones, K Ndousse, A Askell, A Chen, N Das-Sarma, D Drain, S Fort, D Ganguli, T Henighan, N Joseph, S Kadavath, J Kernion, T Conerly, S El-Showk, N Elhage, Z Hatfield-Dodds, D Hernandez, T Hume, S Johnston, S Kravec, L Lovitt, N Nanda, C Olsson, D Amodei, T Brown, J Clark, S Mccandlish, C Olah, B Mann, J Kaplan. Training a helpful and harmless assistant with reinforcement learning from human feedback. (2022). Training a helpful and harmless assistant with reinforcement learning from human feedback
[b3]: Y Bai, A Jones, K Ndousse, A Askell, A Chen, N Das-Sarma, D Drain, S Fort, D Ganguli, T Henighan, N Joseph, S Kadavath, J Kernion, T Conerly, S El-Showk, N Elhage, Z Hatfield-Dodds, D Hernandez, T Hume, S Johnston, S Kravec, L Lovitt, N Nanda, C Olsson, D Amodei, T Brown, J Clark, S Mccandlish, C Olah, B Mann, J Kaplan. 
[b4]: Y Bai, S Kadavath, S Kundu, A Askell, J Kernion, A Jones, A Chen, A Goldie, A Mirhoseini, C Mckinnon, C Chen, C Olsson, C Olah, D Hernandez, D Drain, D Ganguli, D Li, E Tran-Johnson, E Perez, J Kerr, J Mueller, J Ladish, J Landau, K Ndousse, K Lukosuite, L Lovitt, M Sellitto, N Elhage, N Schiefer, N Mercado, N Dassarma, R Lasenby, R Larson, S Ringer, S Johnston, S Kravec, S Showk, S Fort, T Lanham, T Telleen-Lawton, T Conerly, T Henighan, T Hume, S Bowman, Z Hatfield-Dodds, B Mann, D Amodei, N Joseph, S Mccandlish, T Brown, J Kaplan. Harmlessness from ai feedback. (2022). Harmlessness from ai feedback
[b5]: P Chao, A Robey, E Dobriban, H Hassani, G Pappas, E Wong. Jailbreaking black box large language models in twenty queries. (2023). Jailbreaking black box large language models in twenty queries
[b6]: R Chen, A Arditi, H Sleight, O Evans, J Lindsey. Persona vectors: Monitoring and controlling char-acter traits in language models. (2025). Persona vectors: Monitoring and controlling char-acter traits in language models
[b7]: A Ghandeharioun, A Yuan, M Guerard, E Reif, M Lepori, L Dixon. Who's asking? user personas and the mechanics of latent misalignment. (2024). Who's asking? user personas and the mechanics of latent misalignment
[b8]: A Grattafiori, A Dubey, A Jauhri, A Pandey, A Kadian, A Al-Dahle, A Letman, A Mathur, A Schelten, A Vaughan. The llama 3 herd of models. (2024). The llama 3 herd of models
[b9]: J Ji, M Liu, J Dai, X Pan, C Zhang, C Bian, C Zhang, R Sun, Y Wang, Y Yang. Beavertails: Towards improved safety alignment of llm via a human-preference dataset. (2023). Beavertails: Towards improved safety alignment of llm via a human-preference dataset
[b10]: A Korznikov, A Galichin, A Dontsov, O Rogov, I Oseledets, E Tutubalina. The rogue scalpel: Activation steering compromises llm safety. (2025). The rogue scalpel: Activation steering compromises llm safety
[b11]: B Lee, I Padhi, K Ramamurthy, E Miehling, P Dognin, M Nagireddy, A Dhurandhar. Programming refusal with conditional activation steering. (2025). Programming refusal with conditional activation steering
[b12]: X Li, T Zhang, Y Dubois, R Taori, I Gulrajani, C Guestrin, P Liang, T Hashimoto. Alpacaeval: An automatic evaluator of instruction-following models. (2023). Alpacaeval: An automatic evaluator of instruction-following models
[b13]: X Liu, P Li, E Suh, Y Vorobeychik, Z Mao, S Jha, P Mcdaniel, H Sun, B Li. Autodanturbo: A lifelong agent for strategy self-exploration to jailbreak llms. (2024). Autodanturbo: A lifelong agent for strategy self-exploration to jailbreak llms
[b14]: L Maaten, G Hinton. Visualizing data using tsne. (2008-11)
[b15]: M Mazeika, L Phan, X Yin, A Zou, Z Wang, N Mu, E Sakhaee, N Li, S Basart, B Li, D Forsyth, D Hendrycks. A standardized evaluation framework for automated red teaming and robust refusal. (2024). A standardized evaluation framework for automated red teaming and robust refusal
[b16]: A Mehrotra, M Zampetakis, P Kassianik, B Nelson, H Anderson, Y Singer, A Karbasi. Tree of attacks: Jailbreaking black-box llms automatically. (2023). Tree of attacks: Jailbreaking black-box llms automatically
[b17]: N Nanda, J Bloom. . (2022)
[b18]: L Ouyang, J Wu, X Jiang, D Almeida, C Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, J Schulman, J Hilton, F Kelton, L Miller, M Simens, A Askell, P Welinder, P Christiano, J Leike, R Lowe. . (2022)
[b19]: Fine-tuned distilroberta-base for rejection in the output detection. (2024). Fine-tuned distilroberta-base for rejection in the output detection
[b20]: X Qi, Y Zeng, T Xie, P.-Y Chen, R Jia, P Mittal, P Henderson. Fine-tuning aligned language models compromises safety, even when users do not intend to!. (2023). Fine-tuning aligned language models compromises safety, even when users do not intend to!
[b21]: X Qi, A Panda, K Lyu, X Ma, S Roy, A Beirami, P Mittal, P Henderson. Safety alignment should be made more than just a few tokens deep. (2024). Safety alignment should be made more than just a few tokens deep
[b22]: N Razin, S Malladi, A Bhaskar, D Chen, S Arora, B Hanin. Unintentional unalignment: Likelihood displacement in direct preference optimization. (2025). Unintentional unalignment: Likelihood displacement in direct preference optimization
[b23]: A Stolfo, V Balachandran, S Yousefi, E Horvitz, B Nushi. Improving instruction-following in language models through activation steering. (2025). Improving instruction-following in language models through activation steering
[b24]: F Tajwar, A Singh, A Sharma, R Rafailov, J Schneider, T Xie, S Ermon, C Finn, A Kumar. Preference fine-tuning of llms should leverage suboptimal, on-policy data. (2024). Preference fine-tuning of llms should leverage suboptimal, on-policy data
[b25]: G Team, T Mesnard, C Hardin, R Dadashi, S Bhupatiraju, S Pathak, L Sifre, M Rivière, M Kale, J Love. Open models based on gemini research and technology. (2024). Open models based on gemini research and technology
[b26]: H Touvron, T Lavril, G Izacard, X Martinet, M Lachaux, T Lacroix, B Rozière, N Goyal, E Hambro, F Azhar, A Rodriguez, A Joulin, E Grave, G Lample. Llama: Open and efficient foundation language models. (2023). Llama: Open and efficient foundation language models
[b27]: A Turner, L Thiergart, G Leech, D Udell, J Vazquez, U Mini, M Macdiarmid. Steering language models with activation engineering. (2024). Steering language models with activation engineering