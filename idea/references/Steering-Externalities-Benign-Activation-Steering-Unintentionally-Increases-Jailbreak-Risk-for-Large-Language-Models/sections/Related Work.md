Related Work
Post-training behavior modification can occur either at inference time (e.g., activation steering interventions on hidden states) or at training time (e.g., fine-tuning or preference optimization). Across both regimes, recent work has shown that even targeted interventions can have non-obvious failure modes, including degraded safety alignment and increased susceptibility to adversarial use. Training-time misalignment in fine-tuning and preference optimization. Complementary work shows that safety regressions can also arise during training-time customization, including in settings that are not intended to remove guardrails. Qi et al. (2023) find that fine-tuning aligned language models introduces a new attack surface: a small number of adversarially constructed fine-tuning examples can jailbreak safety protections, and even benign fine-tuning on commonly used utility datasets can inadvertently degrade safety alignment. Razin et al. (2025) analyze Direct Preference Optimization (DPO) and identify likelihood displacement, where preference margins increase while the likelihood of both preferred and dispreferred responses decreases; in catastrophic cases, probability mass can shift toward responses with opposite meaning. They show that this path can cause unintentional unalignment even if DPO is trained with the benign goal of refusing unsafe prompts. Inference-time steering as an attacker. A growing body of work studies activation steering under an attacker framing, showing that steering directions can bypass or suppress refusal mechanisms. Arditi et al. ( 2024) identify a "refusal direction" whose removal induces compliance and whose addition induces refusal. ~\cite{b10} show that steering can break safety even when the steering direction is not explicitly designed for jailbreaks (e.g., random directions or directions derived from SAE features), highlighting the fragility of safety under hidden-state interventions. ~\cite{b7} demonstrate that persona steering can elicit harmful behaviors more effectively than prompting, further emphasizing that inference-time control mechanisms can be misused to circumvent aligned behavior. Steering externalities under utility-first deployment. In contrast to work that frames steering primarily as an attacker tool, our focus is on the common deployment setting where activation steering is applied by model developers to improve utility on benign tasks. We study steering externalities: unintended safety regressions that emerge despite utility-first intent and despite learning steering vectors from benign data (see Table 3). Further, we show that these regressions can compound with black-box jailbreak pipelines, amplifying attack success rates even when the steering intervention is not purposely designed for harmful behavior. While our analysis is strictly concerned with inference-time, post-training interventions, our findings parallel concerns raised in training-time customization-namely, that seemingly benign modifications can quietly erode safety guarantees-highlighting a broader fragility of alignment under post hoc control.

[TABLE START]Table 3 . Comparison of inference-time steering and training-time customization risks relevant to safety externalities. <table><row><cell>Work</cell><cell>Phase</cell><cell cols="4">Benign objective priority Utility Studied safety Attack/ side-effects JB Eval</cell></row><row><cell>Refusal Direction Interventions (Arditi et al., 2024)</cell><cell>Inference</cell><cell>✗</cell><cell>✗</cell><cell>✓</cell><cell>✓</cell></row><row><cell>Rogue Scalpel</cell><cell/><cell/><cell/><cell/><cell/></row></table>[TABLE END]


Section references:
[b10]: A Korznikov, A Galichin, A Dontsov, O Rogov, I Oseledets, E Tutubalina. The rogue scalpel: Activation steering compromises llm safety. (2025). The rogue scalpel: Activation steering compromises llm safety
[b7]: A Ghandeharioun, A Yuan, M Guerard, E Reif, M Lepori, L Dixon. Who's asking? user personas and the mechanics of latent misalignment. (2024). Who's asking? user personas and the mechanics of latent misalignment