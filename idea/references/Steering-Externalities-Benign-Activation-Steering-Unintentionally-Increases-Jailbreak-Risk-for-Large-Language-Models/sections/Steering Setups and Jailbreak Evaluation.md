Steering Setups and Jailbreak Evaluation
We study steering externalities: safety regressions that arise when a developer applies benign, utility-motivated activation steering at inference time. Concretely, we focus on realistic workflows where steering vectors are learned from benign data to (a) reduce refusals on innocuous requests ~\cite{b11} or (b) enforce instruction-following on structured formatting such as JSON ~\cite{b23}, then deployed globally at inference time. Instead of introducing a new steering algorithm, our goal is to evaluate how these common interventions affect the safety margin of the LLMs under a composite jailbreak evaluation that combines intrinsic safety degradation and amplification under multiple black-box adaptive jailbreak attacks. We evaluate safety in two regimes in the paper. Benchmarkonly means we query the model using the original harmful prompts provided by the benchmark dataset (i.e., direct harmful requests, without any rewriting). Adaptive attack means we run a black-box jailbreak algorithm that iteratively revises the harmful request based on the steered model's feedback, producing an adapted adversarial prompt.

Section references:
[b11]: B Lee, I Padhi, K Ramamurthy, E Miehling, P Dognin, M Nagireddy, A Dhurandhar. Programming refusal with conditional activation steering. (2025). Programming refusal with conditional activation steering
[b23]: A Stolfo, V Balachandran, S Yousefi, E Horvitz, B Nushi. Improving instruction-following in language models through activation steering. (2025). Improving instruction-following in language models through activation steering