Token-level Evidence: Per-Token KL Divergence Analysis
We provide supporting evidence for this hypothesis by analyzing the distributional shift introduced by activation steering. Following the methodology of ~\cite{b21}, we calculated the per-token Kullback-Leibler (KL) divergence between the original aligned models and their steered counterparts for both Llama-3-8B-Instruct and Gemma-7B-it (in Appendix D). Rather than relying on an external harmful prompt-response corpus (HEx-PHI datasets), we construct a harmful prompt-response set from HarmBench to match the benchmark used throughout the paper: We use Mistral-7B-Instruct-v0.2 to generate on full harmful questions from Harmbench dataset and record the harmful responses (evaluated by the Harmbench classifier). Here we obtained 125 instruction and response pairs. Figure 3 and Figure 6 in Appendix D show token-wise KL divergence between the original model and the steered model under STEER-COMPLIANCE and STEER-JSON, respectively. In both settings, the KL divergence is largest in the first few generated tokens (especially on harmful prompts) and then rapidly stabilizes. This pattern suggests that activation steering primarily perturbs the model during the critical prefix window where instruction-tuned models typically choose between a refusal template (e.g., "Sorry, I can't. . . ") and a compliant opener (e.g., "Sure-here is. . . "). Because generation is autoregressive, these early tokens effectively act as a mode-setting prefix: once steering shifts probability mass away from early refusal markers and toward a non-refusal opening, subsequent token distributions are conditioned on that new prefix and tend to continue along a more compliant trajectory, which can result in harmful completions. Importantly, this mechanism is not unique to explicitly "compliance"-oriented directions; even ostensibly benign JSON steering can be safety-relevant if it disrupts the usual refusal prefix/structure and prevents the model from entering the refusal trajectory in the first place. Overall, the early KL spikes provide empirical support that steering reduces the model's safety margin by inducing a distributional shift at the beginning of the response, which can induce a qualitatively different downstream generation trajectory. We further illustrate this phenomenon with qualitative examples in Figure 9 and Figure 12 (Appendix E), which compare outputs for identical harmful queries across Llama-2-7B-Chat and Gemma-7B-it. We observe that after activation steering-whether for compliance or JSON formatting-responses shift significantly toward the steered objective. Notably, the first generated token immediately establishes a positive tone or initiates a JSON structure, contrasting sharply with the baseline refusal. This confirms that steering effectively overrides the refusal gate by manipulating the initial token distribution.

[IMAGE START]Figure 3 . Figure 3. Per-token KL Divergence between Original and Compliance Steered Model on Llama-3-8B-Instruct. Red lines indicate the KL Divergence on Harmbench responses, blue lines are the KL Divergence on Alpaca (Benign) responses.[IMAGE END]


[IMAGE START]Figure 6 .. Figure 6. Per-token KL Divergence between Original and JSON Steered Model on Llama-3-8B-Instruct. Red lines indicate the KL Divergence on Harmbench responses, blue lines are the KL Divergence on Alpaca (Benign) responses.[IMAGE END]


[IMAGE START]Figure 9 . Figure 9. Qualitative comparison on Llama-2-7B-Chat jailbreak responses between Original and Compliance Steered Models[IMAGE END]


Section references:
[b21]: X Qi, A Panda, K Lyu, X Ma, S Roy, A Beirami, P Mittal, P Henderson. Safety alignment should be made more than just a few tokens deep. (2024). Safety alignment should be made more than just a few tokens deep