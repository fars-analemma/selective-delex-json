## References

- [Beyond Prompts: Space-Time Decoupling Control-Plane Jailbreaks in LLM Structured Output](./references/Output-Constraints-as-Attack-Surface-Exploiting-Structured-Generation-to-Bypass-LLM-Safety-Mechanisms/meta/meta_info.txt) - Zhang et al., 2026
- [Steering Externalities: Benign Activation Steering Unintentionally Increases Jailbreak Risk for Large Language Models](./references/Steering-Externalities-Benign-Activation-Steering-Unintentionally-Increases-Jailbreak-Risk-for-Large-Language-Models/meta/meta_info.txt) - Xiong et al., 2026
- [Generating Structured Outputs from Language Models: Benchmark and Studies (JSONSchemaBench)](./references/Generating-Structured-Outputs-from-Language-Models-Benchmark-and-Studies/meta/meta_info.txt) - Geng et al., 2025
- [CARE: Decoding Time Safety Alignment via Rollback and Introspection Intervention](./references/CARE-Decoding-Time-Safety-Alignment-via-Rollback-and-Introspection-Intervention/meta/meta_info.txt) - Hu et al., 2025
- [StruQ: Defending Against Prompt Injection with Structured Queries](./references/StruQ-Defending-Against-Prompt-Injection-with-Structured-Queries/meta/meta_info.txt) - Chen et al., 2025
- [HarmBench](https://arxiv.org/abs/2402.04249) - Mazeika et al., 2024 (code: https://github.com/centerforaisafety/HarmBench)
- [StrongREJECT](https://openreview.net/forum?id=al303JJkGO) - Souly et al., 2024
- [Llama Guard](https://arxiv.org/abs/2312.06674) - Inan et al., 2023
- [Outlines](https://arxiv.org/abs/2307.09702) - Willard & Louf, 2023
- (Plus additional citations embedded in Related Work.)