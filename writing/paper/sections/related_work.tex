\section{Related Work}
\label{sec:related}

\subsection{LLM Safety and Jailbreaks}

Large language models undergo safety alignment through reinforcement learning from human feedback (RLHF) and constitutional AI methods~\citep{Bai2022ConstitutionalAH} to prevent harmful content generation. However, these safeguards remain vulnerable to jailbreak attacks that manipulate models into producing objectionable outputs. White-box attacks such as GCG~\citep{Zou2023UniversalAT} use gradient-based optimization to find adversarial suffixes that transfer across models. Black-box methods including PAIR~\citep{Chao2023JailbreakingBB} and TAP~\citep{Mehrotra2023TreeOA} employ attacker LLMs to iteratively refine prompts, achieving high success rates with limited queries. Defense mechanisms include input-output safeguards like Llama Guard~\citep{Inan2023LlamaGL}, which performs multi-class classification on prompts and responses. A comprehensive taxonomy of these attacks and defenses is provided by~\citet{Yi2024JailbreakAA}. Critically, existing work focuses on prompt-level attacks where adversarial content resides in the user input, whereas our work addresses a distinct attack surface in the schema specification.

\subsection{Structured Output Generation}

Constrained decoding has emerged as the dominant approach for guaranteeing structured outputs from LLMs. Outlines~\citep{Willard2023EfficientGG} reformulates text generation as transitions between finite-state machine states, enabling efficient enforcement of regular expressions and context-free grammars through vocabulary indexing. XGrammar~\citep{Dong2024XGrammarFA} accelerates grammar execution by partitioning vocabulary into context-independent and context-dependent tokens, achieving up to 100$\times$ speedup over prior methods. SynCode~\citep{Ugare2024SynCodeLG} constructs offline DFA mask stores for programming language grammars, reducing syntax errors by 96\%. These techniques are integrated into production serving systems like vLLM~\citep{Kwon2023EfficientMM}, which provides efficient memory management for LLM inference. JSONSchemaBench~\citep{geng2025jsonschemabenchrigorousbenchmarkstructured} provides a comprehensive benchmark of 10K real-world JSON schemas for evaluating constrained decoding frameworks. While these systems guarantee syntactic validity, they introduce a new attack surface: forced enum and const literals in JSON schemas can embed arbitrary content that the model must output verbatim.

\subsection{Control-Plane Attacks on Structured Outputs}

Recent work has identified structured output interfaces as a novel attack vector distinct from traditional prompt injection~\citep{Liu2023PromptIA}. \citet{zhang2026promptsspacetimedecouplingcontrolplane} introduce Constrained Decoding Attack (CDA), demonstrating that malicious intent can be embedded in schema-level grammar rules (control-plane) while maintaining benign surface prompts (data-plane). Their EnumAttack achieves 96.2\% attack success rate across proprietary and open-weight LLMs. Similarly, \citet{Li2025ExploitingPI} propose AttackPrefixTree (APT), which exploits structured output interfaces to dynamically construct attack patterns by leveraging prefixes of safety refusal responses. While StruQ~\citep{struqdefendingagainstunknown} addresses prompt injection through structured queries that separate prompts and data into distinct channels, it does not address control-plane attacks where the schema itself contains malicious content. Our work presents the first defense specifically targeting control-plane jailbreaks in structured output APIs.

